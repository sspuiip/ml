<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>7. 表示学习 &#8212; Machine Learning Fundation 1.0 文档</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/nature.css?v=279e0f84" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <script src="../_static/documentation_options.js?v=f115507d"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="../_static/mathjax/tex-chtml.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="1. 神经网络" href="neuro_network_mlp.html" />
    <link rel="prev" title="6. 集成学习" href="ensemble.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             accesskey="I">索引</a></li>
        <li class="right" >
          <a href="neuro_network_mlp.html" title="1. 神经网络"
             accesskey="N">下一页</a> |</li>
        <li class="right" >
          <a href="ensemble.html" title="6. 集成学习"
             accesskey="P">上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">7. </span>表示学习</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1><span class="section-number">7. </span>表示学习<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h1>
<section id="autoencoder">
<h2><span class="section-number">7.1. </span>自编码器(AutoEncoder)<a class="headerlink" href="#autoencoder" title="Link to this heading">¶</a></h2>
<p>  自编码器（Autoencoder）是一种无监督学习的神经网络模型，用于学习输入数据的有效编码（表示）。它的基本目标是让输出尽可能地重构输入。自编码器通常包括二个部分：</p>
<ol class="arabic simple">
<li><p>编码器（Encoder）：将原始输入数据压缩为一个低维的潜在表示（latent representation）。通常是一个神经网络。如：</p></li>
</ol>
<div class="math notranslate nohighlight" id="equation-encoder">
<span class="eqno">(1)<a class="headerlink" href="#equation-encoder" title="Link to this equation">¶</a></span>\[
\pmb{h} = E_\phi(\pmb{x}) = \sigma(\pmb{Wx} + \pmb{b})
\]</div>
<ol class="arabic simple" start="2">
<li><p>解码器（Decoder）：将潜在表示解码回原始输入数据的近似值。通常也是一个神经网络。如：</p></li>
</ol>
<div class="math notranslate nohighlight" id="equation-decoder">
<span class="eqno">(2)<a class="headerlink" href="#equation-decoder" title="Link to this equation">¶</a></span>\[
\pmb{\hat{x}} = D_\theta(\pmb{h}) = \sigma(\pmb{Gh} + \pmb{d})
\]</div>
<p>  对于任意<span class="math notranslate nohighlight">\(\pmb{x}\in\mathcal{X}\)</span>,</p>
<div class="math notranslate nohighlight" id="equation-auto-encoder">
<span class="eqno">(3)<a class="headerlink" href="#equation-auto-encoder" title="Link to this equation">¶</a></span>\[
\pmb{h}\triangleq E_\phi(\pmb{x})=\sigma(\pmb{Wx}+\pmb{b}),\quad \pmb{\hat{x}}\triangleq D_\theta (\pmb{h})=\sigma(\pmb{Gh}+\pmb{d})
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{W},\pmb{H}\)</span>分别为编码层参数和解码层参数，<span class="math notranslate nohighlight">\(\sigma\)</span>为激活函数。</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="../_images/autoencoder.png"><img alt="AutoEncoder" src="../_images/autoencoder.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-text">自编码器示例</span><a class="headerlink" href="#id2" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>训练自编码器</strong></p></li>
</ul>
<p>  自编码器的目标是对于样本<span class="math notranslate nohighlight">\(\pmb{x}\sim q(\pmb{x}) \in\mathcal{X}\)</span>的重构误差测度<span class="math notranslate nohighlight">\(d:\pmb{X}\times\pmb{X}\rightarrow [0,\infty]\)</span>尽可能的小，即</p>
<div class="math notranslate nohighlight" id="equation-ae-min0">
<span class="eqno">(4)<a class="headerlink" href="#equation-ae-min0" title="Link to this equation">¶</a></span>\[
\min\limits_{\theta,\phi} \quad \mathcal{L}(\phi,\theta)=\mathbb{E}_{x\sim q}[d(\pmb{x},D_\theta (E_\phi (\pmb{x})))]
\]</div>
<p>  一般来说，<span class="math notranslate nohighlight">\(q(\pmb{x})\)</span>取值为经验分布，</p>
<div class="math notranslate nohighlight" id="equation-ae-q">
<span class="eqno">(5)<a class="headerlink" href="#equation-ae-q" title="Link to this equation">¶</a></span>\[
q(\pmb{x})=\frac{1}{N}\sum_{i=1}^N \delta_{\pmb{x}_i}
\]</div>
<p>以及<span class="math notranslate nohighlight">\(d(\pmb{x},\pmb{x}')=\Vert \pmb{x}-\pmb{x}'\Vert_2^2\)</span>. 因此，寻找最估自编码器就等价于一个最小二乘优化，即</p>
<div class="math notranslate nohighlight" id="equation-ae-min">
<span class="eqno">(6)<a class="headerlink" href="#equation-ae-min" title="Link to this equation">¶</a></span>\[
\min\limits_{\theta,\phi}\quad\frac{1}{N}\sum_{i=1}^N \left\Vert \pmb{x}-D_\theta (E_\phi (\pmb{x}))\right\Vert_2^2
\]</div>
<div class="dropdown admonition">
<p class="admonition-title">示例代码</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. 导入库</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>


<span class="c1"># 2. 加载 MNIST 数据集</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># Flatten to 784</span>
<span class="p">])</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>


<span class="c1"># 3. 定义自编码器模型</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Autoencoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>  <span class="c1"># 将输出限制在 [0,1]</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_hat</span>

<span class="c1">#  4. 初始化模型和优化器</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="c1"># 5.训练模型</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">x_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">x_hat</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">total_loss</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
<span class="c1"># 6.可视化重建结果</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">x_hat</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">break</span>  <span class="c1"># 只取一批</span>

<span class="c1"># 显示前8张原图与重建图</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="c1"># 原图</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
    
    <span class="c1"># 重建图</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_hat</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Top: Original | Bottom: Reconstructed&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</section>
<section id="sparse-autoencoder">
<h2><span class="section-number">7.2. </span>稀疏自编码器(Sparse AutoEncoder)<a class="headerlink" href="#sparse-autoencoder" title="Link to this heading">¶</a></h2>
<p>  稀疏自编码器（Sparse AutoEncoder）是一种自编码器的变体，其目标是在编码过程中强制大部分神经元保持非激活状态，从而学习到更稀疏的表示。稀疏性可以通过在损失函数中添加一个正则化项来实现。典型的稀疏自编码器的损失函数形式为：</p>
<div class="math notranslate nohighlight" id="equation-sparse-ae-min">
<span class="eqno">(7)<a class="headerlink" href="#equation-sparse-ae-min" title="Link to this equation">¶</a></span>\[
\mathcal{L}(\phi,\theta) = \mathbb{E}_{x\sim q}\left[d(\pmb{x},D_\theta (E_\phi (\pmb{x}))) + \lambda \cdot \text{Sparsity}(E_\phi (\pmb{x}))\right]
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\lambda\)</span>是稀疏性正则化的权重，<span class="math notranslate nohighlight">\(\text{Sparsity}\)</span>是一个衡量编码器输出稀疏性的函数。例如：</p>
<div class="math notranslate nohighlight" id="equation-sparse-ae-min2">
<span class="eqno">(8)<a class="headerlink" href="#equation-sparse-ae-min2" title="Link to this equation">¶</a></span>\[
\mathcal{L}(\phi,\theta) = \mathbb{E}_{x\sim q}\left[\Vert \pmb{x}-D_\theta (E_\phi (\pmb{x}))\Vert_2^2 + \lambda \cdot \sum_{j=1}^m \text{KL}(\rho || \hat{\rho}_j)\right]
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\rho\)</span>是期望的稀疏激活率，<span class="math notranslate nohighlight">\(\hat{\rho}_j\)</span>是编码器第<span class="math notranslate nohighlight">\(j\)</span>个神经元的实际激活率。</p>
<div class="dropdown admonition">
<p class="admonition-title">示例代码</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">kl_divergence</span><span class="p">(</span><span class="n">rho</span><span class="p">,</span> <span class="n">rho_hat</span><span class="p">):</span>
    <span class="n">rho_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">rho_hat</span><span class="p">,</span> <span class="mf">1e-10</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-10</span><span class="p">)</span>  <span class="c1"># 避免 log(0)</span>
    <span class="k">return</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">rho</span> <span class="o">/</span> <span class="n">rho_hat</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">rho_hat</span><span class="p">))</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SparseAutoencoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rho</span> <span class="o">=</span> <span class="n">rho</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">z</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">loss_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="c1"># 重建误差</span>
        <span class="n">recon_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">x_hat</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
        <span class="c1"># 稀疏性惩罚项</span>
        <span class="n">rho_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 平均激活值</span>
        <span class="n">kl</span> <span class="o">=</span> <span class="n">kl_divergence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rho</span><span class="p">,</span> <span class="n">rho_hat</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">recon_loss</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">kl</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># Flatten to 784</span>
<span class="p">])</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>


<span class="c1"># 训练过程</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SparseAutoencoder</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">x_hat</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">total_loss</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1"># 可视化隐藏层激活分布</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">sample_x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">sample_x</span> <span class="o">=</span> <span class="n">sample_x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">sample_x</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">z</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Hidden Layer Activations (1st sample)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Hidden Unit Index&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Activation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</section>
<section id="denoising-autoencoder">
<h2><span class="section-number">7.3. </span>去噪自编码器(Denoising AutoEncoder)<a class="headerlink" href="#denoising-autoencoder" title="Link to this heading">¶</a></h2>
<p>  去噪自编码器（Denoising AutoEncoder）是一种自编码器的变体，其目标是在输入数据中添加噪声，然后训练模型从噪声中恢复原始数据。这样可以使模型学习到更鲁棒的特征表示。去噪自编码器的训练过程通常包括以下步骤：</p>
<ol class="arabic simple">
<li><p>对输入数据添加噪声，生成噪声数据。</p></li>
<li><p>使用噪声数据作为输入，原始数据作为目标，训练自编码器模型。</p></li>
</ol>
<div class="dropdown admonition">
<p class="admonition-title">示例代码</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># ---------------------------</span>
<span class="c1"># 添加高斯噪声的变换</span>
<span class="c1"># ---------------------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">AddGaussianNoise</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.3</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">std</span> <span class="o">=</span> <span class="n">std</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">std</span>

<span class="c1"># ---------------------------</span>
<span class="c1"># 自定义 Dataset：输出 (noisy, clean)</span>
<span class="c1"># ---------------------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">NoisyMNISTDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">clean_dataset</span><span class="p">,</span> <span class="n">noisy_dataset</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clean</span> <span class="o">=</span> <span class="n">clean_dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">noisy</span> <span class="o">=</span> <span class="n">noisy_dataset</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">clean</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">clean_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clean</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>   <span class="c1"># Tensor</span>
        <span class="n">noisy_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">noisy</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>   <span class="c1"># Tensor</span>
        <span class="k">return</span> <span class="n">noisy_img</span><span class="p">,</span> <span class="n">clean_img</span>

<span class="c1"># ---------------------------</span>
<span class="c1"># 去噪自编码器模型</span>
<span class="c1"># ---------------------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">DenoisingAutoencoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="c1"># ---------------------------</span>
<span class="c1"># 加载数据集</span>
<span class="c1"># ---------------------------</span>
<span class="k">def</span><span class="w"> </span><span class="nf">get_dataloaders</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="n">transform_clean</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="p">])</span>
    <span class="n">transform_noisy</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
        <span class="n">AddGaussianNoise</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
    <span class="p">])</span>

    <span class="n">train_clean</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_clean</span><span class="p">)</span>
    <span class="n">train_noisy</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_noisy</span><span class="p">)</span>
    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">NoisyMNISTDataset</span><span class="p">(</span><span class="n">train_clean</span><span class="p">,</span> <span class="n">train_noisy</span><span class="p">)</span>

    <span class="n">test_clean</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_clean</span><span class="p">)</span>
    <span class="n">test_noisy</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform_noisy</span><span class="p">)</span>
    <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">NoisyMNISTDataset</span><span class="p">(</span><span class="n">test_clean</span><span class="p">,</span> <span class="n">test_noisy</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span>

<span class="c1"># ---------------------------</span>
<span class="c1"># 训练模型</span>
<span class="c1"># ---------------------------</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">noisy_x</span><span class="p">,</span> <span class="n">clean_x</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">noisy_x</span><span class="p">,</span> <span class="n">clean_x</span> <span class="o">=</span> <span class="n">noisy_x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">clean_x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">noisy_x</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">clean_x</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">total_loss</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># ---------------------------</span>
<span class="c1"># 可视化重建效果</span>
<span class="c1"># ---------------------------</span>
<span class="k">def</span><span class="w"> </span><span class="nf">visualize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">noisy_x</span><span class="p">,</span> <span class="n">clean_x</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">))</span>
        <span class="n">noisy_x</span><span class="p">,</span> <span class="n">clean_x</span> <span class="o">=</span> <span class="n">noisy_x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">clean_x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">denoised_x</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">noisy_x</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
        <span class="c1"># noisy</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">noisy_x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
        <span class="c1"># denoised</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">num_samples</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">denoised_x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
        <span class="c1"># clean</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_samples</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">clean_x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Top: Noisy | Middle: Denoised | Bottom: Original&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># ---------------------------</span>
<span class="c1"># 主函数</span>
<span class="c1"># ---------------------------</span>
<span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span> <span class="o">=</span> <span class="n">get_dataloaders</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DenoisingAutoencoder</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">visualize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
</section>
<section id="variational-autoencoder">
<h2><span class="section-number">7.4. </span>变分自编码器(Variational AutoEncoder)<a class="headerlink" href="#variational-autoencoder" title="Link to this heading">¶</a></h2>
<p>  <strong>变分自编码器</strong>是一种带有概率建模思想的自编码器，相比普通自编码器，它不仅学习一个固定的隐空间向量，而是学习一个潜在变量的分布（通常是高斯分布）。VAE的<strong>主要思想</strong>是将输入编码为一个高斯分布（均值<span class="math notranslate nohighlight">\(\mu\)</span>，方差<span class="math notranslate nohighlight">\(\sigma^2\)</span>），然后从中采样一个<span class="math notranslate nohighlight">\(\pmb{z}\)</span>向量，再用解码器重构原始输入。</p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="../_images/vae-frame.png"><img alt="AutoEncoder" src="../_images/vae-frame.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-text">变分自编码器示例</span><a class="headerlink" href="#id3" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>特点</p></th>
<th class="head"><p>普通自编码器（AE）</p></th>
<th class="head"><p>变分自编码器（VAE）</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>编码器输出</p></td>
<td><p>一个确定向量 z</p></td>
<td><p>一个分布（均值 μ 和标准差 σ）</p></td>
</tr>
<tr class="row-odd"><td><p>解码器输入</p></td>
<td><p>固定的 z</p></td>
<td><p>从分布中采样的 z ~ N(μ, σ²)</p></td>
</tr>
<tr class="row-even"><td><p>表达形式</p></td>
<td><p>点表示</p></td>
<td><p>分布式表示（更适合生成）</p></td>
</tr>
<tr class="row-odd"><td><p>生成能力</p></td>
<td><p>弱</p></td>
<td><p>强，可用于图像生成</p></td>
</tr>
</tbody>
</table>
<p>  变分自编码器本质上是一个有潜变量的概率模型，其<strong>目标</strong>是建模观测变量的生成分布<span class="math notranslate nohighlight">\(p(\pmb{x})\)</span>，即</p>
<div class="math notranslate nohighlight" id="equation-vae-px">
<span class="eqno">(9)<a class="headerlink" href="#equation-vae-px" title="Link to this equation">¶</a></span>\[
\pmb{x}\sim p_\theta(\pmb{x})=\int p_\theta(\pmb{x}|\pmb{z})p(\pmb{z})d\pmb{z}
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{x}\)</span>是观测变量；<span class="math notranslate nohighlight">\(\pmb{z}\)</span>是潜变量；<span class="math notranslate nohighlight">\(p(\pmb{z})\)</span>是潜变量的先验分布，通常取为标准正态分布，即<span class="math notranslate nohighlight">\(p(\pmb{z})=\mathcal{N}(\pmb{0},\pmb{I})\)</span>；<span class="math notranslate nohighlight">\(p_\theta(\pmb{x}|\pmb{z})\)</span>为解码器，通常是一个神经网络，用于从潜变量<span class="math notranslate nohighlight">\(\pmb{z}\)</span>生成观测变量<span class="math notranslate nohighlight">\(\pmb{x}\)</span>，<span class="math notranslate nohighlight">\(\theta\)</span>是解码器的参数。</p>
<p>  <strong>由于直接计算<span class="math notranslate nohighlight">\(p(\pmb{x})\)</span>通常是不可行的</strong>，因为式<a class="reference internal" href="#equation-vae-px">(9)</a>中的积分是高维的，且潜变量<span class="math notranslate nohighlight">\(\pmb{z}\)</span>的分布<span class="math notranslate nohighlight">\(p_\theta(\pmb{z}|\pmb{x})\)</span>通常是未知的。为了克服这个问题，引入变分分布<span class="math notranslate nohighlight">\(q_\phi(\pmb{z}|\pmb{x})\)</span>来近似分布<span class="math notranslate nohighlight">\(p_\theta(\pmb{z}|\pmb{x})\)</span>，其中<span class="math notranslate nohighlight">\(\phi\)</span>是变分分布的参数，即</p>
<div class="math notranslate nohighlight" id="equation-vae-qz">
<span class="eqno">(10)<a class="headerlink" href="#equation-vae-qz" title="Link to this equation">¶</a></span>\[
q_\phi(\pmb{z}|\pmb{x})\approx p_\theta(\pmb{z}|\pmb{x})
\]</div>
<p>这是变分推断的核心思想。使用Jensen不等式来构造一个变分下界（ELBO）来逼近真实的对数似然。</p>
<div class="math notranslate nohighlight" id="equation-vae-elbo">
<span class="eqno">(11)<a class="headerlink" href="#equation-vae-elbo" title="Link to this equation">¶</a></span>\[\begin{split}
\begin{split}
\log p_\theta(\pmb{x}) &amp;= \log \int p_\theta(\pmb{x}|\pmb{z})p(\pmb{z})d\pmb{z} \\
&amp;= \log \int q_\phi(\pmb{z}|\pmb{x})\frac{p_\theta(\pmb{x}|\pmb{z})p(\pmb{z})}{q_\phi(\pmb{z}|\pmb{x})}d\pmb{z} \\
&amp;\geq \int q_\phi(\pmb{z}|\pmb{x})\log \frac{p_\theta(\pmb{x}|\pmb{z})p(\pmb{z})}{q_\phi(\pmb{z}|\pmb{x})}d\pmb{z} \\
&amp;= \mathbb{E}_{q_\phi(\pmb{z}|\pmb{x})}\left[\log p_\theta(\pmb{x}|\pmb{z})\right] - D_{KL}(q_\phi(\pmb{z}|\pmb{x})||p(\pmb{z}))
\end{split}
\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(D_{KL}\)</span>是Kullback-Leibler散度，用于衡量两个分布之间的差异。上式中的第一项是重构误差，第二项是正则化项，用于控制潜变量的分布与先验分布的差异。由式<a class="reference internal" href="#equation-vae-elbo">(11)</a>的等式右边，可以得到一个<strong>变分下界</strong>，即</p>
<div class="math notranslate nohighlight" id="equation-vae-elbo-def">
<span class="eqno">(12)<a class="headerlink" href="#equation-vae-elbo-def" title="Link to this equation">¶</a></span>\[
\mathcal{L}(\theta, \phi;\pmb{x}) = \mathbb{E}_{q_\phi(\pmb{z}|\pmb{x})}\left[\log p_\theta(\pmb{x}|\pmb{z})\right] - D_{KL}(q_\phi(\pmb{z}|\pmb{x})||p(\pmb{z}))
\]</div>
<p>其中，等式右边的<span class="math notranslate nohighlight">\(\mathbb{E}_{q_\phi(\pmb{z}|\pmb{x})}\left[\log p_\theta(\pmb{x}|\pmb{z})\right]\)</span>称为<strong>证据下界</strong>（Evidence Lower Bound, ELBO）。因为散度<span class="math notranslate nohighlight">\(D_{KL}(q_\phi(\pmb{z}|\pmb{x})||p(\pmb{z}))\)</span>是非负的，所以<strong>最大化ELBO等价于最大化对数似然</strong><span class="math notranslate nohighlight">\(\log p(\pmb{x})\)</span>。</p>
<p>  <strong>一般来说，通常使用高斯假设来简化变分分布</strong><span class="math notranslate nohighlight">\(q_\phi(\pmb{z}|\pmb{x})\)</span>，即假设<span class="math notranslate nohighlight">\(q_\phi(\pmb{z}|\pmb{x})=\mathcal{N}(\mu_\phi(\pmb{x}),\sigma_\phi^2(\pmb{x}))\)</span>，其中<span class="math notranslate nohighlight">\(\mu_\phi(\pmb{x})\)</span>和<span class="math notranslate nohighlight">\(\sigma_\phi^2(\pmb{x})\)</span>是编码器的输出。先验分布<span class="math notranslate nohighlight">\(p(\pmb{z})\)</span>通常取为标准正态分布，即<span class="math notranslate nohighlight">\(p(\pmb{z})=\mathcal{N}(\pmb{0},\pmb{I})\)</span>。解码器输出<span class="math notranslate nohighlight">\(p_\theta(\pmb{x}|\pmb{z})\)</span>通常也是一个神经网络，用于从潜变量<span class="math notranslate nohighlight">\(\pmb{z}\)</span>生成观测变量<span class="math notranslate nohighlight">\(\pmb{x}\)</span>。解码器完成采样潜变量<span class="math notranslate nohighlight">\(\pmb{z}\)</span>(固定参数的正态分布采样)，再将<span class="math notranslate nohighlight">\(\pmb{z}\)</span>通过神经网络变换为<span class="math notranslate nohighlight">\(\pmb{x}\)</span>，其过程如下：</p>
<div class="math notranslate nohighlight" id="equation-vae-decoder">
<span class="eqno">(13)<a class="headerlink" href="#equation-vae-decoder" title="Link to this equation">¶</a></span>\[\begin{split}
\boxed{
\begin{split}
\pmb{z} \sim p(\pmb{z}) &amp;= \mathcal{N}(\pmb{0},\pmb{I}) \\
\hat{\pmb{x}} &amp;= \textrm{NeuroNet}(\pmb{z};\theta)\\
p(\pmb{x}|\pmb{z}) &amp;= \mathcal{N}(\hat{\pmb{x}},\pmb{I}) \\
\end{split}}
\end{split}\]</div>
<p>编码器则对应观测变量<span class="math notranslate nohighlight">\(\pmb{x}\)</span>到潜变量<span class="math notranslate nohighlight">\(\pmb{z}\)</span>的映射，其过程如下：</p>
<div class="math notranslate nohighlight" id="equation-vae-encoder">
<span class="eqno">(14)<a class="headerlink" href="#equation-vae-encoder" title="Link to this equation">¶</a></span>\[\begin{split}
\boxed{
\begin{split}
\pmb{\mu},\sigma  &amp;= \textrm{NeuroNet}(\pmb{x};\phi)\\
q_\phi(\pmb{z}|\pmb{x}) &amp;= \mathcal{N}(\pmb{z};\pmb{\mu},\sigma^2\pmb{I}) \\
\end{split} }
\end{split}\]</div>
<p>  需要注意的是，直接从分布中采样<span class="math notranslate nohighlight">\(\pmb{z}\)</span>会导致梯度无法传播，因此需要使用<strong>重参数化技巧</strong>（Reparameterization Trick）来解决这个问题。具体来说，可以将采样过程改为：</p>
<div class="math notranslate nohighlight" id="equation-vae-reparam">
<span class="eqno">(15)<a class="headerlink" href="#equation-vae-reparam" title="Link to this equation">¶</a></span>\[
\pmb{z} = \pmb{\mu} + \sigma \odot \pmb{\epsilon}
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{\epsilon} \sim \mathcal{N}(\pmb{0},\pmb{I})\)</span>是一个标准正态分布的随机变量，<span class="math notranslate nohighlight">\(\odot\)</span>表示逐元素相乘。这样就可以将采样过程转化为一个确定性函数，从而使得梯度可以传播。</p>
<p>  经过上述处理后，<strong>变分自编码器的训练目标就变成了最大化ELBO，即最小化以下损失函数</strong>：</p>
<div class="math notranslate nohighlight" id="equation-vae-loss">
<span class="eqno">(16)<a class="headerlink" href="#equation-vae-loss" title="Link to this equation">¶</a></span>\[
\mathcal{L}(\theta, \phi;\pmb{x}) = -\mathbb{E}_{q_\phi(\pmb{z}|\pmb{x})}\left[\log p_\theta(\pmb{x}|\pmb{z})\right] + D_{KL}(q_\phi(\pmb{z}|\pmb{x})||p(\pmb{z}))
\]</div>
<p>其中，第一项是重构误差，第二项是KL散度。通常使用均方误差（MSE）作为重构误差的测度。将式<a class="reference internal" href="#equation-vae-decoder">(13)</a>和式<a class="reference internal" href="#equation-vae-encoder">(14)</a>代入式<a class="reference internal" href="#equation-vae-loss">(16)</a>，可以得到变分自编码器的最终损失函数：</p>
<div class="math notranslate nohighlight" id="equation-vae-loss-final">
<span class="eqno">(17)<a class="headerlink" href="#equation-vae-loss-final" title="Link to this equation">¶</a></span>\[\begin{split}
\boxed{
\begin{split}
\mathcal{L}(\theta, \phi;\pmb{x}) &amp;= -\mathbb{E}_{\pmb{z}\sim q_\phi(\pmb{z}|\pmb{x})}\left[\log p(\pmb{x}|\pmb{z})\right] + \frac12\sum_{i=1}^D(\mu_i^2+\sigma_i^2-\log(\sigma_i^2)-1) \\
&amp;= -\frac12\sum_{i=1}^D\left(x_i - \hat{x}_i\right)^2 + \frac12\sum_{i=1}^D(\mu_i^2+\sigma_i^2-\log(\sigma_i^2)-1) + \textrm{const} \\
\end{split}}
\end{split}\]</div>
<p>其中，期望项的计算使用了蒙特卡罗方法近似。确定损失函数后，就可以使用反向传播算法来优化模型参数<span class="math notranslate nohighlight">\(\theta\)</span>和<span class="math notranslate nohighlight">\(\phi\)</span>。</p>
<div class="dropdown admonition">
<p class="admonition-title">示例代码</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchvision</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>

<span class="c1"># 参数</span>
<span class="n">input_dim</span> <span class="o">=</span> <span class="mi">784</span>  <span class="c1"># 28x28 images</span>
<span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">latent_dim</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-4</span>

<span class="c1"># x --&gt; z</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_logvar</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_mu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_logvar</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span>
    
<span class="c1"># z --&gt; x_hat</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">x_hat</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_hat</span>

<span class="k">def</span><span class="w"> </span><span class="nf">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">sigma</span>
    <span class="k">return</span> <span class="n">z</span>

<span class="k">class</span><span class="w"> </span><span class="nc">VAE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">)</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">get_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
        <span class="n">x_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">L1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">x_hat</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
        <span class="n">L2</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">L1</span> <span class="o">+</span> <span class="n">L2</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch_size</span>
    
<span class="c1"># 数据集</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
                <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                <span class="n">transforms</span><span class="o">.</span><span class="n">Lambda</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">)</span> <span class="c1"># falatten</span>
            <span class="p">])</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># 训练</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">loss_sum</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">loss_sum</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">loss_avg</span> <span class="o">=</span> <span class="n">loss_sum</span> <span class="o">/</span> <span class="n">cnt</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">loss_avg</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_avg</span><span class="p">)</span>

<span class="c1"># 可视化损失</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># 可视化重构结果</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">sample_size</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">sample_size</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">generated_images</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">sample_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>

<span class="n">grid_img</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">generated_images</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">grid_img</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">目录</a></h3>
    <ul>
<li><a class="reference internal" href="#">7. 表示学习</a><ul>
<li><a class="reference internal" href="#autoencoder">7.1. 自编码器(AutoEncoder)</a></li>
<li><a class="reference internal" href="#sparse-autoencoder">7.2. 稀疏自编码器(Sparse AutoEncoder)</a></li>
<li><a class="reference internal" href="#denoising-autoencoder">7.3. 去噪自编码器(Denoising AutoEncoder)</a></li>
<li><a class="reference internal" href="#variational-autoencoder">7.4. 变分自编码器(Variational AutoEncoder)</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>上一主题</h4>
    <p class="topless"><a href="ensemble.html"
                          title="上一章"><span class="section-number">6. </span>集成学习</a></p>
  </div>
  <div>
    <h4>下一主题</h4>
    <p class="topless"><a href="neuro_network_mlp.html"
                          title="下一章"><span class="section-number">1. </span>神经网络</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>本页</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/ml/neuro_representation.md.txt"
            rel="nofollow">显示源代码</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">快速搜索</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="提交" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             >索引</a></li>
        <li class="right" >
          <a href="neuro_network_mlp.html" title="1. 神经网络"
             >下一页</a> |</li>
        <li class="right" >
          <a href="ensemble.html" title="6. 集成学习"
             >上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">7. </span>表示学习</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; 版权所有 2022-2024, SSPUIIP.
      由 <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3创建。
    </div>
  </body>
</html>