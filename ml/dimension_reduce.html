<!doctype html>
<html class="no-js" lang="zh-CN" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="索引" href="../genindex.html" /><link rel="search" title="搜索" href="../search.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2025.07.19 -->
        <title>数据降维 - Machine Learning Fundation 1.0 文档</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=25af2a20" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Machine Learning Fundation 1.0 文档</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <span class="sidebar-brand-text">Machine Learning Fundation 1.0 文档</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="搜索" name="q" aria-label="搜索">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">基础知识</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../base/prob_dist.html">1. 概率及分布</a></li>
<li class="toctree-l1"><a class="reference internal" href="../base/gaussian_model.html">2. 高斯模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../base/gaussian_process.html">3. 高斯过程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../base/calc_theory.html">4. 计算学习理论</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">监督学习</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dimension_reduce_2.html">1. 数据降维</a></li>
<li class="toctree-l1"><a class="reference internal" href="decision_tree.html">2. 决策树</a></li>
<li class="toctree-l1"><a class="reference internal" href="bayes.html">3. 贝叶斯分类</a></li>
<li class="toctree-l1"><a class="reference internal" href="EM.html">4. EM算法概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="PGM.html">5. 概率图模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="ensemble.html">6. 集成学习</a></li>
<li class="toctree-l1"><a class="reference internal" href="neuro_representation.html">7. 表示学习</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">深度学习</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="neuro_network_mlp.html">1. 神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="neuro_network_cnn.html">2. 卷积神经网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="neuro_network_rnn.html">3. 循环神经网络</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">无监督学习</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cmeans.html">1. 聚类(一)</a></li>
<li class="toctree-l1"><a class="reference internal" href="biClustering.html">2. BiClustering</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">核方法</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../kernel/base.html">1. 核函数基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kernel/base2.html">2. 核函数基础2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kernel/RKHS.html">3. 再生核希尔伯特空间</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kernel/MMD.html">4. 最大均值差</a></li>
<li class="toctree-l1"><a class="reference internal" href="../kernel/covariance_operators.html">5. 协方差算子</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">最优化</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../optimization/convex_prob.html">1. 凸优化问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimization/convex_solve.html">2. 优化问题求解(1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimization/convex_neq_solve.html">3. 优化问题求解(2)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">矩阵分析</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../matrix/base.html">1. 矩阵性能指标</a></li>
<li class="toctree-l1"><a class="reference internal" href="../matrix/matrixoper.html">2. 矩阵运算</a></li>
<li class="toctree-l1"><a class="reference internal" href="../matrix/vectorspace.html">3. 向量空间</a></li>
<li class="toctree-l1"><a class="reference internal" href="../matrix/matrixdiff.html">4. 矩阵微分</a></li>
<li class="toctree-l1"><a class="reference internal" href="../matrix/special_matrix.html">5. 特殊矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../matrix/subspace.html">6. 子空间分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../matrix/decomposition.html">7. 矩阵分解</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">粗糙集</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../roughset/roughbase.html">1. 粗糙集基础</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">数学建模</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mathmodel/statistic_model.html">1. 不确定模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mathmodel/bregman_divergence.html">2. Bregman divergence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mathmodel/entropy.html">3. 信息熵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mathmodel/conjugate_dist.html">4. 共轭分布</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mathmodel/mcmc.html">5. 随机模拟</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mathmodel/RFF.html">6. Random Fourier features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mathmodel/gumbel_trick.html">7. Gumbel Trick</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="../_sources/ml/dimension_reduce.md.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>数据降维<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h1>
<p>  一般来说，机器学习中的原始数据是高维数据。高维数据往往具有复杂性、冗余性等特点。高维空间会有很多不一样的特征，也称之为维度灾难。</p>
<ul class="simple">
<li><p>大多数数据对象之间相距都很远(高维数据有很大可能非常稀疏)</p></li>
<li><p>理论上，通过增大训练集，使训练集达到足够密度，是可以避免维度灾难的。但实践中，要达到给定密度，所需的训练数据随着维度的增加呈指数式上升</p></li>
</ul>
<p>  为了避免维度灾难，以及找到问题求解最合适的数据表示形式，需要研究原有数据的表示问题（合适的维度），这一过程也称之为<strong>数据降维</strong>。</p>
<hr class="docutils" />
<section id="id2">
<h2>主成分分析<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h2>
<p>  主成分分析(Principal Component Analysis, PCA)是一种通过某种正交变换将一组可能存在相关关系的变量转换为一组线性不相关的变量。对于训练数据，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\pmb{X}=\begin{pmatrix}|&amp;|&amp;\dots&amp;|\\\pmb{x}_1&amp;\pmb{x}_2&amp;\dots&amp;\pmb{x}_m\\ |&amp;|&amp;\dots&amp;| \end{pmatrix}_{n\times m}
\end{split}\]</div>
</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{x}_i=(x_{i1},...,x_{in})^\top\)</span>。PCA的<strong>目标</strong>是找到一个基<span class="math notranslate nohighlight">\((\pmb{w}_1,\pmb{w}_2,...,\pmb{w}_d)=\pmb{W}_{n\times d}\)</span>，使得<span class="math notranslate nohighlight">\(\pmb{Z}=\pmb{W}^\top\pmb{X}\)</span>的重构矩阵<span class="math notranslate nohighlight">\(\hat{\pmb{X}}=\pmb{WZ}\)</span>与<span class="math notranslate nohighlight">\(\pmb{X}\)</span>的误差尽可能的小，即，投影的超平面<span class="math notranslate nohighlight">\(\pmb{W}\)</span>使得投影后的数据矩阵<span class="math notranslate nohighlight">\(\pmb{Z}\)</span>丢失的信息最少。</p>
<p>  如何找到这个超平面呢？可行的一个办法是比较<span class="math notranslate nohighlight">\(\pmb{X}\)</span>与<span class="math notranslate nohighlight">\(\hat{\pmb{X}}\)</span>之间的平均距离（<span class="math notranslate nohighlight">\(\parallel \pmb{x}_i-\hat{\pmb{x}}_i\parallel^2\)</span>），使得这个距离最小的超平面就是最优投影超平面。这是PCA这主要思想。</p>
<p>  对于空间的所有样本点，如何用一个超平面来恰当的表示？有两种办法，即</p>
<ul class="simple">
<li><p>样本点到这个超平面的距离都足够近(投影距离最小)</p></li>
<li><p>样本点在这个超平面的投影尽可能分开</p></li>
</ul>
<hr class="docutils" />
<section id="id3">
<h3>最近重构性<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h3>
<p>  假设数据样本已<font color="red">中心化</font>所有样本减去均值即为中心化；(中心化之后再除以样本标准差即为标准化)，变换后的新坐标系为<span class="math notranslate nohighlight">\((\pmb{w}_1,\pmb{w}_2,...,\pmb{w}_d)\)</span>，若丢弃部分坐标，将维度降至<span class="math notranslate nohighlight">\(d'&lt;d\)</span>，则样本在低维坐标系中的投影为<span class="math notranslate nohighlight">\(\pmb{z}_i=(z_{i1},z_{i2},...,z_{id'})\)</span> ，其中<span class="math notranslate nohighlight">\(z_{ij}=\langle \pmb{x}_i,\pmb{w}_j\rangle\)</span>是<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>在低维坐标系的第<span class="math notranslate nohighlight">\(j\)</span>维的坐标。若用<span class="math notranslate nohighlight">\(\pmb{z}_i\)</span>来重构<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>，则会有<span class="math notranslate nohighlight">\(\hat{\pmb{x}}_i=\sum_{j=1}^{d'}z_{ij}\pmb{w}_j=\pmb{W}\pmb{z}_i\)</span>。</p>
<p>  对于整个数据集，原样本点<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>与投影重构<span class="math notranslate nohighlight">\(\hat{\pmb{x}}_i\)</span>之间距离为，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\sum_{i=1}^m \left\Vert \sum_{j=1}^{d'}z_{ij}\pmb{w}_j-\pmb{x}_i\right\Vert^2 &amp;=\sum_{i=1}^m \pmb{z}_i^\top\pmb{z}_i-2\sum_{i=1}^m\pmb{z}_i^\top\pmb{W}^\top\pmb{x}_i + \textrm{const}\\
&amp;=\sum_{i=1}\pmb{x}_i^\top\pmb{W}\pmb{W}^\top\pmb{x}_i-2\sum_{i=1}\pmb{x}_i^\top\pmb{W}\pmb{W}^\top\pmb{x}_i +\textrm{const}\\
&amp;=-\text{tr}\left( \sum_{i=1}\pmb{x}_i^\top\pmb{W}\pmb{W}^\top\pmb{x}_i  \right)+\textrm{const}\\
&amp;\propto-\text{tr}\left(\pmb{W}^\top\left(\sum_{i=1}^m \pmb{x}_i\pmb{x}_i^\top  \right)\pmb{W} \right)
\end{split}
\end{split}\]</div>
</div>
<p>PCA的优化目标则变成，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\min\limits_{\pmb{W}}\quad &amp;-\text{tr}\left(\pmb{W}^\top\pmb{XX}^\top\pmb{W} \right)\\
\textrm{s.t.}\quad &amp;\pmb{W}^\top\pmb{W}=\pmb{I}
\end{split}
\end{split}\]</div>
</div>
<p>其中，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\pmb{X}=\begin{pmatrix}|&amp;|&amp;\dots&amp;|\\\pmb{x}_1&amp;\pmb{x}_2&amp;\dots&amp;\pmb{x}_m\\ |&amp;|&amp;\dots&amp;| \end{pmatrix}_{d\times m}, \qquad
\pmb{W}=\begin{pmatrix}|&amp;|&amp;\dots&amp;|\\\pmb{w}_1&amp;\pmb{w}_2&amp;\dots&amp;x_{d'}\\ |&amp;|&amp;\dots&amp;| \end{pmatrix}_{d\times d'}
\end{split}\]</div>
</div>
</section>
<hr class="docutils" />
<section id="id4">
<h3>最大可分性<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h3>
<p>  样本点<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>在新空间的超平面投影为<span class="math notranslate nohighlight">\(\pmb{W}^\top\pmb{x}_i\)</span>，若要投影尽可能分开，则应使投影后的样本方差最大化，即，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\max_{\pmb{W}} \quad &amp;\text{tr}(\pmb{W}^\top\pmb{XX}^\top\pmb{W})\\
\text{s.t.}\quad &amp;\pmb{W}^\top\pmb{W}=\pmb{I}
\end{split}
\end{split}\]</div>
</div>
<p>可以看出，该问题与第一种情况是等价的。</p>
</section>
<hr class="docutils" />
<section id="id5">
<h3>优化问题的求解<a class="headerlink" href="#id5" title="Link to this heading">¶</a></h3>
<p>  使用拉格朗日乘子法，可得，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\mathcal{L}(\pmb{W},\pmb{\lambda})&amp;=\textrm{tr}(\pmb{W}^\top\pmb{XX}^\top\pmb{W})+\lambda(\pmb{W}^\top\pmb{W}-\pmb{I})\\
&amp;=\sum_{i=1}\pmb{w}_i^\top\pmb{XX}^\top\pmb{w}_i + \sum_{i=1}\lambda_i(\pmb{w}_i^\top\pmb{w}_i-1)
\end{split}
\end{split}\]</div>
</div>
<p>则有，<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial \pmb{w}_i}\)</span>,</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \pmb{w}_i}=\pmb{XX}^\top\pmb{w}_i-\lambda_i\pmb{w}_i
\]</div>
</div>
<p>令<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial \pmb{w}_i}=0\)</span>，则有</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\pmb{X}\pmb{X}^\top\pmb{w}_i=\lambda_i\pmb{w}_i
\]</div>
</div>
<p>于是，只要对样本协方差矩阵进行特征值分解，将求得的特征值排序后，取前<span class="math notranslate nohighlight">\(d'\)</span>个特征值对应的特征向量构成<font color="red">投影矩阵<span class="math notranslate nohighlight">\(W^*=(w_1,w_2,...,w_{d'})\)</span></font>。该矩阵即为主成分分析的解。</p>
</section>
<hr class="docutils" />
<section id="id6">
<h3>算法1–特征值分解<a class="headerlink" href="#id6" title="Link to this heading">¶</a></h3>
<p>  通过样本协方差矩阵计算PCA。</p>
<p>  <strong>输入</strong>：样本集<span class="math notranslate nohighlight">\(\mathcal{D}=\{x_1,x_2,...,x_m\}\)</span>，低维空间维数<span class="math notranslate nohighlight">\(d'\)</span>.</p>
<p>  <strong>过程</strong>：</p>
<p>  1：样本中心化： <span class="math notranslate nohighlight">\(\pmb{x}_i=\pmb{x}_i-\frac{1}{m}\sum_{i=1}^m\pmb{x}_i\)</span>；</p>
<p>  2：计算样本的协方差矩阵<span class="math notranslate nohighlight">\(\pmb{XX}^T\)</span>;</p>
<p>  3：对协方差矩阵做特征值分解；</p>
<p>  4：取出最大的<span class="math notranslate nohighlight">\(d'\)</span>个特征值对应的特征向量<span class="math notranslate nohighlight">\(\pmb{w}_1,\pmb{w}_2,...,\pmb{w}_{d'}\)</span>；</p>
<p>  <strong>输出</strong>： 投影矩阵<span class="math notranslate nohighlight">\(\pmb{W}^*=(\pmb{w}_1,\pmb{w}_2,...,\pmb{w}_{d'})\)</span>。</p>
<ul class="simple">
<li><p><strong>示例代码</strong></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>


<span class="k">def</span><span class="w"> </span><span class="nf">pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="n">n_samples</span><span class="p">,</span><span class="n">n_features</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="o">-</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">scatter_matrix</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="n">X</span><span class="p">)</span>
    <span class="n">eig_val</span><span class="p">,</span><span class="n">eig_vec</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">scatter_matrix</span><span class="p">)</span>
    <span class="n">eig_pairs</span><span class="o">=</span><span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">eig_val</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span><span class="n">eig_vec</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">)]</span>
    <span class="n">eig_pairs</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">features</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ele</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">ele</span> <span class="ow">in</span> <span class="n">eig_pairs</span><span class="p">[:</span><span class="n">k</span><span class="p">]])</span>
    <span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">,</span><span class="n">features</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span> 
    <span class="n">X_new</span><span class="p">,</span><span class="n">features</span><span class="o">=</span><span class="n">pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    
    
    
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;ro&#39;</span><span class="p">)</span><span class="c1">#,c = &#39;r&#39;,marker = &#39;o&#39;)</span>
    <span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">x1</span><span class="o">=</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">y1</span><span class="o">=</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">,</span><span class="s1">&#39;b-&#39;</span><span class="p">)</span>

    <span class="n">proj_dir</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">proj_dir</span><span class="o">=</span><span class="n">proj_dir</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">proj_dir</span><span class="p">)</span>
    <span class="c1">#计算投影</span>
    <span class="n">PX</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
        <span class="n">p</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">proj_dir</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">proj_dir</span><span class="p">)</span>
        <span class="n">px</span><span class="o">=</span><span class="n">p</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">py</span><span class="o">=</span><span class="n">p</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">PX</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">px</span><span class="p">,</span><span class="n">py</span><span class="p">])</span>
    <span class="n">PX</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">PX</span><span class="p">)</span>  
    <span class="k">for</span> <span class="n">ix</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;s&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">0</span><span class="p">]],[</span><span class="n">X</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span><span class="s1">&#39;y:&#39;</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">xy</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;(</span><span class="si">%.0f</span><span class="s2">,</span><span class="si">%.0f</span><span class="s2">)&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">xy</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span> <span class="c1">#标注数据样本</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="svd">
<h3>算法2–SVD分解<a class="headerlink" href="#svd" title="Link to this heading">¶</a></h3>
<p>  PCA除了对于协方差矩阵<span class="math notranslate nohighlight">\(\pmb{XX}^\top\)</span>进行特征值分解计算得到投影特征向量之外，还可以通过SVD矩阵分解技术得到投影向量。SVD矩阵分解如下式所示，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\hat{\pmb{X}}=\pmb{U\Sigma V}^\top, \quad \hat{\pmb{X}}^\top=\pmb{V\Sigma U}^\top
\]</div>
</div>
<p>这里的<span class="math notranslate nohighlight">\(\hat{\pmb{X}}\)</span>是正常的数据集矩阵。即</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{\pmb{X}}=\begin{pmatrix}-&amp;\pmb{x}_1 &amp;-\\ -&amp;\pmb{x}_1 &amp;-\\ 
\vdots&amp;\vdots &amp;\vdots\\ -&amp;\pmb{x}_m &amp;-\\ \end{pmatrix}
\end{split}\]</div>
</div>
<p>  PCA中的<span class="math notranslate nohighlight">\(\pmb{X}=\hat{\pmb{X}}^\top\)</span>，因此有，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\pmb{XX}^\top=\hat{\pmb{X}}^\top\hat{\pmb{X}}=\pmb{V\Sigma U}^\top \pmb{U\Sigma V}^\top=\pmb{V\Sigma}^2\pmb{V}^\top
\]</div>
</div>
<p>  最终，<span class="math notranslate nohighlight">\(\pmb{V}\)</span>的最大前<span class="math notranslate nohighlight">\(d'\)</span>个特征值对应的特征向量所组成的矩阵即为变换矩阵<span class="math notranslate nohighlight">\(\pmb{W}^*=(\pmb{w}_1,\pmb{w}_2,...,\pmb{w}_{d'})\)</span>。而<span class="math notranslate nohighlight">\(\pmb{V}\)</span>可以通过SVD分解获得。</p>
<p>  投影后的新数据(<span class="math notranslate nohighlight">\(d'&lt;n\)</span>)，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\pmb{Z}=\pmb{X}^\top\pmb{W}^*=\pmb{U\Sigma V}^\top\pmb{W}^*\approx\pmb{U\Sigma}
\]</div>
</div>
<ul class="simple">
<li><p>示例</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="n">n_samples</span><span class="p">,</span><span class="n">n_features</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="o">-</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">scatter_matrix</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="n">X</span><span class="p">)</span>
    <span class="n">eig_val</span><span class="p">,</span><span class="n">eig_vec</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">scatter_matrix</span><span class="p">)</span>
    <span class="n">eig_pairs</span><span class="o">=</span><span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">eig_val</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span><span class="n">eig_vec</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">)]</span>
    <span class="n">eig_pairs</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">features</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ele</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">ele</span> <span class="ow">in</span> <span class="n">eig_pairs</span><span class="p">[:</span><span class="n">k</span><span class="p">]])</span>
    <span class="n">features</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">features</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">,</span><span class="n">features</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pca_svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="o">-</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">U</span><span class="p">,</span><span class="n">S</span><span class="p">,</span><span class="n">Vt</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">W</span><span class="o">=</span><span class="n">Vt</span><span class="o">.</span><span class="n">T</span><span class="p">[:,:</span><span class="n">k</span><span class="p">]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">W</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">,</span><span class="n">W</span>
<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span> 
    <span class="n">X_new</span><span class="p">,</span><span class="n">features</span><span class="o">=</span><span class="n">pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">X_new_svd</span><span class="p">,</span><span class="n">f_svd</span><span class="o">=</span><span class="n">pca_svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;X_eig_decom&#39;</span><span class="p">,</span><span class="n">X_new</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;X_svd&#39;</span><span class="p">,</span><span class="n">X_new_svd</span><span class="p">)</span>
    
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>    
        <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">features</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span>    
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;ro&#39;</span><span class="p">)</span><span class="c1">#,c = &#39;r&#39;,marker = &#39;o&#39;)</span>
        <span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">x1</span><span class="o">=</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">b</span><span class="o">/</span><span class="n">a</span><span class="p">))</span><span class="c1">#features[1,0]/features[0,0]))</span>
        <span class="n">y1</span><span class="o">=</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">b</span><span class="o">/</span><span class="n">a</span><span class="p">))</span><span class="c1">#features[1,0]/features[0,0]))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">,</span><span class="s1">&#39;b--&#39;</span><span class="p">)</span>    
    
        <span class="n">proj_dir</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">])</span><span class="c1">#features[0,0],features[1,0]])</span>
        <span class="n">proj_dir</span><span class="o">=</span><span class="n">proj_dir</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">proj_dir</span><span class="p">)</span>
        <span class="c1">#计算投影</span>
        <span class="n">PX</span><span class="o">=</span><span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
            <span class="n">p</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">proj_dir</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">proj_dir</span><span class="p">)</span>
            <span class="n">px</span><span class="o">=</span><span class="n">p</span><span class="o">*</span><span class="n">proj_dir</span>
            <span class="n">PX</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">px</span><span class="p">)</span>
            <span class="c1">#px=p*np.cos(np.arctan(features[1,0]/features[0,0]))</span>
            <span class="c1">#py=p*np.sin(np.arctan(features[1,0]/features[0,0]))</span>
            <span class="c1">#PX.append([px,py])</span>
        <span class="n">PX</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">PX</span><span class="p">)</span>  
        <span class="k">for</span> <span class="n">ix</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;s&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">0</span><span class="p">]],[</span><span class="n">X</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span><span class="s1">&#39;y:&#39;</span><span class="p">)</span>       
        
   
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="id7">
<h3>核主成分分析<a class="headerlink" href="#id7" title="Link to this heading">¶</a></h3>
<p>  前面我们通过计算样本协方差矩阵<span class="math notranslate nohighlight">\(\pmb{XX}^\top\)</span>的特征向量组成投影矩阵来实现PCA。对于核函数的隐式映射<span class="math notranslate nohighlight">\(\phi :\pmb{x}\rightarrow \phi(\pmb{x})\)</span>形成的映射数据矩阵<span class="math notranslate nohighlight">\(\pmb{\Phi}^\top\)</span>，如何计算PCA。也就是映射后的协方差矩阵<span class="math notranslate nohighlight">\(\pmb{\Phi\Phi}^\top\)</span>如何分解出特征向量组成投影矩阵？针对这一问题，研究人员提出了核主成分分析(kernel PCA)。</p>
<p>  <strong>首先考查协方差矩阵<span class="math notranslate nohighlight">\(m\pmb{C}\triangleq\pmb{X}^\top\pmb{X}\)</span>与核矩阵<span class="math notranslate nohighlight">\(\pmb{K}\triangleq\pmb{XX}^\top\)</span>特征向量之间的关系</strong>。
对实对称矩阵<span class="math notranslate nohighlight">\(\pmb{X}^\top\pmb{X}\)</span>进行特征值分解<span class="math notranslate nohighlight">\(\pmb{X}^\top\pmb{X}\pmb{U}=\pmb{U\Lambda}\)</span>，等式两边同时乘上<span class="math notranslate nohighlight">\(\pmb{X}\)</span>，则可以得到，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
(\pmb{XX}^\top)(\pmb{XU})=(\pmb{XU})\pmb{\Lambda}
\]</div>
</div>
<p>从上式可以得到<span class="math notranslate nohighlight">\(\pmb{XX}^\top\)</span>的特征向量为<span class="math notranslate nohighlight">\(\pmb{V}\triangleq\pmb{XU}\)</span>，特征值对角矩阵为<span class="math notranslate nohighlight">\(\pmb{\Lambda}\)</span>。注意到特征向量的模长，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\Vert \pmb{v}_j\Vert^2=\pmb{u}_j^\top\pmb{X}^\top\pmb{X}\pmb{u}_j=\pmb{u}_j^\top\pmb{u}_j\lambda_j\pmb{u}_j^\top\pmb{u}_j=\lambda_j
\]</div>
</div>
<p>可以得到单位化的特征向量矩阵<span class="math notranslate nohighlight">\(\pmb{V}_{\textrm{pca}}=(\pmb{XU})\pmb{\Lambda}^{-1/2}\)</span>。</p>
<p>  <strong>现在考虑Gram矩阵<span class="math notranslate nohighlight">\(\pmb{K}\triangleq\pmb{X}^\top\pmb{X}\)</span></strong>。根据Mercer定理，当使用一个核函数时，隐含了一个潜在的特征空间，因此，可以将<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>表示为<span class="math notranslate nohighlight">\(\pmb{\phi}_i\triangleq\phi(\pmb{x}_i)\)</span>。相应地，数据矩阵<span class="math notranslate nohighlight">\(\pmb{X}^\top\)</span>映射为<span class="math notranslate nohighlight">\(\pmb{\Phi}^\top\)</span>，协方差矩阵<span class="math notranslate nohighlight">\(\pmb{X}\pmb{X}^\top\)</span>映射为<span class="math notranslate nohighlight">\(\pmb{\Phi}\pmb{\Phi}^\top\)</span>。由<span class="math notranslate nohighlight">\(\pmb{X}^\top\pmb{X}\)</span>与<span class="math notranslate nohighlight">\(\pmb{XX}^\top\)</span>的关系可知，<span class="math notranslate nohighlight">\(\pmb{\Phi}\pmb{\Phi}^\top\)</span>的特征向量矩阵为</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\pmb{V}_{\textrm{kpca}}=\pmb{\Phi U\Lambda}^{-1/2}\]</div>
</div>
<p>其中<span class="math notranslate nohighlight">\(\pmb{U\Lambda}\)</span>分别为<span class="math notranslate nohighlight">\(\pmb{K}=\pmb{\Phi}^\top\pmb{\Phi}\)</span>的特征向量矩阵以及对应的特征值。</p>
<p>  根据上面计算的结果，从特征向量矩阵中取<span class="math notranslate nohighlight">\(k\)</span>个特征向量即可组成投影矩阵，经过数据投影即可得到样本的<span class="math notranslate nohighlight">\(k\)</span>维压缩表示。<strong>但是</strong>，映射<span class="math notranslate nohighlight">\(\phi()\)</span>可能没有显示表示，或难以直接计算。<strong>解决办法是使用核函数间接计算<span class="math notranslate nohighlight">\(\phi()\)</span></strong>。任意给定样本<span class="math notranslate nohighlight">\(\pmb{x}_*\)</span>，则其在特征空间的投影<span class="math notranslate nohighlight">\(\hat{\pmb{x}}_i\)</span>可通过以下方式计算。</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\hat{\pmb{x}}_i=\phi(\pmb{x}_*)^\top\pmb{V}_{\textrm{kpca}}=\phi(\pmb{x}_*)^\top\pmb{\Phi U\Lambda}^{-1/2}=\pmb{k}_*^{\top}\pmb{U\Lambda}^{-1/2}
\]</div>
</div>
<p>  最后要注意的是<span class="math notranslate nohighlight">\(\pmb{K}\)</span>在特征值分解之前，需要中心化。中心化可通过以下步骤计算得到。</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\tilde{\pmb{K}}&amp;=\pmb{K}-\frac1N\pmb{K11}^\top-\frac1N\pmb{11}^\top\pmb{K}+\frac{1}{N^2}(\pmb{1}^\top\pmb{K}\pmb{1})\pmb{11}^\top\\
&amp;=\pmb{K}-\pmb{KO}-\pmb{{OK}}+\pmb{OKO}
\end{split}
\end{split}\]</div>
</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{O}=\frac1N \pmb{1}\pmb{1}^\top, \pmb{1}=[1,1,...,1]_{1\times N}^\top\)</span>。</p>
<ul class="simple">
<li><p>示例</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">kpca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; </span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : np.array with n x d</span>
<span class="sd">    k : int rank of the low-dimension</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    data : projection data</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span><span class="p">,</span><span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Dimensions of output data has to be lesser than the dimensions of input data</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span>
    
    <span class="c1"># construct K</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">k_ij</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span><span class="p">[</span><span class="n">row</span><span class="p">,:]</span><span class="o">-</span><span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">,:])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">K</span><span class="p">[</span><span class="n">row</span><span class="p">,</span><span class="n">col</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">k_ij</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">+</span><span class="n">K</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">K</span><span class="p">[</span><span class="n">row</span><span class="p">,</span><span class="n">row</span><span class="p">]</span><span class="o">=</span><span class="n">K</span><span class="p">[</span><span class="n">row</span><span class="p">,</span><span class="n">row</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span>
        
    <span class="c1"># normalize K</span>
    <span class="n">all1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span><span class="o">/</span><span class="n">n</span>
    <span class="n">K_center</span> <span class="o">=</span> <span class="n">K</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">all1</span><span class="p">,</span><span class="n">K</span><span class="p">)</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">all1</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">all1</span><span class="p">,</span><span class="n">K</span><span class="p">),</span><span class="n">all1</span><span class="p">)</span>
    
    <span class="c1"># eigvector</span>
    <span class="n">S</span><span class="p">,</span><span class="n">U</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">K_center</span><span class="p">)</span>      
    <span class="n">V</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">S</span><span class="p">))))</span>
    
    <span class="n">eig_pairs</span><span class="o">=</span><span class="p">[(</span><span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">V</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">))]</span>
    <span class="n">eig_pairs</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(([</span><span class="n">ele</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">ele</span> <span class="ow">in</span> <span class="n">eig_pairs</span><span class="p">[:</span><span class="n">k</span><span class="p">]]))</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span>
    <span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_center</span><span class="p">,</span><span class="n">V</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="id8">
<h2>多维缩放<a class="headerlink" href="#id8" title="Link to this heading">¶</a></h2>
<p>  多维缩放(multiple dimensional scaling, MDS)的主要思想是原始空间中样本之间的距离在低维空间得以保持。假设<span class="math notranslate nohighlight">\(m\)</span>个样本在原始空间的距离矩阵为<span class="math notranslate nohighlight">\(\pmb{D}\subseteq \mathbb{R}^{m\times m}\)</span>。MDS的任务是获得样本集在<span class="math notranslate nohighlight">\(d'\)</span>维空间的表示<span class="math notranslate nohighlight">\(\pmb{Z}\in \mathbb{R}^{m\times d'}\)</span>，且任意两个样本在<span class="math notranslate nohighlight">\(d'\)</span>维空间的欧式距离等于原始空间的距离，即<span class="math notranslate nohighlight">\(\parallel \pmb{z}_i-\pmb{z}_j\parallel^2=\)</span><span class="math notranslate nohighlight">\(D_{ij}, \forall 0&lt;i,j\leq m\)</span>。</p>
<ul class="simple">
<li><p>MDS的求解</p></li>
</ul>
<p>  令<span class="math notranslate nohighlight">\(\pmb{B}=\pmb{Z}^\top \pmb{Z}\in\mathbb{R}^{m\times m}\)</span>为降维后的样本内积矩阵，<span class="math notranslate nohighlight">\(b_{ij}=\pmb{z}_i^\top\pmb{z}_j\)</span>,则有，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
dist_{ij}^2&amp;=\parallel z_i \parallel^2+\parallel z_j\parallel^2-2z_i^Tz_j\\
&amp;=b_{ii}+b_{jj}-2b_{ij}\\
\end{split}
\end{split}\]</div>
</div>
<p>假设<span class="math notranslate nohighlight">\(\pmb{Z}\)</span>已中心化，即<span class="math notranslate nohighlight">\(\sum_{i=1}^m\pmb{z}_i=0\)</span>，显然<span class="math notranslate nohighlight">\(\sum_{i=1}^mb_{ij}=\sum_{j=1}^mb_{ij}=0\)</span>，由此可知，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\sum_{i=1}^mdist_{ij}^2&amp;=\text{tr}(B)+mb_{jj}\\
\sum_{j=1}^mdist_{ij}^2&amp;=\text{tr}(B)+mb_{ii}\\
\sum_{i=1}^m\sum_{j=1}^mdist_{ij}^2&amp;=2m\cdot\text{tr}(B)\\
\end{split}
\end{split}\]</div>
</div>
<p>令,</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
dist_{i\cdot}^2&amp;=\frac{1}{m}\sum_{j=1}^mdist_{ij}^2\\
dist_{\cdot j}^2&amp;=\frac{1}{m}\sum_{i=1}^mdist_{ij}^2\\
dist_{\cdot\cdot}^2&amp;=\frac{1}{m^2}\sum_{i=1}\sum_{j=1}^mdist_{ij}^2\\
\end{split}
\end{split}\]</div>
</div>
<p>最终可得，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
b_{ij}=-\frac{1}{2}(dist_{ij}^2-dist_{i\cdot}^2-dist_{\cdot j}^2+dist_{\cdot\cdot}^2)
\]</div>
</div>
<p>其中，<span class="math notranslate nohighlight">\(dist_{ij}=D_{ij}\)</span>。由此，可以根据降维前的距离矩阵<span class="math notranslate nohighlight">\(\pmb{D}\)</span>求得降维后距离不变的矩阵<span class="math notranslate nohighlight">\(\pmb{B}\)</span>。令<span class="math notranslate nohighlight">\(\pmb{C}_n=\pmb{I}_n-\frac1n\pmb{11}^\top\)</span>为中心化矩阵(Centering matrix)，则，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\pmb{B}=-\frac12\pmb{C}_n\pmb{D}\pmb{C}_n
\]</div>
</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{X}\pmb{C}_n\)</span>相当于对<span class="math notranslate nohighlight">\(\pmb{X}\)</span>的所有行向量减去行向量均值；<span class="math notranslate nohighlight">\(\pmb{C}_n\pmb{X}\)</span>相当于对<span class="math notranslate nohighlight">\(\pmb{X}\)</span>的所有列向量减去列向量均值；</p>
<ul class="simple">
<li><p>获得降维后的样本投影矩阵<span class="math notranslate nohighlight">\(\pmb{Z}\)</span></p></li>
</ul>
<p>  对矩阵<span class="math notranslate nohighlight">\(\pmb{B}\)</span>（实对称矩阵）做特征值分解，<span class="math notranslate nohighlight">\(\pmb{B}=\pmb{V\Lambda V}^\top\)</span>。假设有<span class="math notranslate nohighlight">\(d_*\)</span>个非零特征值构成对角矩阵<span class="math notranslate nohighlight">\(\pmb{\Lambda}_*=\textrm{diag}(\lambda_1,\lambda_2,...,\lambda_{d_*})\)</span>,以及所对应的特征向量矩阵<span class="math notranslate nohighlight">\(\pmb{V}_*\)</span>，则<span class="math notranslate nohighlight">\(\pmb{Z}\)</span>可以表示为，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\pmb{Z}=\pmb{\Lambda}_*^{\frac{1}{2}}\pmb{V}_*^\top \in \mathbb{R}^{m\times d_*}
\]</div>
</div>
<p>  现实应用中，可以选择<span class="math notranslate nohighlight">\(d'&lt;d\)</span>个最大特征值构成的对角阵<span class="math notranslate nohighlight">\(\hat{\pmb{\Lambda}}\)</span>及特征向量矩阵<span class="math notranslate nohighlight">\(\hat{\pmb{V}}\)</span>，即</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\pmb{Z}=\hat{\pmb{\Lambda}}^{\frac{1}{2}} \hat{\pmb{V}}^\top \in \mathbb{R}^{m\times d'}
\]</div>
</div>
<hr class="docutils" />
<section id="id9">
<h3>算法<a class="headerlink" href="#id9" title="Link to this heading">¶</a></h3>
<p>  <strong>输入</strong>：距离矩阵<span class="math notranslate nohighlight">\(\pmb{D}\)</span>，低维空间维数<span class="math notranslate nohighlight">\(d'\)</span>.</p>
<p>  <strong>过程</strong>：</p>
<p>    1. 计算<span class="math notranslate nohighlight">\(\pmb{D}\)</span>;</p>
<p>    2. 计算矩阵<span class="math notranslate nohighlight">\(\pmb{B}\)</span>;</p>
<p>    3. 矩阵<span class="math notranslate nohighlight">\(\pmb{B}\)</span>做特征值分解；</p>
<p>    4. 选取<span class="math notranslate nohighlight">\(\hat{\pmb{V}},\hat{\pmb{\Lambda}}\)</span>；</p>
<p>  <strong>输出</strong>： 矩阵<span class="math notranslate nohighlight">\(\hat{\pmb{V}}\hat{\pmb{\Lambda}}^{1/2}\)</span>每一行即为一个样本的低维坐标。</p>
<ul class="simple">
<li><p>示例代码</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>   <span class="c1"># 与MDS进行对比</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.manifold</span><span class="w"> </span><span class="kn">import</span> <span class="n">MDS</span>
    
<span class="n">ris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">new_X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">new_X_pca</span> <span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">new_X_pca</span> <span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">mds</span> <span class="o">=</span> <span class="n">MDS</span><span class="p">(</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">new_X_mds</span> <span class="o">=</span> <span class="n">mds</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">new_X_mds</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">new_X_mds</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="id10">
<h2>等度量映射<a class="headerlink" href="#id10" title="Link to this heading">¶</a></h2>
<p>  等度量映射(Isometric Mapping, Isomap)的基本出发点在于，Isomap认为低维流行嵌入到高维空间之后，直接在高维空间计算直线距离具有误导性，因为高维空间的直线距离在低维流行是不可达的（如：瑞士卷上两个点（位于同一<span class="math notranslate nohighlight">\(x,y\)</span>坐标，<span class="math notranslate nohighlight">\(z\)</span>不同坐标）是不能用直线距离来计算的，因为该流行是扭曲过的）。</p>
<p>  所谓<span class="math notranslate nohighlight">\(d\)</span>维流形是<span class="math notranslate nohighlight">\(n\)</span>维空间<span class="math notranslate nohighlight">\((d&lt;n)\)</span>的一部分，局部类似于<span class="math notranslate nohighlight">\(d\)</span>维超平面。例如：2D流形是一个2D形状，该形状可以在更高维的空间中弯曲和扭曲。<strong>流形学习</strong>通过训练实例所在的流形进行建模。流形学习基于流行假设，即大多数现实世界的高维数据集都接近于低维流形。如：三维空间的球面，其实可以只用经度和纬度两个特征来表示。低维嵌入流形上的本真距离（即测地线距离，如：北京至上海的距离（地球是圆的，直线距离要穿过地下层））不能用高维空间的直线距离来计算，但能用近邻距离来近似。</p>
<p>  如何计算测地线距离呢？利用流形在局部与欧氏空间同胚这个性质，对每个样本点基于欧氏距离找出其近邻点，建立一个近邻连接图。于是，计算两点之间的测地线距离的问题就转变为计算近邻连接图上两点之间最短路径的问题。近邻图计算两点之间的最短路径，可以采用Dijkstra算法或Floyd算法，在得到任意两点的距离之后，就可以用多维缩放(MDS)方法来获得样本点在低维空间的坐标。</p>
<p>  它的<strong>核心思想</strong>是沿着图的边移动的距离近似于沿着流形移动的距离。</p>
<hr class="docutils" />
<section id="id11">
<h3>算法<a class="headerlink" href="#id11" title="Link to this heading">¶</a></h3>
<p><strong>输入</strong>：样本集<span class="math notranslate nohighlight">\(\mathcal{D}=\{\pmb{x}_1,\pmb{x}_2,...,\pmb{x}_m\}\)</span>，低维空间维数<span class="math notranslate nohighlight">\(d'\)</span>.</p>
<p><strong>过程</strong>：</p>
<ol class="arabic simple">
<li><p>确定每个样本<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>的<span class="math notranslate nohighlight">\(k\)</span>近邻;</p></li>
<li><p>使用最短路径算法(例如：Dijkstra)计算<span class="math notranslate nohighlight">\(k\)</span>近邻图的任意样本间距离<span class="math notranslate nohighlight">\(dist(\pmb{x}_i,\pmb{x}_j)\)</span>;</p></li>
</ol>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
 dist(\pmb{x}_i,\pmb{x}_j)=\left\{\begin{array}{ll} dist(\pmb{x}_i,\pmb{x}_j), &amp; \pmb{x}_j\textrm{ is a nearest neighbor of }\pmb{x}_i\\ \infty, &amp; \textrm{otherwise.}\end{array}\right.
\end{split}\]</div>
</div>
<ol class="arabic simple" start="3">
<li><p>以<span class="math notranslate nohighlight">\(dist(\pmb{x}_i,\pmb{x}_j)\)</span>为输入，使用MDS计算低维坐标；</p></li>
</ol>
<p><strong>输出</strong>： MDS计算的低维坐标。</p>
<ul class="simple">
<li><p>示例</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.manifold</span><span class="w"> </span><span class="kn">import</span> <span class="n">Isomap</span>

<span class="n">iris</span><span class="o">=</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span><span class="n">neighbor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">]):</span>
    <span class="n">isomap</span><span class="o">=</span><span class="n">Isomap</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="n">neighbor</span><span class="p">)</span>
    <span class="n">X_new</span><span class="o">=</span><span class="n">isomap</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X_new</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Isomap(n_neighbors=</span><span class="si">%d</span><span class="s2">)&quot;</span><span class="o">%</span><span class="n">neighbor</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="id12">
<h3>流形<a class="headerlink" href="#id12" title="Link to this heading">¶</a></h3>
<p>  在介绍流形前，需要一些有关的背景知识。</p>
<ul class="simple">
<li><p><strong>拓扑空间</strong></p></li>
</ul>
<p>  给定集合<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>，以及<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>的一些子集构成的族<span class="math notranslate nohighlight">\(\mathcal{O}\)</span>，如果以下性质成立，则<span class="math notranslate nohighlight">\((\mathcal{X},\mathcal{O})\)</span>称为一个拓扑空间：</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\emptyset\)</span>和<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>都属于<span class="math notranslate nohighlight">\(\mathcal{O}\)</span>；</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{O}\)</span>中的任意多个元素的并仍属于<span class="math notranslate nohighlight">\(\mathcal{O}\)</span>；</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{O}\)</span>中的任意多个元素的交仍属于<span class="math notranslate nohighlight">\(\mathcal{O}\)</span>；</p></li>
</ol>
<p>此时，<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>中的元素称为点，<span class="math notranslate nohighlight">\(\mathcal{O}\)</span>中的元素称为开集（可以理解为开区间）。</p>
<ul class="simple">
<li><p><strong>度量空间</strong></p></li>
</ul>
<p>  度量空间是一个二元对<span class="math notranslate nohighlight">\((\mathcal{M},d)\)</span>，其中<span class="math notranslate nohighlight">\(\mathcal{M}\)</span>是一个集合，<span class="math notranslate nohighlight">\(d\)</span>是定义在<span class="math notranslate nohighlight">\(\mathcal{M}\)</span>上的一个度量，即映射<span class="math notranslate nohighlight">\(d: \mathcal{M}\times \mathcal{M}\rightarrow \mathbb{R}\)</span>，对于任意<span class="math notranslate nohighlight">\(\pmb{x,y,z}\in M\)</span>满足以下条件：</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(d(\pmb{x,y})=0 \Leftrightarrow \pmb{x}=\pmb{y}\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(d(\pmb{x,y})=d(\pmb{y,x})\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(d(\pmb{x,z})\le d(\pmb{x,y})+d(\pmb{y,z})\)</span>;</p></li>
</ol>
<ul class="simple">
<li><p><strong>流形</strong></p></li>
</ul>
<p>  流形是一个拓扑空间，对于每个点，其周围的邻域局部类似于欧几里得空间。更确切地说，<span class="math notranslate nohighlight">\(n\)</span>维流形的每个点都有一个邻域开集，该邻域与<span class="math notranslate nohighlight">\(n\)</span>维欧几里德空间的邻域开集同胚。人们经常可以想象拉伸或平坦流形的局部邻域以得到一个平坦的欧几里得平面。大致地说，拓扑空间是一个几何物体，同胚就是把物体连续延展和弯曲，使其成为一个新的物体。因此，正方形和圆是同胚的，但球面和环面就不是。</p>
<p>  在拓扑学中，同胚(homeomorphism、topological isomorphism、bi continuous function)是两个拓扑空间之间的<strong>双连续函数</strong>。同胚是拓扑空间范畴中的同构；也就是说，它们是保持给定空间的所有拓扑性质的映射。如果两个空间之间存在同胚，那么这两个空间就称为同胚的，从拓扑学的观点来看，两个空间是相同的。</p>
<p>  一个较大的<span class="math notranslate nohighlight">\(m\)</span>维空间(<span class="math notranslate nohighlight">\(n&lt;m\)</span>)中的<span class="math notranslate nohighlight">\(n\)</span>维流形
)局部类似于<span class="math notranslate nohighlight">\(n\)</span>维欧几里得超平面。例如</p>
<ol class="arabic simple">
<li><p>1维流形：圆，正方形，曲线等。但8字形不是1维流形，因为8字的中心点局部是2维的欧氏空间同胚。</p></li>
<li><p>2维流形：球面，环面等。</p></li>
</ol>
<p>  由于流形结构是由“局部”类似于欧几里得空间的性质定义的，我们不必考虑任何全局的、外部定义的坐标系的几何关系，相反，我们可以只考虑流形的内在几何和拓扑性质。</p>
</section>
</section>
<hr class="docutils" />
<section id="lle">
<h2>LLE局部线性嵌入<a class="headerlink" href="#lle" title="Link to this heading">¶</a></h2>
<section id="id13">
<h3>LLE基本思想<a class="headerlink" href="#id13" title="Link to this heading">¶</a></h3>
<p>  Isomap试图<strong>保持</strong>局部近邻样本之间的<strong>距离</strong>。LLE则试图<strong>保持</strong>局部邻域内样本之间的<strong>线性关系</strong>。假设样本<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>的坐标可由邻居样本<span class="math notranslate nohighlight">\(\pmb{x}_j,\pmb{x}_k,\pmb{x}_l\)</span>的坐标通过线性组合重构出来，即，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\pmb{x}_i=w_{ij}\pmb{x}_j+w_{ik}\pmb{x}_k+w_{il}\pmb{x}_l
\]</div>
</div>
<p>则LLE希望此关系在低维空间依旧能得以保持。</p>
</section>
<hr class="docutils" />
<section id="id14">
<h3>LLE求解<a class="headerlink" href="#id14" title="Link to this heading">¶</a></h3>
<p>  <strong>Step 1</strong>. 寻找样本<span class="math notranslate nohighlight">\(\forall \pmb{x}_i\in \mathcal{X}\)</span>的<span class="math notranslate nohighlight">\(k\)</span>个近邻。</p>
<p>  <strong>Step 2</strong>. 求解重构系数矩阵<span class="math notranslate nohighlight">\(\pmb{W}\)</span>。</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\min_W \quad &amp;\varepsilon(\pmb{W})= \sum_{i=1}^m \left\lVert \pmb{x}_i-\sum_{j\in \mathcal{N}_i} w_{ij}\pmb{x}_j \right\rVert^2\\
\text{s.t.}\quad &amp;\sum_{j\in \mathcal{N}_i} w_{ij}=1
\end{split}
\end{split}\]</div>
</div>
<p>令<span class="math notranslate nohighlight">\(\varepsilon_i=\left\lVert \pmb{x}_i-\sum_{j\in \mathcal{N}_i} w_{ij}\pmb{x}_j \right\rVert^2\)</span>，则有，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\varepsilon_i&amp;=\left\lVert \pmb{x}_i-\sum_{j\in \mathcal{N}_i} w_{ij}\pmb{x}_j \right\rVert^2\\
&amp;=\left\lVert (w_{i1}+w_{i2}+\cdots+w_{ik})\pmb{x}_i-\sum_{j\in \mathcal{N}_i} w_{ij}\pmb{x}_j \right\rVert^2\quad \textrm{(sum of weights equals to 1)}\\
&amp;=\left\lVert \sum_{j\in \mathcal{N}_i} w_{ij}\pmb{x}_i-\sum_{j\in \mathcal{N}_i} w_{ij}\pmb{x}_j \right\rVert^2\\
&amp;=\left\lVert \sum_{j\in \mathcal{N}_i} w_{ij}(\pmb{x}_i-\pmb{x}_j) \right\rVert^2
\end{split}
\end{split}\]</div>
</div>
<p>令<span class="math notranslate nohighlight">\(C_{jk}=(\pmb{x}_i-\pmb{x}_j)^T(\pmb{x}_i-\pmb{x}_k)\)</span>，则<span class="math notranslate nohighlight">\(w_{ij}\)</span>有闭式解，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
w_{ij}=\frac{\sum_{k\in\mathcal{N}_i} C_{jk}^{-1} }{ \sum_{l,s\in\mathcal{N}_i} C_{ls}^{-1} }
\]</div>
</div>
<p>  <strong>Step 3</strong>. 恢复低维空间坐标。LLE在低维空间保持<span class="math notranslate nohighlight">\(\pmb{W}\)</span>不变，于是<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>对应的低维空间坐标<span class="math notranslate nohighlight">\(\pmb{z}_i\)</span>可以通过下式求解获得，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\min\limits_{\pmb{Z}}\quad \sum_{i=1}^m \left\lVert \pmb{z}_i-\sum_{j\in \mathcal{N}_i}w_{ij}\pmb{z}_j\right\rVert^2
\]</div>
</div>
<p>  上述两优化问题目标同形，唯一区别在于前一个问题要确定<span class="math notranslate nohighlight">\(\pmb{W}\)</span>，而后一个需要确定<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>所对应的低维坐标<span class="math notranslate nohighlight">\(\pmb{z}_i\)</span>。</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\varepsilon(\pmb{Z})&amp;=\sum_{i=1}^n \left\Vert \pmb{z}_i-\sum_j w_{ij}\pmb{z}_j \right\Vert^2\\
\end{split}\]</div>
</div>
<p>令,</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\pmb{W}=\begin{bmatrix}-&amp;\pmb{w}_1&amp;-\\-&amp;\pmb{w}_2&amp;-\\ &amp;\vdots &amp;\\ -&amp;\pmb{w}_n&amp;- \end{bmatrix} \quad \pmb{Z}=\begin{bmatrix}|&amp;|&amp;\cdots &amp;|\\\pmb{z}_1&amp;\pmb{z}_2&amp;\cdots&amp;\pmb{z}_n\\ |&amp;|&amp;\cdots &amp;|\\ \end{bmatrix}
\end{split}\]</div>
</div>
<p>则有，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\varepsilon(\pmb{Z})&amp;=\Vert \pmb{Z}^\top -\pmb{WZ}^\top\Vert_F^2\\
&amp;=\Vert (\pmb{I}-\pmb{W})\pmb{Z}^\top\Vert_F^2\\
&amp;=\textrm{tr}(\pmb{Z}\pmb{M}\pmb{Z}^\top)
\end{split}
\end{split}\]</div>
</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{M}=(\pmb{I}-\pmb{W})^\top(\pmb{I}-\pmb{W})\)</span>。则问题可重写为，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\min_\pmb{Z}\quad &amp;\text{tr}(\pmb{ZMZ}^\top)\\
\text{s.t.}\quad &amp;\pmb{ZZ}^\top=\pmb{I}
\end{split}
\end{split}\]</div>
</div>
<p>该问题可以通过特征值分解求得<span class="math notranslate nohighlight">\(\pmb{M}\)</span>的最大<span class="math notranslate nohighlight">\(d'\)</span>个特征值对应的特征向量组成的矩阵即为<span class="math notranslate nohighlight">\(\pmb{Z}^\top\)</span>。</p>
</section>
</section>
<hr class="docutils" />
<section id="id15">
<h2>拉普拉斯特征映射<a class="headerlink" href="#id15" title="Link to this heading">¶</a></h2>
<section id="id16">
<h3>拉普拉斯算子<a class="headerlink" href="#id16" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>连续型</strong></p></li>
</ul>
<p>  <strong>[定义]</strong>- 假设函数<span class="math notranslate nohighlight">\(f\)</span>连续二阶可微，则拉普拉斯算子(<span class="math notranslate nohighlight">\(\Delta f\)</span>)由下式给出，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\Delta f\triangleq\sum_{i=1}^n\frac{\partial^2f}{\partial x_i^2}=\nabla\cdot\nabla f=\textrm{div}(\textrm{grad}f)
\]</div>
</div>
<p>其中，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\textrm{grad} f&amp;=\nabla f=\left(\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},...,\frac{\partial f}{\partial x_n}\right)\\
\textrm{div}(\textrm{grad}f)&amp;=\nabla\cdot\nabla f=\frac{\partial^2 f}{\partial x_1^2}+\frac{\partial^2 f}{\partial x_2^2}+...+\frac{\partial^2 f}{\partial x_n^2}
\end{split}
\end{split}\]</div>
</div>
<ul class="simple">
<li><p><strong>离散型</strong></p></li>
</ul>
<p>  在离散情况下，我们仍然希望拉普拉斯算子将输入的函数映射为其它函数。只不过，这种情况下，函数将被定义在离散的域上（如图<span class="math notranslate nohighlight">\(G\)</span>的有限顶点集<span class="math notranslate nohighlight">\(V\)</span>）。因此，我们可以将离散拉普拉斯算子<span class="math notranslate nohighlight">\((\Delta)\phi(v)\)</span>作用在函数 <span class="math notranslate nohighlight">\(\phi :V\rightarrow R\)</span>。而<span class="math notranslate nohighlight">\(\phi\)</span>是一个定义在图的顶点集上的一个函数。我们也使用有限差分作为导数的离散类比，因此，我们不是使用导数来比较连续域的局部区域，而是使用有限差分来比较离散图的局部邻域。</p>
<p>  对于一个定义在图<span class="math notranslate nohighlight">\(G\)</span>的顶点集的函数<span class="math notranslate nohighlight">\(\phi :V\rightarrow R\)</span>，离散拉普拉斯算子定义为，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
(\Delta\phi)(v_i)=\sum_{v_j\in \mathcal{N}(v_i)}W_{ij}[\phi(v_j)-\phi(v_i)]
\]</div>
</div>
<p>其中，<span class="math notranslate nohighlight">\(W_{ij}\)</span>为连接<span class="math notranslate nohighlight">\(v_i\)</span>和<span class="math notranslate nohighlight">\(v_j\)</span>的边<span class="math notranslate nohighlight">\(e_{ij}\)</span>的权值。</p>
<p>  与连续版本一样，当<span class="math notranslate nohighlight">\(\phi(v_i)\)</span>的值比其周围的邻居大时(极大值)，离散拉普拉斯值较小；当<span class="math notranslate nohighlight">\(\phi(v_i)\)</span>的值比其周围的邻居小时（极小值），离散拉普拉斯值较大。</p>
<ol class="arabic simple">
<li><p><strong>例：图像Laplacian算子</strong></p></li>
</ol>
<p>  图像是一种离散型的数据，图像上的Laplacian算子可以大致进行如下运算。</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\frac{\partial^2 f(x,y)}{\partial x^2}&amp;=f_x^{''}(x,y)\\
&amp;\approx f_x^{'}(x,y)-f_x^{'}(x-1,y)\\
&amp;\approx f(x+1,y)-f(x,y)-f(x,y)+f(x-1,y)\\
&amp;=f(x+1,y)+f(x-1,y)-2f(x,y)
\end{split}
\end{split}\]</div>
</div>
<p>同理，<span class="math notranslate nohighlight">\(\frac{\partial^2 f(x,y)}{\partial y^2}=f(x,y+1)+f(x,y-1)-2f(x,y)\)</span>。因此有Laplacian值，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\Delta f(x,y)=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)
\]</div>
</div>
<p>  可以得出<strong>结论</strong>：Laplacian算子近似等于所有方向（自由度）差分累积（增益）。</p>
<ol class="arabic simple" start="2">
<li><p><strong>例：图拉普拉斯算子</strong></p></li>
</ol>
<p>  图数据上的Laplacian算子又该如何应用呢？图由<span class="math notranslate nohighlight">\(N\)</span>个结点及其连接边权值<span class="math notranslate nohighlight">\(W\)</span>所构成。和图像Laplacian算子类似，图Laplacian算子可以近似等于所有方向(<span class="math notranslate nohighlight">\(\mathcal{N}_i\)</span>个邻接结点)的差分累积。</p>
<p>  对于任意结点<span class="math notranslate nohighlight">\(i\)</span>，可以通过映射<span class="math notranslate nohighlight">\(f: V\rightarrow R\)</span>得到值<span class="math notranslate nohighlight">\(f_i\)</span>。显然结点<span class="math notranslate nohighlight">\(i\)</span>的Laplacian就等于其所有邻接点的差分累积，即</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
(\Delta f)_i = \sum_{j\in\mathcal{N}_i}W_{ij}(f_j-f_i)
\]</div>
</div>
<p>因为<span class="math notranslate nohighlight">\(j\notin\mathcal{N}_i,W_{ij}=0\)</span>，上式可继续简化，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
(\Delta f)_i&amp;=\sum_j W_{ij}f_j -\sum_j W_{ij}f_i\\
&amp;=(Wf)_i-(Df)_i\\
&amp;=[(W-D)f]_i
\end{split}
\end{split}\]</div>
</div>
<p>若<span class="math notranslate nohighlight">\(F: \mathbb{R}^d \rightarrow \mathbb{R}^p\)</span>,则有，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\Delta \pmb{F}&amp;=\sum_{ij}\Vert \pmb{f}_i-\pmb{f}_j\Vert^2W_{ij}\\
&amp;=\sum_{ij}\pmb{f}_i^\top\pmb{f}_iW_{ij}-2\sum_{ij}\pmb{f}_i^\top W_{ij}\pmb{f}_j+\sum_{ij}\pmb{f}_j^\top\pmb{f}_jW_{ij}\\
&amp;=2\left(\sum_i \pmb{f}_i^\top D_{ii}\pmb{f}_i)-\sum_{ij}\pmb{f}_i^\top W_{ij}\pmb{f}_j\right)\\
&amp;=2\textrm{tr}\left(\pmb{Y}^\top\pmb{L}\pmb{Y} \right)
\end{split}
\end{split}\]</div>
</div>
</section>
<hr class="docutils" />
<section id="id17">
<h3>拉普拉斯矩阵<a class="headerlink" href="#id17" title="Link to this heading">¶</a></h3>
<p>  离散Laplacian算子表示为一个矩阵时，映射函数<span class="math notranslate nohighlight">\(\phi\)</span>可以写为列向量，<span class="math notranslate nohighlight">\(\Delta \phi\)</span>则表示为Laplacian矩阵<span class="math notranslate nohighlight">\(\pmb{L}\)</span>与列向量的乘积，即</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\Delta\phi=\pmb{L}\times \phi
\]</div>
</div>
<p>Laplacian矩阵<span class="math notranslate nohighlight">\(\pmb{L}=\pmb{D}-\pmb{W}\)</span>。其中，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\pmb{D}_{ii}=\sum_j \pmb{W}_{ij}
\]</div>
</div>
</section>
<hr class="docutils" />
<section id="id18">
<h3>拉普拉斯变换<a class="headerlink" href="#id18" title="Link to this heading">¶</a></h3>
<p>  如果数据样本<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>与<span class="math notranslate nohighlight">\(\pmb{x}_j\)</span>很相似，则在拉普拉斯变换后的子空间与原空间一样，尽可能的接近。即，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\min\limits_{\pmb{Y}}\quad &amp;\textrm{tr}(\pmb{Y}^\top\pmb{L}\pmb{Y})\\ 
\textrm{s.t.}\quad &amp;\pmb{Y}^\top\pmb{DY}=\pmb{I}
\end{split}
\end{split}\]</div>
</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{L}\)</span>为拉普拉斯矩阵，<span class="math notranslate nohighlight">\(\phi:\pmb{x}\rightarrow\pmb{y},\pmb{x}\in\mathbb{R}^d, \pmb{y}\in\mathbb{R}^p\)</span>。</p>
<p>  使用Lagrangian乘子法，可得，Lagrangian函数</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
f(\pmb{Y})=\textrm{tr}(\pmb{Y}^\top\pmb{L}\pmb{Y})+ \textrm{tr}[\pmb{\Lambda}(\pmb{Y}^\top\pmb{DY}-\pmb{I})]
\]</div>
</div>
<p>对其求偏导数，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\frac{\partial f}{\partial \pmb{Y}}&amp;=2\pmb{LY}+\pmb{D}^\top\pmb{Y}\pmb{\Lambda}^\top+\pmb{DY\Lambda}\\
&amp;=2\pmb{LY}+2\pmb{DY\Lambda}
\end{split}
\end{split}\]</div>
</div>
<p>令<span class="math notranslate nohighlight">\(\frac{\partial f}{\partial \pmb{Y}}=0\)</span>，可得<span class="math notranslate nohighlight">\(\pmb{LY}=-\pmb{DY\Lambda}\)</span>，即</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\pmb{Ly}=\lambda\pmb{Dy}
\]</div>
</div>
<p>  因此，只需选择<span class="math notranslate nohighlight">\(p\)</span>个最小的特征值所对应的特征向量即可得到最优解<span class="math notranslate nohighlight">\(\hat{\pmb{Y}}\)</span>。</p>
</section>
</section>
<hr class="docutils" />
<section id="id19">
<h2>随机近邻嵌入<a class="headerlink" href="#id19" title="Link to this heading">¶</a></h2>
<section id="sne">
<h3>SNE<a class="headerlink" href="#sne" title="Link to this heading">¶</a></h3>
<p>  随机近邻嵌入(Stochastic Neighbor Embedding,SNE)的主要思想是：若两数据在高维空间相似(距离很近)，那么通过某种降维映射至2-3维空间时，它们应该离的很近。</p>
<p>  使用条件概率<span class="math notranslate nohighlight">\(p_{j|i}\)</span>来评价数据点<span class="math notranslate nohighlight">\(\pmb{x}_i,\pmb{x}_j\)</span>的相似性。高维空间的距离可以通过<strong>高斯分布</strong>进行近似计算，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\textrm{P}\{\textrm{高维空间}\pmb{x}_j是\pmb{x}_i的邻居\}\triangleq p_{j|i}=\frac{\exp(-\Vert\pmb{x}_i-\pmb{x}_j\Vert^2/(2\sigma_i^2))}{\sum_{k\neq i}\exp(-\Vert\pmb{x}_i-\pmb{x}_k\Vert^2/(2\sigma_i^2))}
\]</div>
</div>
<p>可以不用关心与自身的相似性，故可令<span class="math notranslate nohighlight">\(p_{i|i}=0\)</span>。假设经过低维映射<span class="math notranslate nohighlight">\(\phi\)</span>后<span class="math notranslate nohighlight">\(\pmb{x}_i\rightarrow \pmb{y}_i, \pmb{x}_j \rightarrow \pmb{y}_j\)</span>，则在该低维空间两点之间的相似性，即<span class="math notranslate nohighlight">\(\pmb{y}_j\)</span>是<span class="math notranslate nohighlight">\(\pmb{y}_i\)</span>邻居的条件概率<span class="math notranslate nohighlight">\(q_{j|i}\)</span>可以通过以下分布计算，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
q_{j|i}=\frac{\exp(-\Vert \pmb{y}_i-\pmb{y}_j \Vert^2)}{\sum_{k\neq i}\exp(-\Vert \pmb{y}_i-\pmb{y}_k \Vert^2)}
\]</div>
</div>
<p>为简化计算，令<span class="math notranslate nohighlight">\(\sigma_i=2^{-1/2}\)</span>，同样有<span class="math notranslate nohighlight">\(q_{i|i}=0\)</span>。</p>
<p>  为确保<span class="math notranslate nohighlight">\(p_{j|i}\)</span>与<span class="math notranslate nohighlight">\(q_{j|i}\)</span>尽可能一致，可以通过KL散度来评估两个条件分布的差异性。因此，目标函数就变成了尽可能的降低<span class="math notranslate nohighlight">\(p_{j|i},q_{j|i}\)</span>的散度，即使用<span class="math notranslate nohighlight">\(q_{j|i}\)</span>替代<span class="math notranslate nohighlight">\(p_{j|i}\)</span>的信息损失，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\mathcal{L}=\sum_i\sum_j p_{j|i}\log\frac{p_{j|i}}{q_{j|i}}
\]</div>
</div>
<p>  梯度法求解上述问题，对<span class="math notranslate nohighlight">\(\pmb{y}_i\)</span>求偏导，可得梯度</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \pmb{y}_i}=2\sum_j (\pmb{y}_i-\pmb{y}_j)(p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j})
\]</div>
</div>
<div style="background-color: #F8F8F8  ">
<p>  提示：使用链式法则求解。令<span class="math notranslate nohighlight">\(p_{ij}=p_{i|j}\)</span>, <span class="math notranslate nohighlight">\(q_{ij}=q_{i|j}\)</span>,以及，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
w_{ij}=\exp(-\Vert \pmb{y}_i-\pmb{y}_j \Vert^2),\quad f_{ij}= \Vert \pmb{y}_i-\pmb{y}_j \Vert^2, \quad d_{ij}=\Vert \pmb{y}_i-\pmb{y}_j \Vert
\]</div>
</div>
<p>由链式法则可知，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\begin{split}
\frac{\partial \mathcal{L}}{\partial \pmb{y}_n} &amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\sum_{kl}\frac{\partial q_{ij}}{w_{kl}}\sum_{mn}\frac{\partial w_{kl}}{\partial f_{mn}}\sum_{pq}\frac{\partial f_{mn}}{\partial d_{pq}}\frac{\partial d_{pq}}{\partial \pmb{y}_n}
\end{split}
\]</div>
</div>
<p>显然，只有当<span class="math notranslate nohighlight">\(pq=mn=kl\)</span>时，偏导数才不为0，因此，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\frac{\partial \mathcal{L}}{\partial \pmb{y}_n} &amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\sum_{kl}\frac{\partial q_{ij}}{w_{kl}}\sum_{mn}\frac{\partial w_{kl}}{\partial f_{mn}}\sum_{pq}\frac{\partial f_{mn}}{\partial d_{pq}}\frac{\partial d_{pq}}{\partial \pmb{y}_n}\\
&amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\sum_{kl}\frac{\partial q_{ij}}{w_{kl}} \frac{\partial w_{kl}}{\partial f_{kl}}\frac{\partial f_{kl}}{\partial d_{kl}}\frac{\partial d_{kl}}{\partial \pmb{y}_n}\\
&amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\sum_{l}\frac{\partial q_{ij}}{w_{il}} \frac{\partial w_{il}}{\partial f_{il}}\frac{\partial f_{il}}{\partial d_{il}}\frac{\partial d_{il}}{\partial \pmb{y}_n}\\
\end{split}
\end{split}\]</div>
</div>
<p>由于<span class="math notranslate nohighlight">\(w_{il}\)</span>涉及<span class="math notranslate nohighlight">\(\pmb{y}_n\)</span>分两种情况：<span class="math notranslate nohighlight">\(i=n\)</span>或<span class="math notranslate nohighlight">\(l=n\)</span>,因此，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\frac{\partial \mathcal{L}}{\partial \pmb{y}_n} 
&amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\sum_{l}\frac{\partial q_{ij}}{w_{il}} \frac{\partial w_{il}}{\partial f_{il}}\frac{\partial f_{il}}{\partial d_{il}}\frac{\partial d_{il}}{\partial \pmb{y}_n}\\
&amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\frac{\partial q_{ij}}{w_{in}} \frac{\partial w_{in}}{\partial f_{in}}\frac{\partial f_{in}}{\partial d_{in}}\frac{\partial d_{in}}{\partial \pmb{y}_n} +  \sum_{j}\frac{\partial \mathcal{L}}{\partial q_{nj}}\sum_{l}\frac{\partial q_{nj}}{w_{nl}} \frac{\partial w_{nl}}{\partial f_{nl}}\frac{\partial f_{nl}}{\partial d_{nl}}\frac{\partial d_{nl}}{\partial \pmb{y}_n}
\end{split}
\end{split}\]</div>
</div>
<p>整理：<span class="math notranslate nohighlight">\(n\rightarrow i\)</span>，左式：<span class="math notranslate nohighlight">\(i\rightarrow j, j\rightarrow k\)</span>, 右式： <span class="math notranslate nohighlight">\(j\rightarrow k, l\rightarrow j\)</span>, 以及<span class="math notranslate nohighlight">\(f_{ij}=f{ji}, d_{ij}=d_{ji}\)</span>，可得</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\frac{\partial \mathcal{L}}{\partial \pmb{y}_n} 
&amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\frac{\partial q_{ij}}{w_{in}} \frac{\partial w_{in}}{\partial f_{in}}\frac{\partial f_{in}}{\partial d_{in}}\frac{\partial d_{in}}{\partial \pmb{y}_n} +  \sum_{j}\frac{\partial \mathcal{L}}{\partial q_{nj}}\sum_{l}\frac{\partial q_{nj}}{w_{nl}} \frac{\partial w_{nl}}{\partial f_{nl}}\frac{\partial f_{nl}}{\partial d_{nl}}\frac{\partial d_{nl}}{\partial \pmb{y}_n}\\
&amp;=\sum_{jk}\frac{\partial \mathcal{L}}{\partial q_{jk}}\frac{\partial q_{jk}}{w_{ji}} \frac{\partial w_{ji}}{\partial f_{ji}}\frac{\partial f_{ji}}{\partial d_{ji}}\frac{\partial d_{ji}}{\partial \pmb{y}_i} + \sum_{jk}\frac{\partial \mathcal{L}}{\partial q_{ik}}\frac{\partial q_{ik}}{w_{ij}} \frac{\partial w_{ij}}{\partial f_{ij}}\frac{\partial f_{ij}}{\partial d_{ij}}\frac{\partial d_{ij}}{\partial \pmb{y}_i}\\
&amp;=\sum_j\left(   \underbrace{\sum_k \frac{\partial \mathcal{L}}{\partial q_{jk}}\frac{\partial q_{jk}}{w_{ji}} \frac{\partial w_{ji}}{\partial f_{ji}} }_{t_{ji}}   +  \underbrace{\sum_k \frac{\partial \mathcal{L}}{\partial q_{ik}}\frac{\partial q_{ik}}{w_{ij}} \frac{\partial w_{ij}}{\partial f_{ij}}}_{t_{ij}}  \right)\frac{\partial f_{ij}}{\partial d_{ij}}\frac{\partial d_{ij}}{\partial \pmb{y}_i}
\end{split}
\end{split}\]</div>
</div>
<p>  注意到，<span class="math notranslate nohighlight">\(q_{ij}=\frac{w_{ij}}{\sum_l w_{il}}\triangleq \frac{w_{ij}}{S_i}\)</span>，则</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\frac{\partial q_{ij}}{\partial w_{ij}}=\frac{1}{S_i}-\frac{q_{ij}}{S_i},\quad \frac{\partial q_{ik}}{\partial w_{ij}}=-\frac{q_{ik}}{S_i}
\]</div>
</div>
<p>代入<span class="math notranslate nohighlight">\(t_{ij}\)</span>，可得</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
t_{ij}&amp;=\sum_k \frac{\partial \mathcal{L}}{\partial q_{ik}}\frac{\partial q_{ik}}{w_{ij}} \frac{\partial w_{ij}}{\partial f_{ij}}\\
&amp;=\left( \frac{\partial \mathcal{L}}{\partial q_{ij}}\frac{\partial q_{ij}}{w_{ij}} +\sum_{k\neq j}\frac{\partial \mathcal{L}}{\partial q_{ik}}\frac{\partial q_{ik}}{w_{ij}} \right)\frac{\partial w_{ij}}{\partial f_{ij}}\\
&amp;=\left( \frac{1}{S_i} \frac{\partial \mathcal{L}}{\partial q_{ij}}  -\frac{q_{ij}}{S_i}\frac{\partial \mathcal{L}}{\partial q_{ij}} - \sum_{k\neq j}\frac{\partial \mathcal{L}}{\partial q_{ik}}\frac{q_{ik}}{S_i}        \right)\frac{\partial w_{ij}}{\partial f_{ij}}\\
&amp;=\frac{1}{S_i}\left(\frac{\partial \mathcal{L}}{\partial q_{ij}}  - \sum_{k}\frac{\partial \mathcal{L}}{\partial q_{ik}}q_{ik}  \right)\frac{\partial w_{ij}}{\partial f_{ij}}\\
&amp;=\frac{1}{S_i}\left(\frac{-p_{ij} }{q_{ij} }  - \sum_{k}\frac{-p_{ik} }{q_{ik} }q_{ik}  \right)(-w_{ij})\\
&amp;=-q_{ij}\left( \frac{-p_{ij} }{q_{ij} }+\sum_k p_{ik} \right)\\
&amp;=p_{ij}-q_{ij}
\end{split}
\end{split}\]</div>
</div>
<p>同理可求得<span class="math notranslate nohighlight">\(t_{ji}\)</span>，最后整理可得梯度为，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\boxed{\frac{\partial \mathcal{L}}{\partial \pmb{y}_i}=2\sum_j (\pmb{y}_i-\pmb{y}_j)(p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j})}
\]</div>
</div>
</div>
<ul class="simple">
<li><p>存在的问题</p></li>
</ul>
<ol class="arabic simple">
<li><p><strong>只关注数据的局部性，忽略了数据的全局性</strong>。对于目标函数来说，当<span class="math notranslate nohighlight">\(p_{j|i}\)</span>较大<span class="math notranslate nohighlight">\(q_{j|i}\)</span>较小时，代价较高；当<span class="math notranslate nohighlight">\(p_{j|i}\)</span>较小<span class="math notranslate nohighlight">\(q_{j|i}\)</span>较大时，代价较小；也就是说，高维空间数据点较近时，映射至低维空间后较远，那么会得到一个较高的惩罚，这是合理的。反之，高维空间较远的点映射后距离较近将会得到一个较低的惩罚，这就有问题了，这里应该得到一个较高的惩罚才合理。</p></li>
<li><p><strong>不对称性</strong>。<span class="math notranslate nohighlight">\(p_{j|i}\neq p_{i|j}, q_{j|i}\neq q_{i|j}\)</span>导致梯度计算量过大。</p></li>
<li><p><strong>拥挤问题</strong>。不同的类簇挤在一起即为拥挤问题。</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="id20">
<h3>对称SNE<a class="headerlink" href="#id20" title="Link to this heading">¶</a></h3>
<p>  针对SNE的不对称问题，可以在低维和高维空间构造联合分布，使之任意<span class="math notranslate nohighlight">\(i,j\)</span>都有<span class="math notranslate nohighlight">\(p_{ij}=p_{ji}, q_{ij}=q_{ji}\)</span>。在低维空间构造，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
q_{ij}=\frac{\exp(-\Vert \pmb{y}_i-\pmb{y}_j\Vert^2)}{\sum_{k\neq l} \exp(-\Vert \pmb{y}_k-\pmb{y}_l\Vert^2}
\]</div>
</div>
<p>但在高维空间，按<span class="math notranslate nohighlight">\(q_{ij}\)</span>的形式构造<span class="math notranslate nohighlight">\(p_{ij}\)</span>则会存在问题。如果有一个离群点存在，则会造成离群点<span class="math notranslate nohighlight">\(i\)</span>与其余点<span class="math notranslate nohighlight">\(j\)</span>的<span class="math notranslate nohighlight">\(p_{ij}\)</span>都比较小。这就意味着，无论离群点映射在低维空间的任何位置，惩罚值都不会太大。所以需要采用其它方法。以下是一种简单直观的方法，既满足对称性又符合离群点的惩罚值不会过小。</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
p_{ij}=\frac{p_{j|i}+p_{i|j}}{2n}\quad (n\textrm{为数据总量})
\]</div>
</div>
<p>此时，目标函数和梯度变为，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\mathcal{L}=\sum_i\sum_j p_{ij}\log\frac{p_{ij}}{q_{ij}},\quad\frac{\partial \mathcal{L}}{\partial\pmb{y}_i}=4\sum_j (p_{ij}-q_{ij})(\pmb{y}_i-\pmb{y}_j)
\]</div>
</div>
<p>  虽然梯度计算简化了也解决了不对称问题，但效果只是略优于SNE,拥挤问题仍未解决。</p>
</section>
<hr class="docutils" />
<section id="t-sne">
<h3>t-SNE<a class="headerlink" href="#t-sne" title="Link to this heading">¶</a></h3>
<p>  拥挤问题实质上是高维空间距离分布和低维空间距离分布的差异造成的。例如，100维空间可以存在101个点两两距离相等，但在2维空间，最多只能保证3个点两两距离相等。因此高维空间的距离关系完整保持到低维空间是很困难的。从维度上来说，随着维度的增加，大部分数据点之间距离越来越远，如果将这种距离关系保留到低维空间，必定会出现拥挤问题。</p>
<p>  首先,考虑一下两个分布：正态分布与<span class="math notranslate nohighlight">\(t\)</span>分布。<div align="center">
<img alt="difference between t-dist and norm" src="../_images/norm-t-fit.png" /></p>
</div>
<p>这里可以看出，正态分布为了照顾异常点，会偏离大部分样本所在位置，且方差大。<span class="math notranslate nohighlight">\(t\)</span>分布尾部较高，对异常点不敏感，更鲁棒。需要注意的是：如果没有异常点，这两个分布相差不大。</p>
<p>  对于这个问题，可以引入t分布来改进。考察一下<strong>低维使用<span class="math notranslate nohighlight">\(t\)</span>分布，高维使用正态分布</strong>的情况。<div align="center">
<img alt="difference between t-dist and norm" src="../_images/norm-t-dist.png" /></p>
</div>
<p>从图中可以看出，高维空间较近的点，为满足<span class="math notranslate nohighlight">\(p_{ij}=q_{ij}\)</span>，在低维空间相同的<span class="math notranslate nohighlight">\(q_{ij}\)</span>，距离比高维空间更近。高维空间较远的点，为满足<span class="math notranslate nohighlight">\(p_{ij}=q_{ij}\)</span>，在低维空间相同的<span class="math notranslate nohighlight">\(q_{ij}\)</span>，距离比高维空间更远。换句话说，映射至低维空间后，高维空间较远的点，在低维空间距离更远；高维空间较近的点，低维空间距离更近。</p>
<p>  引入<span class="math notranslate nohighlight">\(t\)</span>分布。在高维空间使用高斯分布，在低维空间使用<span class="math notranslate nohighlight">\(t\)</span>分布。可以使得高维空间较近的点在低维空间距离更近，高维空间较远的点在低维空间距离更远。这就达到了同一类簇内的点聚合的更紧密，不同类簇之间的点更加疏远的目标。在低维空间定义自由度为1的<span class="math notranslate nohighlight">\(t\)</span>分布重新定义<span class="math notranslate nohighlight">\(q_{ij}\)</span>，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
q_{ij}=\frac{(1+\Vert \pmb{y}_i-\pmb{y}_j\Vert^2)^{-1}}{\sum_{k\neq l}(1+\Vert \pmb{y}_k-\pmb{y}_l\Vert^2)^{-1}}
\]</div>
</div>
<p>仍然使用KL散度来衡量高维空间正态分布与低维空间<span class="math notranslate nohighlight">\(t\)</span>分布之间的差异程度，此时，梯度变为，</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\pmb{y}_i}=4\sum_{j}(p_{ij}-q_{ij})(1+\Vert \pmb{y}_i-\pmb{y}_j\Vert^2)^{-1}
\]</div>
</div>
<p>  仍然使用随机梯度下降法训练，即可得到<span class="math notranslate nohighlight">\(\hat{\pmb{Y}}\)</span>。这就是t-SNE算法。原文参考(<a class="reference external" href="https://papers.nips.cc/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf">Stochastic Neighbor Embedding</a>,<a class="reference external" href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">Visualizing Data using t-SNE</a>).</p>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2022-2024, SSPUIIP
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">数据降维</a><ul>
<li><a class="reference internal" href="#id2">主成分分析</a><ul>
<li><a class="reference internal" href="#id3">最近重构性</a></li>
<li><a class="reference internal" href="#id4">最大可分性</a></li>
<li><a class="reference internal" href="#id5">优化问题的求解</a></li>
<li><a class="reference internal" href="#id6">算法1–特征值分解</a></li>
<li><a class="reference internal" href="#svd">算法2–SVD分解</a></li>
<li><a class="reference internal" href="#id7">核主成分分析</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id8">多维缩放</a><ul>
<li><a class="reference internal" href="#id9">算法</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id10">等度量映射</a><ul>
<li><a class="reference internal" href="#id11">算法</a></li>
<li><a class="reference internal" href="#id12">流形</a></li>
</ul>
</li>
<li><a class="reference internal" href="#lle">LLE局部线性嵌入</a><ul>
<li><a class="reference internal" href="#id13">LLE基本思想</a></li>
<li><a class="reference internal" href="#id14">LLE求解</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id15">拉普拉斯特征映射</a><ul>
<li><a class="reference internal" href="#id16">拉普拉斯算子</a></li>
<li><a class="reference internal" href="#id17">拉普拉斯矩阵</a></li>
<li><a class="reference internal" href="#id18">拉普拉斯变换</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id19">随机近邻嵌入</a><ul>
<li><a class="reference internal" href="#sne">SNE</a></li>
<li><a class="reference internal" href="#id20">对称SNE</a></li>
<li><a class="reference internal" href="#t-sne">t-SNE</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=f115507d"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="../_static/mathjax/tex-chtml.js"></script>
    </body>
</html>