

<!DOCTYPE html>


<html lang="zh-CN" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>1. 表示学习 &#8212; MLBOOK 1.0 文档</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../_static/translations.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="../_static/mathjax/tex-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ml/dimension_reduce';</script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="1. 核函数基础" href="../kernel/base.html" />
    <link rel="prev" title="机器学习基础" href="../index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="zh-CN"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    <p class="title logo__title">MLBOOK 1.0 文档</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">机器学习</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">1. 表示学习</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">核函数</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../kernel/base.html">1. 核函数基础</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">最优化</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../optimization/convex_prob.html">1. 凸优化问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimization/convex_solve.html">2. 优化问题求解(1)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimization/convex_neq_solve.html">3. 优化问题求解(2)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">矩阵分析</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../matrix/base.html">1. 矩阵性能指标</a></li>
<li class="toctree-l1"><a class="reference internal" href="../matrix/matrixoper.html">2. 矩阵运算</a></li>
<li class="toctree-l1"><a class="reference internal" href="../matrix/vectorspace.html">3. 向量空间</a></li>
<li class="toctree-l1"><a class="reference internal" href="../matrix/matrixdiff.html">4. 矩阵微分</a></li>
<li class="toctree-l1"><a class="reference internal" href="../matrix/special_matrix.html">5. 特殊矩阵</a></li>
<li class="toctree-l1"><a class="reference internal" href="../matrix/subspace.html">6. 子空间分析</a></li>
<li class="toctree-l1"><a class="reference internal" href="../matrix/decomposition.html">7. 矩阵分解</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">数学建模</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../mathmodel/statistic_model.html">1. 不确定模型</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="下载此页面">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/ml/dimension_reduce.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="下载源文件"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="列印成 PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="全屏模式"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="搜索" aria-label="搜索" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>表示学习</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> 目录 </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.1. 主成分分析</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1.1.1. 最近重构性</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">1.1.2. 最大可分性</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">1.1.3. 优化问题的求解</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">1.1.4. 算法1–特征值分解</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svd">1.1.5. 算法2–SVD分解</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">1.1.6. 核主成分分析</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">1.2. 多维缩放</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">1.2.1. 算法</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">1.3. 等度量映射</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">1.3.1. 算法</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">1.3.2. 流形</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lle">1.4. LLE局部线性嵌入</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">1.4.1. LLE基本思想</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">1.4.2. LLE求解</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">1.5. 拉普拉斯特征映射</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">1.5.1. 拉普拉斯算子</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">1.5.2. 拉普拉斯矩阵</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">1.5.3. 拉普拉斯变换</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">1.6. 随机近邻嵌入</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sne">1.6.1. SNE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">1.6.2. 对称SNE</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1><span class="section-number">1. </span>表示学习<a class="headerlink" href="#id1" title="此标题的永久链接">#</a></h1>
<p>  一般来说，机器学习中的原始数据是高维数据。高维数据往往具有复杂性、冗余性等特点。高维空间会有很多不一样的特征，也称之为维度灾难。</p>
<ul class="simple">
<li><p>大多数数据对象之间相距都很远(高维数据有很大可能非常稀疏)</p></li>
<li><p>理论上，通过增大训练集，使训练集达到足够密度，是可以避免维度灾难的。但实践中，要达到给定密度，所需的训练数据随着维度的增加呈指数式上升</p></li>
</ul>
<p>  为了避免维度灾难，以及找到问题求解最合适的数据表示形式，需要研究原有数据的表示问题，这一过程也称之为<strong>表示学习</strong>。</p>
<hr class="docutils" />
<section id="id2">
<h2><span class="section-number">1.1. </span>主成分分析<a class="headerlink" href="#id2" title="此标题的永久链接">#</a></h2>
<p>  主成分分析(Principal Component Analysis, PCA)是一种通过某种正交变换将一组可能存在相关关系的变量转换为一组线性不相关的变量。对于训练数据，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\pmb{X}=\begin{pmatrix}|&amp;|&amp;\dots&amp;|\\\pmb{x}_1&amp;\pmb{x}_2&amp;\dots&amp;\pmb{x}_m\\ |&amp;|&amp;\dots&amp;| \end{pmatrix}_{n\times m}
\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{x}_i=(x_{i1},...,x_{in})^\top\)</span>。PCA的<strong>目标</strong>是找到一个基<span class="math notranslate nohighlight">\((\pmb{w}_1,\pmb{w}_2,...,\pmb{w}_d)=\pmb{W}_{n\times d}\)</span>，使得<span class="math notranslate nohighlight">\(\pmb{Z}=\pmb{W}^\top\pmb{X}\)</span>的重构矩阵<span class="math notranslate nohighlight">\(\hat{\pmb{X}}=\pmb{WZ}\)</span>与<span class="math notranslate nohighlight">\(\pmb{X}\)</span>的误差尽可能的小，即，投影的超平面<span class="math notranslate nohighlight">\(\pmb{W}\)</span>使得投影后的数据矩阵<span class="math notranslate nohighlight">\(\pmb{Z}\)</span>丢失的信息最少。</p>
<p>  如何找到这个超平面呢？可行的一个办法是比较<span class="math notranslate nohighlight">\(\pmb{X}\)</span>与<span class="math notranslate nohighlight">\(\hat{\pmb{X}}\)</span>之间的平均距离（<span class="math notranslate nohighlight">\(\parallel \pmb{x}_i-\hat{\pmb{x}}_i\parallel^2\)</span>），使得这个距离最小的超平面就是最优投影超平面。这是PCA这主要思想。</p>
<p>  对于空间的所有样本点，如何用一个超平面来恰当的表示？有两种办法，即</p>
<ul class="simple">
<li><p>样本点到这个超平面的距离都足够近(投影距离最小)</p></li>
<li><p>样本点在这个超平面的投影尽可能分开</p></li>
</ul>
<section id="id3">
<h3><span class="section-number">1.1.1. </span>最近重构性<a class="headerlink" href="#id3" title="此标题的永久链接">#</a></h3>
<p>  假设数据样本已<font color="red">中心化</font>所有样本减去均值即为中心化；(中心化之后再除以样本标准差即为标准化)，变换后的新坐标系为<span class="math notranslate nohighlight">\((\pmb{w}_1,\pmb{w}_2,...,\pmb{w}_d)\)</span>，若丢弃部分坐标，将维度降至<span class="math notranslate nohighlight">\(d'&lt;d\)</span>，则样本在低维坐标系中的投影为<span class="math notranslate nohighlight">\(\pmb{z}_i=(z_{i1},z_{i2},...,z_{id'})\)</span> ，其中<span class="math notranslate nohighlight">\(z_{ij}=\langle \pmb{x}_i,\pmb{w}_j\rangle\)</span>是<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>在低维坐标系的第<span class="math notranslate nohighlight">\(j\)</span>维的坐标。若用<span class="math notranslate nohighlight">\(\pmb{z}_i\)</span>来重构<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>，则会有<span class="math notranslate nohighlight">\(\hat{\pmb{x}}_i=\sum_{j=1}^{d'}z_{ij}\pmb{w}_j=\pmb{W}\pmb{z}_i\)</span>。</p>
<p>  对于整个数据集，原样本点<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>与投影重构<span class="math notranslate nohighlight">\(\hat{\pmb{x}}_i\)</span>之间距离为，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\sum_{i=1}^m \left\Vert \sum_{j=1}^{d'}z_{ij}\pmb{w}_j-\pmb{x}_i\right\Vert^2 &amp;=\sum_{i=1}^m \pmb{z}_i^\top\pmb{z}_i-2\sum_{i=1}^m\pmb{z}_i^\top\pmb{W}^\top\pmb{x}_i + \textrm{const}\\
&amp;=\sum_{i=1}\pmb{x}_i^\top\pmb{W}\pmb{W}^\top\pmb{x}_i-2\sum_{i=1}\pmb{x}_i^\top\pmb{W}\pmb{W}^\top\pmb{x}_i +\textrm{const}\\
&amp;=-\text{tr}\left( \sum_{i=1}\pmb{x}_i^\top\pmb{W}\pmb{W}^\top\pmb{x}_i  \right)+\textrm{const}\\
&amp;\propto-\text{tr}\left(\pmb{W}^\top\left(\sum_{i=1}^m \pmb{x}_i\pmb{x}_i^\top  \right)\pmb{W} \right)
\end{split}
\end{split}\]</div>
<p>PCA的优化目标则变成，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\min\limits_{\pmb{W}}\quad &amp;-\text{tr}\left(\pmb{W}^\top\pmb{XX}^\top\pmb{W} \right)\\
\textrm{s.t.}\quad &amp;\pmb{W}^\top\pmb{W}=\pmb{I}
\end{split}
\end{split}\]</div>
<p>其中，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\pmb{X}=\begin{pmatrix}|&amp;|&amp;\dots&amp;|\\\pmb{x}_1&amp;\pmb{x}_2&amp;\dots&amp;\pmb{x}_m\\ |&amp;|&amp;\dots&amp;| \end{pmatrix}_{d\times m}, \qquad
\pmb{W}=\begin{pmatrix}|&amp;|&amp;\dots&amp;|\\\pmb{w}_1&amp;\pmb{w}_2&amp;\dots&amp;x_{d'}\\ |&amp;|&amp;\dots&amp;| \end{pmatrix}_{d\times d'}
\end{split}\]</div>
</section>
<section id="id4">
<h3><span class="section-number">1.1.2. </span>最大可分性<a class="headerlink" href="#id4" title="此标题的永久链接">#</a></h3>
<p>  样本点<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>在新空间的超平面投影为<span class="math notranslate nohighlight">\(\pmb{W}^\top\pmb{x}_i\)</span>，若要投影尽可能分开，则应使投影后的样本方差最大化，即，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\max_{\pmb{W}} \quad &amp;\text{tr}(\pmb{W}^\top\pmb{XX}^\top\pmb{W})\\
\text{s.t.}\quad &amp;\pmb{W}^\top\pmb{W}=\pmb{I}
\end{split}
\end{split}\]</div>
<p>可以看出，该问题与第一种情况是等价的。</p>
</section>
<section id="id5">
<h3><span class="section-number">1.1.3. </span>优化问题的求解<a class="headerlink" href="#id5" title="此标题的永久链接">#</a></h3>
<p>  使用拉格朗日乘子法，可得，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\mathcal{L}(\pmb{W},\pmb{\lambda})&amp;=\textrm{tr}(\pmb{W}^\top\pmb{XX}^\top\pmb{W})+\lambda(\pmb{W}^\top\pmb{W}-\pmb{I})\\
&amp;=\sum_{i=1}\pmb{w}_i^\top\pmb{XX}^\top\pmb{w}_i + \sum_{i=1}\lambda_i(\pmb{w}_i^\top\pmb{w}_i-1)
\end{split}
\end{split}\]</div>
<p>则有，<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial \pmb{w}_i}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \pmb{w}_i}=\pmb{XX}^\top\pmb{w}_i-\lambda_i\pmb{w}_i
\]</div>
<p>令<span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial \pmb{w}_i}=0\)</span>，则有</p>
<div class="math notranslate nohighlight">
\[
\pmb{X}\pmb{X}^\top\pmb{w}_i=\lambda_i\pmb{w}_i
\]</div>
<p>于是，只要对样本协方差矩阵进行特征值分解，将求得的特征值排序后，取前<span class="math notranslate nohighlight">\(d'\)</span>个特征值对应的特征向量构成<font color="red">投影矩阵<span class="math notranslate nohighlight">\(W^*=(w_1,w_2,...,w_{d'})\)</span></font>。该矩阵即为主成分分析的解。</p>
</section>
<section id="id6">
<h3><span class="section-number">1.1.4. </span>算法1–特征值分解<a class="headerlink" href="#id6" title="此标题的永久链接">#</a></h3>
<p>  通过样本协方差矩阵计算PCA。</p>
<p>  <strong>输入</strong>：样本集<span class="math notranslate nohighlight">\(\mathcal{D}=\{x_1,x_2,...,x_m\}\)</span>，低维空间维数<span class="math notranslate nohighlight">\(d'\)</span>.</p>
<p>  <strong>过程</strong>：</p>
<p>  1：样本中心化： <span class="math notranslate nohighlight">\(\pmb{x}_i=\pmb{x}_i-\frac{1}{m}\sum_{i=1}^m\pmb{x}_i\)</span>；</p>
<p>  2：计算样本的协方差矩阵<span class="math notranslate nohighlight">\(\pmb{XX}^T\)</span>;</p>
<p>  3：对协方差矩阵做特征值分解；</p>
<p>  4：取出最大的<span class="math notranslate nohighlight">\(d'\)</span>个特征值对应的特征向量<span class="math notranslate nohighlight">\(\pmb{w}_1,\pmb{w}_2,...,\pmb{w}_{d'}\)</span>；</p>
<p>  <strong>输出</strong>： 投影矩阵<span class="math notranslate nohighlight">\(\pmb{W}^*=(\pmb{w}_1,\pmb{w}_2,...,\pmb{w}_{d'})\)</span>。</p>
<ul class="simple">
<li><p><strong>示例代码</strong></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>


<span class="k">def</span> <span class="nf">pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="n">n_samples</span><span class="p">,</span><span class="n">n_features</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="o">-</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">scatter_matrix</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="n">X</span><span class="p">)</span>
    <span class="n">eig_val</span><span class="p">,</span><span class="n">eig_vec</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">scatter_matrix</span><span class="p">)</span>
    <span class="n">eig_pairs</span><span class="o">=</span><span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">eig_val</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span><span class="n">eig_vec</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">)]</span>
    <span class="n">eig_pairs</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">features</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ele</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">ele</span> <span class="ow">in</span> <span class="n">eig_pairs</span><span class="p">[:</span><span class="n">k</span><span class="p">]])</span>
    <span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">,</span><span class="n">features</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span> 
    <span class="n">X_new</span><span class="p">,</span><span class="n">features</span><span class="o">=</span><span class="n">pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    
    
    
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;ro&#39;</span><span class="p">)</span><span class="c1">#,c = &#39;r&#39;,marker = &#39;o&#39;)</span>
    <span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">x1</span><span class="o">=</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">y1</span><span class="o">=</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">,</span><span class="s1">&#39;b-&#39;</span><span class="p">)</span>

    <span class="n">proj_dir</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">proj_dir</span><span class="o">=</span><span class="n">proj_dir</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">proj_dir</span><span class="p">)</span>
    <span class="c1">#计算投影</span>
    <span class="n">PX</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
        <span class="n">p</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">proj_dir</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">proj_dir</span><span class="p">)</span>
        <span class="n">px</span><span class="o">=</span><span class="n">p</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">py</span><span class="o">=</span><span class="n">p</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">features</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">PX</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">px</span><span class="p">,</span><span class="n">py</span><span class="p">])</span>
    <span class="n">PX</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">PX</span><span class="p">)</span>  
    <span class="k">for</span> <span class="n">ix</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;s&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">0</span><span class="p">]],[</span><span class="n">X</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span><span class="s1">&#39;y:&#39;</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">xy</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;(</span><span class="si">%.0f</span><span class="s2">,</span><span class="si">%.0f</span><span class="s2">)&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">xy</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">xy</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">xy</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;offset points&#39;</span><span class="p">)</span> <span class="c1">#标注数据样本</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="svd">
<h3><span class="section-number">1.1.5. </span>算法2–SVD分解<a class="headerlink" href="#svd" title="此标题的永久链接">#</a></h3>
<p>  PCA除了对于协方差矩阵<span class="math notranslate nohighlight">\(\pmb{XX}^\top\)</span>进行特征值分解计算得到投影特征向量之外，还可以通过SVD矩阵分解技术得到投影向量。SVD矩阵分解如下式所示，</p>
<div class="math notranslate nohighlight">
\[
\hat{\pmb{X}}=\pmb{U\Sigma V}^\top, \quad \hat{\pmb{X}}^\top=\pmb{V\Sigma U}^\top
\]</div>
<p>这里的<span class="math notranslate nohighlight">\(\hat{\pmb{X}}\)</span>是正常的数据集矩阵。即</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{\pmb{X}}=\begin{pmatrix}-&amp;\pmb{x}_1 &amp;-\\ -&amp;\pmb{x}_1 &amp;-\\ 
\vdots&amp;\vdots &amp;\vdots\\ -&amp;\pmb{x}_m &amp;-\\ \end{pmatrix}
\end{split}\]</div>
<p>  PCA中的<span class="math notranslate nohighlight">\(\pmb{X}=\hat{\pmb{X}}^\top\)</span>，因此有，</p>
<div class="math notranslate nohighlight">
\[
\pmb{XX}^\top=\hat{\pmb{X}}^\top\hat{\pmb{X}}=\pmb{V\Sigma U}^\top \pmb{U\Sigma V}^\top=\pmb{V\Sigma}^2\pmb{V}^\top
\]</div>
<p>  最终，<span class="math notranslate nohighlight">\(\pmb{V}\)</span>的最大前<span class="math notranslate nohighlight">\(d'\)</span>个特征值对应的特征向量所组成的矩阵即为变换矩阵<span class="math notranslate nohighlight">\(\pmb{W}^*=(\pmb{w}_1,\pmb{w}_2,...,\pmb{w}_{d'})\)</span>。而<span class="math notranslate nohighlight">\(\pmb{V}\)</span>可以通过SVD分解获得。</p>
<p>  投影后的新数据(<span class="math notranslate nohighlight">\(d'&lt;n\)</span>)，</p>
<div class="math notranslate nohighlight">
\[
\pmb{Z}=\pmb{X}^\top\pmb{W}^*=\pmb{U\Sigma V}^\top\pmb{W}^*\approx\pmb{U\Sigma}
\]</div>
<ul class="simple">
<li><p>示例</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="n">n_samples</span><span class="p">,</span><span class="n">n_features</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="o">-</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">scatter_matrix</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="n">X</span><span class="p">)</span>
    <span class="n">eig_val</span><span class="p">,</span><span class="n">eig_vec</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">scatter_matrix</span><span class="p">)</span>
    <span class="n">eig_pairs</span><span class="o">=</span><span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">eig_val</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span><span class="n">eig_vec</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_features</span><span class="p">)]</span>
    <span class="n">eig_pairs</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">features</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ele</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">ele</span> <span class="ow">in</span> <span class="n">eig_pairs</span><span class="p">[:</span><span class="n">k</span><span class="p">]])</span>
    <span class="n">features</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
    <span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">features</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">,</span><span class="n">features</span>
<span class="k">def</span> <span class="nf">pca_svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="o">-</span><span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">U</span><span class="p">,</span><span class="n">S</span><span class="p">,</span><span class="n">Vt</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">W</span><span class="o">=</span><span class="n">Vt</span><span class="o">.</span><span class="n">T</span><span class="p">[:,:</span><span class="n">k</span><span class="p">]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">W</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">,</span><span class="n">W</span>
<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span> 
    <span class="n">X_new</span><span class="p">,</span><span class="n">features</span><span class="o">=</span><span class="n">pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">X_new_svd</span><span class="p">,</span><span class="n">f_svd</span><span class="o">=</span><span class="n">pca_svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;X_eig_decom&#39;</span><span class="p">,</span><span class="n">X_new</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;X_svd&#39;</span><span class="p">,</span><span class="n">X_new_svd</span><span class="p">)</span>
    
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>    
        <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">features</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span>    
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;ro&#39;</span><span class="p">)</span><span class="c1">#,c = &#39;r&#39;,marker = &#39;o&#39;)</span>
        <span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">x1</span><span class="o">=</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">b</span><span class="o">/</span><span class="n">a</span><span class="p">))</span><span class="c1">#features[1,0]/features[0,0]))</span>
        <span class="n">y1</span><span class="o">=</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">b</span><span class="o">/</span><span class="n">a</span><span class="p">))</span><span class="c1">#features[1,0]/features[0,0]))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">,</span><span class="s1">&#39;b--&#39;</span><span class="p">)</span>    
    
        <span class="n">proj_dir</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">])</span><span class="c1">#features[0,0],features[1,0]])</span>
        <span class="n">proj_dir</span><span class="o">=</span><span class="n">proj_dir</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">proj_dir</span><span class="p">)</span>
        <span class="c1">#计算投影</span>
        <span class="n">PX</span><span class="o">=</span><span class="p">[]</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
            <span class="n">p</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">proj_dir</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">proj_dir</span><span class="p">)</span>
            <span class="n">px</span><span class="o">=</span><span class="n">p</span><span class="o">*</span><span class="n">proj_dir</span>
            <span class="n">PX</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">px</span><span class="p">)</span>
            <span class="c1">#px=p*np.cos(np.arctan(features[1,0]/features[0,0]))</span>
            <span class="c1">#py=p*np.sin(np.arctan(features[1,0]/features[0,0]))</span>
            <span class="c1">#PX.append([px,py])</span>
        <span class="n">PX</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">PX</span><span class="p">)</span>  
        <span class="k">for</span> <span class="n">ix</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;s&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">0</span><span class="p">]],[</span><span class="n">X</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">PX</span><span class="p">[</span><span class="n">ix</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span><span class="s1">&#39;y:&#39;</span><span class="p">)</span>       
        
   
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>

</pre></div>
</div>
</section>
<section id="id7">
<h3><span class="section-number">1.1.6. </span>核主成分分析<a class="headerlink" href="#id7" title="此标题的永久链接">#</a></h3>
<p>  前面我们通过计算样本协方差矩阵<span class="math notranslate nohighlight">\(\pmb{XX}^\top\)</span>的特征向量组成投影矩阵来实现PCA。对于核函数的隐式映射<span class="math notranslate nohighlight">\(\phi :\pmb{x}\rightarrow \phi(\pmb{x})\)</span>形成的映射数据矩阵<span class="math notranslate nohighlight">\(\pmb{\Phi}^\top\)</span>，如何计算PCA。也就是映射后的协方差矩阵<span class="math notranslate nohighlight">\(\pmb{\Phi\Phi}^\top\)</span>如何分解出特征向量组成投影矩阵？针对这一问题，研究人员提出了核主成分分析(kernel PCA)。</p>
<p>  首先考查<span class="math notranslate nohighlight">\(\pmb{X}^\top\pmb{X}\)</span>与<span class="math notranslate nohighlight">\(\pmb{XX}^\top\)</span>特征向量之间的关系。
对实对称矩阵<span class="math notranslate nohighlight">\(\pmb{X}^\top\pmb{X}\)</span>进行特征值分解<span class="math notranslate nohighlight">\(\pmb{X}^\top\pmb{X}\pmb{U}=\pmb{U\Lambda}\)</span>，等式两边同时乘上<span class="math notranslate nohighlight">\(\pmb{X}\)</span>，则可以得到，</p>
<div class="math notranslate nohighlight">
\[
(\pmb{XX}^\top)(\pmb{XU})=(\pmb{XU})\pmb{\Lambda}
\]</div>
<p>从上式可以得到<span class="math notranslate nohighlight">\(\pmb{XX}^\top\)</span>的特征向量为<span class="math notranslate nohighlight">\(\pmb{V}\triangleq\pmb{XU}\)</span>，特征值对角矩阵为<span class="math notranslate nohighlight">\(\pmb{\Lambda}\)</span>。注意到特征向量的模长，</p>
<div class="math notranslate nohighlight">
\[
\Vert \pmb{v}_j\Vert^2=\pmb{u}_j^\top\pmb{X}^\top\pmb{X}\pmb{u}_j=\lambda_j\pmb{u}_j^\top\pmb{u}_j=\lambda_j
\]</div>
<p>可以得到单位化的特征向量矩阵<span class="math notranslate nohighlight">\(\pmb{V}_{\textrm{pca}}=(\pmb{XU})\pmb{\Lambda}^{-1/2}\)</span>。</p>
<p>  现在考虑Gram矩阵<span class="math notranslate nohighlight">\(\pmb{K}\triangleq\pmb{X}^\top\pmb{X}\)</span>。根据Mercer定理，当使用一个核函数时，隐含了一个潜在的特征空间，因此，可以将<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>表示为<span class="math notranslate nohighlight">\(\pmb{\phi}_i\triangleq\phi(\pmb{x}_i)\)</span>。相应地，数据矩阵<span class="math notranslate nohighlight">\(\pmb{X}^\top\)</span>映射为<span class="math notranslate nohighlight">\(\pmb{\Phi}^\top\)</span>，协方差矩阵<span class="math notranslate nohighlight">\(\pmb{X}\pmb{X}^\top\)</span>映射为<span class="math notranslate nohighlight">\(\pmb{\Phi}\pmb{\Phi}^\top\)</span>。由<span class="math notranslate nohighlight">\(\pmb{X}^\top\pmb{X}\)</span>与<span class="math notranslate nohighlight">\(\pmb{XX}^\top\)</span>的关系可知，<span class="math notranslate nohighlight">\(\pmb{\Phi}\pmb{\Phi}^\top\)</span>的特征向量矩阵为</p>
<div class="math notranslate nohighlight">
\[\pmb{V}_{\textrm{kpca}}=\pmb{\Phi U\Lambda}^{-1/2}\]</div>
<p>其中<span class="math notranslate nohighlight">\(\pmb{U\Lambda}\)</span>分别为<span class="math notranslate nohighlight">\(\pmb{K}=\pmb{\Phi}^\top\pmb{\Phi}\)</span>的特征向量矩阵以及对应的特征值。</p>
<p>  根据上面计算的结果，从特征向量矩阵中取<span class="math notranslate nohighlight">\(k\)</span>个特征向量即可组成投影矩阵，经过数据投影即可得到样本的<span class="math notranslate nohighlight">\(k\)</span>维压缩表示。<strong>但是</strong>，映射<span class="math notranslate nohighlight">\(\phi()\)</span>可能没有显示表示，或难以直接计算。解决办法是使用核函数间接计算<span class="math notranslate nohighlight">\(\phi()\)</span>。任意给定样本<span class="math notranslate nohighlight">\(\pmb{x}_*\)</span>，则其在特征空间的投影<span class="math notranslate nohighlight">\(\hat{\pmb{x}}_i\)</span>可通过以下方式计算。</p>
<div class="math notranslate nohighlight">
\[
\hat{\pmb{x}}_i=\phi(\pmb{x}_*)^\top\pmb{V}_{\textrm{kpca}}=\phi(\pmb{x}_*)^\top\pmb{\Phi U\Lambda}^{-1/2}=\pmb{k}_*^{\top}\pmb{U\Lambda}^{-1/2}
\]</div>
<p>  最后要注意的是<span class="math notranslate nohighlight">\(\pmb{K}\)</span>在特征值分解之前，需要中心化。中心化可通过以下步骤计算得到。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\tilde{\pmb{K}}&amp;=\pmb{K}-\frac1N\pmb{K11}^\top-\frac1N\pmb{11}^\top\pmb{K}+\frac{1}{N^2}(\pmb{1}^\top\pmb{K}\pmb{1})\pmb{11}^\top\\
&amp;=\pmb{K}-\pmb{KO}-\pmb{{OK}}+\pmb{OKO}
\end{split}
\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{O}=\frac1N \pmb{1}\pmb{1}^\top, \pmb{1}=[1,1,...,1]_{1\times N}^\top\)</span>。</p>
<ul class="simple">
<li><p>示例</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">kpca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">k</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; </span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : np.array with n x d</span>
<span class="sd">    k : int rank of the low-dimension</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    data : projection data</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span><span class="p">,</span><span class="n">d</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">d</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Dimensions of output data has to be lesser than the dimensions of input data</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span>
    
    <span class="c1"># construct K</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">row</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">k_ij</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X</span><span class="p">[</span><span class="n">row</span><span class="p">,:]</span><span class="o">-</span><span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">,:])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">K</span><span class="p">[</span><span class="n">row</span><span class="p">,</span><span class="n">col</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">k_ij</span><span class="p">)</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">+</span><span class="n">K</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">K</span><span class="p">[</span><span class="n">row</span><span class="p">,</span><span class="n">row</span><span class="p">]</span><span class="o">=</span><span class="n">K</span><span class="p">[</span><span class="n">row</span><span class="p">,</span><span class="n">row</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span>
        
    <span class="c1"># normalize K</span>
    <span class="n">all1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span><span class="o">/</span><span class="n">n</span>
    <span class="n">K_center</span> <span class="o">=</span> <span class="n">K</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">all1</span><span class="p">,</span><span class="n">K</span><span class="p">)</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="n">all1</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">all1</span><span class="p">,</span><span class="n">K</span><span class="p">),</span><span class="n">all1</span><span class="p">)</span>
    
    <span class="c1"># eigvector</span>
    <span class="n">S</span><span class="p">,</span><span class="n">U</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">K_center</span><span class="p">)</span>      
    <span class="n">V</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">S</span><span class="p">))))</span>
    
    <span class="n">eig_pairs</span><span class="o">=</span><span class="p">[(</span><span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">V</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">S</span><span class="p">))]</span>
    <span class="n">eig_pairs</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(([</span><span class="n">ele</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">ele</span> <span class="ow">in</span> <span class="n">eig_pairs</span><span class="p">[:</span><span class="n">k</span><span class="p">]]))</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span>
    <span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_center</span><span class="p">,</span><span class="n">V</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="id8">
<h2><span class="section-number">1.2. </span>多维缩放<a class="headerlink" href="#id8" title="此标题的永久链接">#</a></h2>
<p>  多维缩放(multiple dimensional scaling, MDS)的主要思想是原始空间中样本之间的距离在低维空间得以保持。假设<span class="math notranslate nohighlight">\(m\)</span>个样本在原始空间的距离矩阵为<span class="math notranslate nohighlight">\(\pmb{D}\subseteq \mathbb{R}^{m\times m}\)</span>。MDS的任务是获得样本集在<span class="math notranslate nohighlight">\(d'\)</span>维空间的表示<span class="math notranslate nohighlight">\(\pmb{Z}\in \mathbb{R}^{m\times d'}\)</span>，且任意两个样本在<span class="math notranslate nohighlight">\(d'\)</span>维空间的欧式距离等于原始空间的距离，即<span class="math notranslate nohighlight">\(\parallel \pmb{z}_i-\pmb{z}_j\parallel^2=\)</span><span class="math notranslate nohighlight">\(D_{ij}, \forall 0&lt;i,j\leq m\)</span>。</p>
<ul class="simple">
<li><p>MDS的求解</p></li>
</ul>
<p>  令<span class="math notranslate nohighlight">\(\pmb{B}=\pmb{Z}^\top \pmb{Z}\in\mathbb{R}^{m\times m}\)</span>为降维后的样本内积矩阵，<span class="math notranslate nohighlight">\(b_{ij}=\pmb{z}_i^\top\pmb{z}_j\)</span>,则有，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
dist_{ij}^2&amp;=\parallel z_i \parallel^2+\parallel z_j\parallel^2-2z_i^Tz_j\\
&amp;=b_{ii}+b_{jj}-2b_{ij}\\
\end{split}
\end{split}\]</div>
<p>假设<span class="math notranslate nohighlight">\(\pmb{Z}\)</span>已中心化，即<span class="math notranslate nohighlight">\(\sum_{i=1}^m\pmb{z}_i=0\)</span>，显然<span class="math notranslate nohighlight">\(\sum_{i=1}^mb_{ij}=\sum_{j=1}^mb_{ij}=0\)</span>，由此可知，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\sum_{i=1}^mdist_{ij}^2&amp;=\text{tr}(B)+mb_{jj}\\
\sum_{j=1}^mdist_{ij}^2&amp;=\text{tr}(B)+mb_{ii}\\
\sum_{i=1}^m\sum_{j=1}^mdist_{ij}^2&amp;=2m\cdot\text{tr}(B)\\
\end{split}
\end{split}\]</div>
<p>令,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
dist_{i\cdot}^2&amp;=\frac{1}{m}\sum_{j=1}^mdist_{ij}^2\\
dist_{\cdot j}^2&amp;=\frac{1}{m}\sum_{i=1}^mdist_{ij}^2\\
dist_{\cdot\cdot}^2&amp;=\frac{1}{m^2}\sum_{i=1}\sum_{j=1}^mdist_{ij}^2\\
\end{split}
\end{split}\]</div>
<p>最终可得，</p>
<div class="math notranslate nohighlight">
\[
b_{ij}=-\frac{1}{2}(dist_{ij}^2-dist_{i\cdot}^2-dist_{\cdot j}^2+dist_{\cdot\cdot}^2)
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(dist_{ij}=D_{ij}\)</span>。由此，可以根据降维前的距离矩阵<span class="math notranslate nohighlight">\(\pmb{D}\)</span>求得降维后距离不变的矩阵<span class="math notranslate nohighlight">\(\pmb{B}\)</span>。令<span class="math notranslate nohighlight">\(\pmb{C}_n=\pmb{I}_n-\frac1n\pmb{11}^\top\)</span>为中心化矩阵(Centering matrix)，则，</p>
<div class="math notranslate nohighlight">
\[
\pmb{B}=-\frac12\pmb{C}_n\pmb{D}\pmb{C}_n
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{X}\pmb{C}_n\)</span>相当于对<span class="math notranslate nohighlight">\(\pmb{X}\)</span>的所有行向量减去行向量均值；<span class="math notranslate nohighlight">\(\pmb{C}_n\pmb{X}\)</span>相当于对<span class="math notranslate nohighlight">\(\pmb{X}\)</span>的所有列向量减去列向量均值；</p>
<ul class="simple">
<li><p>获得降维后的样本投影矩阵<span class="math notranslate nohighlight">\(\pmb{Z}\)</span></p></li>
</ul>
<p>  对矩阵<span class="math notranslate nohighlight">\(\pmb{B}\)</span>（实对称矩阵）做特征值分解，<span class="math notranslate nohighlight">\(\pmb{B}=\pmb{V\Lambda V}^\top\)</span>。假设有<span class="math notranslate nohighlight">\(d_*\)</span>个非零特征值构成对角矩阵<span class="math notranslate nohighlight">\(\pmb{\Lambda}_*=\textrm{diag}(\lambda_1,\lambda_2,...,\lambda_{d_*})\)</span>,以及所对应的特征向量矩阵<span class="math notranslate nohighlight">\(\pmb{V}_*\)</span>，则<span class="math notranslate nohighlight">\(\pmb{Z}\)</span>可以表示为，</p>
<div class="math notranslate nohighlight">
\[
\pmb{Z}=\pmb{\Lambda}_*^{\frac{1}{2}}\pmb{V}_*^\top \in \mathbb{R}^{m\times d_*}
\]</div>
<p>  现实应用中，可以选择<span class="math notranslate nohighlight">\(d'&lt;d\)</span>个最大特征值构成的对角阵<span class="math notranslate nohighlight">\(\hat{\pmb{\Lambda}}\)</span>及特征向量矩阵<span class="math notranslate nohighlight">\(\hat{\pmb{V}}\)</span>，即</p>
<div class="math notranslate nohighlight">
\[
\pmb{Z}=\hat{\pmb{\Lambda}}^{\frac{1}{2}} \hat{\pmb{V}}^\top \in \mathbb{R}^{m\times d'}
\]</div>
<section id="id9">
<h3><span class="section-number">1.2.1. </span>算法<a class="headerlink" href="#id9" title="此标题的永久链接">#</a></h3>
<p>  <strong>输入</strong>：距离矩阵<span class="math notranslate nohighlight">\(\pmb{D}\)</span>，低维空间维数<span class="math notranslate nohighlight">\(d'\)</span>.</p>
<p>  <strong>过程</strong>：</p>
<p>    1. 计算<span class="math notranslate nohighlight">\(\pmb{D}\)</span>;</p>
<p>    2. 计算矩阵<span class="math notranslate nohighlight">\(\pmb{B}\)</span>;</p>
<p>    3. 矩阵<span class="math notranslate nohighlight">\(\pmb{B}\)</span>做特征值分解；</p>
<p>    4. 选取<span class="math notranslate nohighlight">\(\hat{\pmb{V}},\hat{\pmb{\Lambda}}\)</span>；</p>
<p>  <strong>输出</strong>： 矩阵<span class="math notranslate nohighlight">\(\hat{\pmb{V}}\hat{\pmb{\Lambda}}^{1/2}\)</span>每一行即为一个样本的低维坐标。</p>
<ul class="simple">
<li><p>示例代码</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>   <span class="c1"># 与MDS进行对比</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">MDS</span>
    
<span class="n">ris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">new_X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">new_X_pca</span> <span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">new_X_pca</span> <span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">mds</span> <span class="o">=</span> <span class="n">MDS</span><span class="p">(</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">new_X_mds</span> <span class="o">=</span> <span class="n">mds</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">new_X_mds</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">new_X_mds</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="id10">
<h2><span class="section-number">1.3. </span>等度量映射<a class="headerlink" href="#id10" title="此标题的永久链接">#</a></h2>
<p>  等度量映射(Isometric Mapping, Isomap)的基本出发点在于，Isomap认为低维流行嵌入到高维空间之后，直接在高维空间计算直线距离具有误导性，因为高维空间的直线距离在低维流行是不可达的（如：瑞士卷上两个点（位于同一<span class="math notranslate nohighlight">\(x,y\)</span>坐标，<span class="math notranslate nohighlight">\(z\)</span>不同坐标）是不能用直线距离来计算的，因为该流行是扭曲过的）。</p>
<p>  所谓<span class="math notranslate nohighlight">\(d\)</span>维流形是<span class="math notranslate nohighlight">\(n\)</span>维空间<span class="math notranslate nohighlight">\((d&lt;n)\)</span>的一部分，局部类似于<span class="math notranslate nohighlight">\(d\)</span>维超平面。例如：2D流形是一个2D形状，该形状可以在更高维的空间中弯曲和扭曲。<strong>流形学习</strong>通过训练实例所在的流形进行建模。流形学习基于流行假设，即大多数现实世界的高维数据集都接近于低维流形。如：三维空间的球面，其实可以只用经度和纬度两个特征来表示。低维嵌入流形上的本真距离（即测地线距离，如：北京至上海的距离（地球是圆的，直线距离要穿过地下层））不能用高维空间的直线距离来计算，但能用近邻距离来近似。</p>
<p>  如何计算测地线距离呢？利用流形在局部与欧氏空间同胚这个性质，对每个样本点基于欧氏距离找出其近邻点，建立一个近邻连接图。于是，计算两点之间的测地线距离的问题就转变为计算近邻连接图上两点之间最短路径的问题。近邻图计算两点之间的最短路径，可以采用Dijkstra算法或Floyd算法，在得到任意两点的距离之后，就可以用多维缩放(MDS)方法来获得样本点在低维空间的坐标。</p>
<p>  它的<strong>核心思想</strong>是沿着图的边移动的距离近似于沿着流形移动的距离。</p>
<section id="id11">
<h3><span class="section-number">1.3.1. </span>算法<a class="headerlink" href="#id11" title="此标题的永久链接">#</a></h3>
<p><strong>输入</strong>：样本集<span class="math notranslate nohighlight">\(\mathcal{D}=\{\pmb{x}_1,\pmb{x}_2,...,\pmb{x}_m\}\)</span>，低维空间维数<span class="math notranslate nohighlight">\(d'\)</span>.</p>
<p><strong>过程</strong>：</p>
<ol class="arabic simple">
<li><p>确定每个样本<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>的<span class="math notranslate nohighlight">\(k\)</span>近邻;</p></li>
<li><p>使用最短路径算法(例如：Dijkstra)计算<span class="math notranslate nohighlight">\(k\)</span>近邻图的任意样本间距离<span class="math notranslate nohighlight">\(dist(\pmb{x}_i,\pmb{x}_j)\)</span>;</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
 dist(\pmb{x}_i,\pmb{x}_j)=\left\{\begin{array}{ll} dist(\pmb{x}_i,\pmb{x}_j), &amp; \pmb{x}_j\textrm{ is a nearest neighbor of }\pmb{x}_i\\ \infty, &amp; \textrm{otherwise.}\end{array}\right.
\end{split}\]</div>
<ol class="arabic simple" start="3">
<li><p>以<span class="math notranslate nohighlight">\(dist(\pmb{x}_i,\pmb{x}_j)\)</span>为输入，使用MDS计算低维坐标；</p></li>
</ol>
<p><strong>输出</strong>： MDS计算的低维坐标。</p>
<ul class="simple">
<li><p>示例</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">Isomap</span>

<span class="n">iris</span><span class="o">=</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="n">fig</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span><span class="n">neighbor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">100</span><span class="p">]):</span>
    <span class="n">isomap</span><span class="o">=</span><span class="n">Isomap</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="n">neighbor</span><span class="p">)</span>
    <span class="n">X_new</span><span class="o">=</span><span class="n">isomap</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X_new</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Isomap(n_neighbors=</span><span class="si">%d</span><span class="s2">)&quot;</span><span class="o">%</span><span class="n">neighbor</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


</pre></div>
</div>
</section>
<section id="id12">
<h3><span class="section-number">1.3.2. </span>流形<a class="headerlink" href="#id12" title="此标题的永久链接">#</a></h3>
<p>  在介绍流形前，需要一些有关的背景知识。</p>
<ul class="simple">
<li><p><strong>拓扑空间</strong></p></li>
</ul>
<p>  给定集合<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>，以及<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>的一些子集构成的族<span class="math notranslate nohighlight">\(\mathcal{O}\)</span>，如果以下性质成立，则<span class="math notranslate nohighlight">\((\mathcal{X},\mathcal{O})\)</span>称为一个拓扑空间：</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\emptyset\)</span>和<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>都属于<span class="math notranslate nohighlight">\(\mathcal{O}\)</span>；</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{O}\)</span>中的任意多个元素的并仍属于<span class="math notranslate nohighlight">\(\mathcal{O}\)</span>；</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{O}\)</span>中的任意多个元素的交仍属于<span class="math notranslate nohighlight">\(\mathcal{O}\)</span>；</p></li>
</ol>
<p>此时，<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>中的元素称为点，<span class="math notranslate nohighlight">\(\mathcal{O}\)</span>中的元素称为开集（可以理解为开区间）。</p>
<ul class="simple">
<li><p><strong>度量空间</strong></p></li>
</ul>
<p>  度量空间是一个二元对<span class="math notranslate nohighlight">\((\mathcal{M},d)\)</span>，其中<span class="math notranslate nohighlight">\(\mathcal{M}\)</span>是一个集合，<span class="math notranslate nohighlight">\(d\)</span>是定义在<span class="math notranslate nohighlight">\(\mathcal{M}\)</span>上的一个度量，即映射<span class="math notranslate nohighlight">\(d: \mathcal{M}\times \mathcal{M}\rightarrow \mathbb{R}\)</span>，对于任意<span class="math notranslate nohighlight">\(\pmb{x,y,z}\in M\)</span>满足以下条件：</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(d(\pmb{x,y})=0 \Leftrightarrow \pmb{x}=\pmb{y}\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(d(\pmb{x,y})=d(\pmb{y,x})\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(d(\pmb{x,z})\le d(\pmb{x,y})+d(\pmb{y,z})\)</span>;</p></li>
</ol>
<ul class="simple">
<li><p><strong>流形</strong></p></li>
</ul>
<p>  流形是一个拓扑空间，对于每个点，其周围的邻域局部类似于欧几里得空间。更确切地说，<span class="math notranslate nohighlight">\(n\)</span>维流形的每个点都有一个邻域开集，该邻域与<span class="math notranslate nohighlight">\(n\)</span>维欧几里德空间的邻域开集同胚。人们经常可以想象拉伸或平坦流形的局部邻域以得到一个平坦的欧几里得平面。大致地说，拓扑空间是一个几何物体，同胚就是把物体连续延展和弯曲，使其成为一个新的物体。因此，正方形和圆是同胚的，但球面和环面就不是。</p>
<p>  在拓扑学中，同胚(homeomorphism、topological isomorphism、bi continuous function)是两个拓扑空间之间的<strong>双连续函数</strong>。同胚是拓扑空间范畴中的同构；也就是说，它们是保持给定空间的所有拓扑性质的映射。如果两个空间之间存在同胚，那么这两个空间就称为同胚的，从拓扑学的观点来看，两个空间是相同的。</p>
<p>  一个较大的<span class="math notranslate nohighlight">\(m\)</span>维空间(<span class="math notranslate nohighlight">\(n&lt;m\)</span>)中的<span class="math notranslate nohighlight">\(n\)</span>维流形
)局部类似于<span class="math notranslate nohighlight">\(n\)</span>维欧几里得超平面。例如</p>
<ol class="arabic simple">
<li><p>1维流形：圆，正方形，曲线等。但8字形不是1维流形，因为8字的中心点局部是2维的欧氏空间同胚。</p></li>
<li><p>2维流形：球面，环面等。</p></li>
</ol>
<p>  由于流形结构是由“局部”类似于欧几里得空间的性质定义的，我们不必考虑任何全局的、外部定义的坐标系的几何关系，相反，我们可以只考虑流形的内在几何和拓扑性质。</p>
</section>
</section>
<hr class="docutils" />
<section id="lle">
<h2><span class="section-number">1.4. </span>LLE局部线性嵌入<a class="headerlink" href="#lle" title="此标题的永久链接">#</a></h2>
<section id="id13">
<h3><span class="section-number">1.4.1. </span>LLE基本思想<a class="headerlink" href="#id13" title="此标题的永久链接">#</a></h3>
<p>  Isomap试图<strong>保持</strong>局部近邻样本之间的<strong>距离</strong>。LLE则试图<strong>保持</strong>局部邻域内样本之间的<strong>线性关系</strong>。假设样本<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>的坐标可由邻居样本<span class="math notranslate nohighlight">\(\pmb{x}_j,\pmb{x}_k,\pmb{x}_l\)</span>的坐标通过线性组合重构出来，即，</p>
<div class="math notranslate nohighlight">
\[
\pmb{x}_i=w_{ij}\pmb{x}_j+w_{ik}\pmb{x}_k+w_{il}\pmb{x}_l
\]</div>
<p>则LLE希望此关系在低维空间依旧能得以保持。</p>
</section>
<section id="id14">
<h3><span class="section-number">1.4.2. </span>LLE求解<a class="headerlink" href="#id14" title="此标题的永久链接">#</a></h3>
<p>  <strong>Step 1</strong>. 寻找样本<span class="math notranslate nohighlight">\(\forall \pmb{x}_i\in \mathcal{X}\)</span>的<span class="math notranslate nohighlight">\(k\)</span>个近邻。</p>
<p>  <strong>Step 2</strong>. 求解重构系数矩阵<span class="math notranslate nohighlight">\(\pmb{W}\)</span>。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\min_W \quad &amp;\varepsilon(\pmb{W})= \sum_{i=1}^m \left\lVert \pmb{x}_i-\sum_{j\in \mathcal{N}_i} w_{ij}\pmb{x}_j \right\rVert^2\\
\text{s.t.}\quad &amp;\sum_{j\in \mathcal{N}_i} w_{ij}=1
\end{split}
\end{split}\]</div>
<p>令<span class="math notranslate nohighlight">\(\varepsilon_i=\left\lVert \pmb{x}_i-\sum_{j\in \mathcal{N}_i} w_{ij}\pmb{x}_j \right\rVert^2\)</span>，则有，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\varepsilon_i&amp;=\left\lVert \pmb{x}_i-\sum_{j\in \mathcal{N}_i} w_{ij}\pmb{x}_j \right\rVert^2\\
&amp;=\left\lVert (w_{i1}+w_{i2}+\cdots+w_{ik})\pmb{x}_i-\sum_{j\in \mathcal{N}_i} w_{ij}\pmb{x}_j \right\rVert^2\quad \textrm{(sum of weights equals to 1)}\\
&amp;=\left\lVert \sum_{j\in \mathcal{N}_i} w_{ij}\pmb{x}_i-\sum_{j\in \mathcal{N}_i} w_{ij}\pmb{x}_j \right\rVert^2\\
&amp;=\left\lVert \sum_{j\in \mathcal{N}_i} w_{ij}(\pmb{x}_i-\pmb{x}_j) \right\rVert^2
\end{split}
\end{split}\]</div>
<p>令<span class="math notranslate nohighlight">\(C_{jk}=(\pmb{x}_i-\pmb{x}_j)^T(\pmb{x}_i-\pmb{x}_k)\)</span>，则<span class="math notranslate nohighlight">\(w_{ij}\)</span>有闭式解，</p>
<div class="math notranslate nohighlight">
\[
w_{ij}=\frac{\sum_{k\in\mathcal{N}_i} C_{jk}^{-1} }{ \sum_{l,s\in\mathcal{N}_i} C_{ls}^{-1} }
\]</div>
<p>  <strong>Step 3</strong>. 恢复低维空间坐标。LLE在低维空间保持<span class="math notranslate nohighlight">\(\pmb{W}\)</span>不变，于是<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>对应的低维空间坐标<span class="math notranslate nohighlight">\(\pmb{z}_i\)</span>可以通过下式求解获得，</p>
<div class="math notranslate nohighlight">
\[
\min\limits_{\pmb{Z}}\quad \sum_{i=1}^m \left\lVert \pmb{z}_i-\sum_{j\in \mathcal{N}_i}w_{ij}\pmb{z}_j\right\rVert^2
\]</div>
<p>  上述两优化问题目标同形，唯一区别在于前一个问题要确定<span class="math notranslate nohighlight">\(\pmb{W}\)</span>，而后一个需要确定<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>所对应的低维坐标<span class="math notranslate nohighlight">\(\pmb{z}_i\)</span>。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\varepsilon(\pmb{Z})&amp;=\sum_{i=1}^n \left\Vert \pmb{z}_i-\sum_j w_{ij}\pmb{z}_j \right\Vert^2\\
\end{split}\]</div>
<p>令,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\pmb{W}=\begin{bmatrix}-&amp;\pmb{w}_1&amp;-\\-&amp;\pmb{w}_2&amp;-\\ &amp;\vdots &amp;\\ -&amp;\pmb{w}_n&amp;- \end{bmatrix} \quad \pmb{Z}=\begin{bmatrix}|&amp;|&amp;\cdots &amp;|\\\pmb{z}_1&amp;\pmb{z}_2&amp;\cdots&amp;\pmb{z}_n\\ |&amp;|&amp;\cdots &amp;|\\ \end{bmatrix}
\end{split}\]</div>
<p>则有，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\varepsilon(\pmb{Z})&amp;=\Vert \pmb{Z}^\top -\pmb{WZ}^\top\Vert_F^2\\
&amp;=\Vert (\pmb{I}-\pmb{W})\pmb{Z}^\top\Vert_F^2\\
&amp;=\textrm{tr}(\pmb{Z}\pmb{M}\pmb{Z}^\top)
\end{split}
\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{M}=(\pmb{I}-\pmb{W})^\top(\pmb{I}-\pmb{W})\)</span>。则问题可重写为，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\min_\pmb{Z}\quad &amp;\text{tr}(\pmb{ZMZ}^\top)\\
\text{s.t.}\quad &amp;\pmb{ZZ}^\top=\pmb{I}
\end{split}
\end{split}\]</div>
<p>该问题可以通过特征值分解求得<span class="math notranslate nohighlight">\(\pmb{M}\)</span>的最大<span class="math notranslate nohighlight">\(d'\)</span>个特征值对应的特征向量组成的矩阵即为<span class="math notranslate nohighlight">\(\pmb{Z}^\top\)</span>。</p>
</section>
</section>
<hr class="docutils" />
<section id="id15">
<h2><span class="section-number">1.5. </span>拉普拉斯特征映射<a class="headerlink" href="#id15" title="此标题的永久链接">#</a></h2>
<section id="id16">
<h3><span class="section-number">1.5.1. </span>拉普拉斯算子<a class="headerlink" href="#id16" title="此标题的永久链接">#</a></h3>
<ul class="simple">
<li><p><strong>连续型</strong></p></li>
</ul>
<p>  <strong>[定义]</strong>- 假设函数<span class="math notranslate nohighlight">\(f\)</span>连续二阶可微，则拉普拉斯算子(<span class="math notranslate nohighlight">\(\Delta f\)</span>)由下式给出，</p>
<div class="math notranslate nohighlight">
\[
\Delta f\triangleq\sum_{i=1}^n\frac{\partial^2f}{\partial x_i^2}=\nabla\cdot\nabla f=\textrm{div}(\textrm{grad}f)
\]</div>
<p>其中，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\textrm{grad} f&amp;=\nabla f=\left(\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},...,\frac{\partial f}{\partial x_n}\right)\\
\textrm{div}(\textrm{grad}f)&amp;=\nabla\cdot\nabla f=\frac{\partial^2 f}{\partial x_1^2}+\frac{\partial^2 f}{\partial x_2^2}+...+\frac{\partial^2 f}{\partial x_n^2}
\end{split}
\end{split}\]</div>
<ul class="simple">
<li><p><strong>离散型</strong></p></li>
</ul>
<p>  在离散情况下，我们仍然希望拉普拉斯算子将输入的函数映射为其它函数。只不过，这种情况下，函数将被定义在离散的域上（如图<span class="math notranslate nohighlight">\(G\)</span>的有限顶点集<span class="math notranslate nohighlight">\(V\)</span>）。因此，我们可以将离散拉普拉斯算子<span class="math notranslate nohighlight">\((\Delta)\phi(v)\)</span>作用在函数 <span class="math notranslate nohighlight">\(\phi :V\rightarrow R\)</span>。而<span class="math notranslate nohighlight">\(\phi\)</span>是一个定义在图的顶点集上的一个函数。我们也使用有限差分作为导数的离散类比，因此，我们不是使用导数来比较连续域的局部区域，而是使用有限差分来比较离散图的局部邻域。</p>
<p>  对于一个定义在图<span class="math notranslate nohighlight">\(G\)</span>的顶点集的函数<span class="math notranslate nohighlight">\(\phi :V\rightarrow R\)</span>，离散拉普拉斯算子定义为，</p>
<div class="math notranslate nohighlight">
\[
(\Delta\phi)(v_i)=\sum_{v_j\in \mathcal{N}(v_i)}W_{ij}[\phi(v_j)-\phi(v_i)]
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(W_{ij}\)</span>为连接<span class="math notranslate nohighlight">\(v_i\)</span>和<span class="math notranslate nohighlight">\(v_j\)</span>的边<span class="math notranslate nohighlight">\(e_{ij}\)</span>的权值。</p>
<p>  与连续版本一样，当<span class="math notranslate nohighlight">\(\phi(v_i)\)</span>的值比其周围的邻居大时(极大值)，离散拉普拉斯值较小；当<span class="math notranslate nohighlight">\(\phi(v_i)\)</span>的值比其周围的邻居小时（极小值），离散拉普拉斯值较大。</p>
<ol class="arabic simple">
<li><p><strong>例：图像Laplacian算子</strong></p></li>
</ol>
<p>  图像是一种离散型的数据，图像上的Laplacian算子可以大致进行如下运算。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\frac{\partial^2 f(x,y)}{\partial x^2}&amp;=f_x^{''}(x,y)\\
&amp;\approx f_x^{'}(x,y)-f_x^{'}(x-1,y)\\
&amp;\approx f(x+1,y)-f(x,y)-f(x,y)+f(x-1,y)\\
&amp;=f(x+1,y)+f(x-1,y)-2f(x,y)
\end{split}
\end{split}\]</div>
<p>同理，<span class="math notranslate nohighlight">\(\frac{\partial^2 f(x,y)}{\partial y^2}=f(x,y+1)+f(x,y-1)-2f(x,y)\)</span>。因此有Laplacian值，</p>
<div class="math notranslate nohighlight">
\[
\Delta f(x,y)=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)
\]</div>
<p>  可以得出<strong>结论</strong>：Laplacian算子近似等于所有方向（自由度）差分累积（增益）。</p>
<ol class="arabic simple" start="2">
<li><p><strong>例：图拉普拉斯算子</strong></p></li>
</ol>
<p>  图数据上的Laplacian算子又该如何应用呢？图由<span class="math notranslate nohighlight">\(N\)</span>个结点及其连接边权值<span class="math notranslate nohighlight">\(W\)</span>所构成。和图像Laplacian算子类似，图Laplacian算子可以近似等于所有方向(<span class="math notranslate nohighlight">\(\mathcal{N}_i\)</span>个邻接结点)的差分累积。</p>
<p>  对于任意结点<span class="math notranslate nohighlight">\(i\)</span>，可以通过映射<span class="math notranslate nohighlight">\(f: V\rightarrow R\)</span>得到值<span class="math notranslate nohighlight">\(f_i\)</span>。显然结点<span class="math notranslate nohighlight">\(i\)</span>的Laplacian就等于其所有邻接点的差分累积，即</p>
<div class="math notranslate nohighlight">
\[
(\Delta f)_i = \sum_{j\in\mathcal{N}_i}W_{ij}(f_j-f_i)
\]</div>
<p>因为<span class="math notranslate nohighlight">\(j\notin\mathcal{N}_i,W_{ij}=0\)</span>，上式可继续简化，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
(\Delta f)_i&amp;=\sum_j W_{ij}f_j -\sum_j W_{ij}f_i\\
&amp;=(Wf)_i-(Df)_i\\
&amp;=[(W-D)f]_i
\end{split}
\end{split}\]</div>
<p>若<span class="math notranslate nohighlight">\(F: \mathbb{R}^d \rightarrow \mathbb{R}^p\)</span>,则有，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\Delta \pmb{F}&amp;=\sum_{ij}\Vert \pmb{f}_i-\pmb{f}_j\Vert^2W_{ij}\\
&amp;=\sum_{ij}\pmb{f}_i^\top\pmb{f}_iW_{ij}-2\sum_{ij}\pmb{f}_i^\top W_{ij}\pmb{f}_j+\sum_{ij}\pmb{f}_j^\top\pmb{f}_jW_{ij}\\
&amp;=2\left(\sum_i \pmb{f}_i^\top D_{ii}\pmb{f}_i)-\sum_{ij}\pmb{f}_i^\top W_{ij}\pmb{f}_j\right)\\
&amp;=2\textrm{tr}\left(\pmb{Y}^\top\pmb{L}\pmb{Y} \right)
\end{split}
\end{split}\]</div>
</section>
<section id="id17">
<h3><span class="section-number">1.5.2. </span>拉普拉斯矩阵<a class="headerlink" href="#id17" title="此标题的永久链接">#</a></h3>
<p>  离散Laplacian算子表示为一个矩阵时，映射函数<span class="math notranslate nohighlight">\(\phi\)</span>可以写为列向量，<span class="math notranslate nohighlight">\(\Delta \phi\)</span>则表示为Laplacian矩阵<span class="math notranslate nohighlight">\(\pmb{L}\)</span>与列向量的乘积，即</p>
<div class="math notranslate nohighlight">
\[
\Delta\phi=\pmb{L}\times \phi
\]</div>
<p>Laplacian矩阵<span class="math notranslate nohighlight">\(\pmb{L}=\pmb{D}-\pmb{W}\)</span>。其中，</p>
<div class="math notranslate nohighlight">
\[
\pmb{D}_{ii}=\sum_j \pmb{W}_{ij}
\]</div>
</section>
<section id="id18">
<h3><span class="section-number">1.5.3. </span>拉普拉斯变换<a class="headerlink" href="#id18" title="此标题的永久链接">#</a></h3>
<p>  如果数据样本<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>与<span class="math notranslate nohighlight">\(\pmb{x}_j\)</span>很相似，则在拉普拉斯变换后的子空间与原空间一样，尽可能的接近。即，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\min\limits_{\pmb{Y}}\quad &amp;\textrm{tr}(\pmb{Y}^\top\pmb{L}\pmb{Y})\\ 
\textrm{s.t.}\quad &amp;\pmb{Y}^\top\pmb{DY}=\pmb{I}
\end{split}
\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{L}\)</span>为拉普拉斯矩阵，<span class="math notranslate nohighlight">\(\phi:\pmb{x}\rightarrow\pmb{y},\pmb{x}\in\mathbb{R}^d, \pmb{y}\in\mathbb{R}^p\)</span>。</p>
<p>  使用Lagrangian乘子法，可得，Lagrangian函数</p>
<div class="math notranslate nohighlight">
\[
f(\pmb{Y})=\textrm{tr}(\pmb{Y}^\top\pmb{L}\pmb{Y})+ \textrm{tr}[\pmb{\Lambda}(\pmb{Y}^\top\pmb{DY}-\pmb{I})]
\]</div>
<p>对其求偏导数，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\frac{\partial f}{\partial \pmb{Y}}&amp;=2\pmb{LY}+\pmb{D}^\top\pmb{Y}\pmb{\Lambda}^\top+\pmb{DY\Lambda}\\
&amp;=2\pmb{LY}+2\pmb{DY\Lambda}
\end{split}
\end{split}\]</div>
<p>令<span class="math notranslate nohighlight">\(\frac{\partial f}{\partial \pmb{Y}}=0\)</span>，可得<span class="math notranslate nohighlight">\(\pmb{LY}=-\pmb{DY\Lambda}\)</span>，即</p>
<div class="math notranslate nohighlight">
\[
\pmb{Ly}=\lambda\pmb{Dy}
\]</div>
<p>  因此，只需选择<span class="math notranslate nohighlight">\(p\)</span>个最小的特征值所对应的特征向量即可得到最优解<span class="math notranslate nohighlight">\(\hat{\pmb{Y}}\)</span>。</p>
</section>
</section>
<hr class="docutils" />
<section id="id19">
<h2><span class="section-number">1.6. </span>随机近邻嵌入<a class="headerlink" href="#id19" title="此标题的永久链接">#</a></h2>
<section id="sne">
<h3><span class="section-number">1.6.1. </span>SNE<a class="headerlink" href="#sne" title="此标题的永久链接">#</a></h3>
<p>  随机近邻嵌入(Stochastic Neighbor Embedding,SNE)的主要思想是：若两数据在高维空间相似(距离很近)，那么通过某种降维映射至2-3维空间时，它们应该离的很近。</p>
<p>  使用条件概率<span class="math notranslate nohighlight">\(p_{j|i}\)</span>来评价数据点<span class="math notranslate nohighlight">\(\pmb{x}_i,\pmb{x}_j\)</span>的相似性。高维空间的距离可以通过高斯分布进行计算，</p>
<div class="math notranslate nohighlight">
\[
\textrm{P}\{\textrm{高维空间}\pmb{x}_j是\pmb{x}_i的邻居\}\triangleq p_{j|i}=\frac{\exp(-\Vert\pmb{x}_i-\pmb{x}_j\Vert^2/(2\sigma_i^2))}{\sum_{k\neq i}\exp(-\Vert\pmb{x}_i-\pmb{x}_k\Vert^2/(2\sigma_i^2))}
\]</div>
<p>可以不用关心与自身的相似性，故可令<span class="math notranslate nohighlight">\(p_{i|i}=0\)</span>。假设经过低维映射<span class="math notranslate nohighlight">\(\phi\)</span>后<span class="math notranslate nohighlight">\(\pmb{x}_i\rightarrow \pmb{y}_i, \pmb{x}_j \rightarrow \pmb{y}_j\)</span>，则在该低维空间两点之间的相似性，即<span class="math notranslate nohighlight">\(\pmb{y}_j\)</span>是<span class="math notranslate nohighlight">\(\pmb{y}_i\)</span>邻居的条件概率<span class="math notranslate nohighlight">\(q_{j|i}\)</span>可以通过以下分布计算，</p>
<div class="math notranslate nohighlight">
\[
q_{j|i}=\frac{\exp(-\Vert \pmb{y}_i-\pmb{y}_j \Vert^2)}{\sum_{k\neq i}\exp(-\Vert \pmb{y}_i-\pmb{y}_k \Vert^2)}
\]</div>
<p>为简化计算，令<span class="math notranslate nohighlight">\(\sigma_i=2^{-1/2}\)</span>，同样有<span class="math notranslate nohighlight">\(q_{i|i}=0\)</span>。</p>
<p>  为确保<span class="math notranslate nohighlight">\(p_{j|i}\)</span>与<span class="math notranslate nohighlight">\(q_{j|i}\)</span>尽可能一致，可以通过KL散度来评估两个条件分布的差异性。因此，目标函数就变成了尽可能的降低<span class="math notranslate nohighlight">\(p_{j|i},q_{j|i}\)</span>的散度，即使用<span class="math notranslate nohighlight">\(q_{j|i}\)</span>替代<span class="math notranslate nohighlight">\(p_{j|i}\)</span>的信息损失，</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}=\sum_i\sum_j p_{j|i}\log\frac{p_{j|i}}{q_{j|i}}
\]</div>
<p>  梯度法求解上述问题，对<span class="math notranslate nohighlight">\(\pmb{y}_i\)</span>求偏导，可得梯度</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \pmb{y}_i}=2\sum_j (\pmb{y}_i-\pmb{y}_j)(p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j})
\]</div>
<div style="background-color: lightgray">
<p>  提示：使用链式法则求解。令<span class="math notranslate nohighlight">\(p_{ij}=p_{i|j}\)</span>, <span class="math notranslate nohighlight">\(q_{ij}=q_{i|j}\)</span>,以及，</p>
<div class="math notranslate nohighlight">
\[
w_{ij}=\exp(-\Vert \pmb{y}_i-\pmb{y}_j \Vert^2),\quad f_{ij}= \Vert \pmb{y}_i-\pmb{y}_j \Vert^2, \quad d_{ij}=\Vert \pmb{y}_i-\pmb{y}_j \Vert
\]</div>
<p>由链式法则可知，</p>
<div class="math notranslate nohighlight">
\[
\begin{split}
\frac{\partial \mathcal{L}}{\partial \pmb{y}_n} &amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\sum_{kl}\frac{\partial q_{ij}}{w_{kl}}\sum_{mn}\frac{\partial w_{kl}}{\partial f_{mn}}\sum_{pq}\frac{\partial f_{mn}}{\partial d_{pq}}\frac{\partial d_{pq}}{\partial \pmb{y}_n}
\end{split}
\]</div>
<p>显然，只有当<span class="math notranslate nohighlight">\(pq=mn=kl\)</span>时，偏导数才不为0，因此，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\frac{\partial \mathcal{L}}{\partial \pmb{y}_n} &amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\sum_{kl}\frac{\partial q_{ij}}{w_{kl}}\sum_{mn}\frac{\partial w_{kl}}{\partial f_{mn}}\sum_{pq}\frac{\partial f_{mn}}{\partial d_{pq}}\frac{\partial d_{pq}}{\partial \pmb{y}_n}\\
&amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\sum_{kl}\frac{\partial q_{ij}}{w_{kl}} \frac{\partial w_{kl}}{\partial f_{kl}}\frac{\partial f_{kl}}{\partial d_{kl}}\frac{\partial d_{kl}}{\partial \pmb{y}_n}\\
&amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\sum_{l}\frac{\partial q_{ij}}{w_{il}} \frac{\partial w_{il}}{\partial f_{il}}\frac{\partial f_{il}}{\partial d_{il}}\frac{\partial d_{il}}{\partial \pmb{y}_n}\\
\end{split}
\end{split}\]</div>
<p>由于<span class="math notranslate nohighlight">\(w_{il}\)</span>涉及<span class="math notranslate nohighlight">\(\pmb{y}_n\)</span>分两种情况：<span class="math notranslate nohighlight">\(i=n\)</span>或<span class="math notranslate nohighlight">\(l=n\)</span>,因此，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\frac{\partial \mathcal{L}}{\partial \pmb{y}_n} 
&amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\sum_{l}\frac{\partial q_{ij}}{w_{il}} \frac{\partial w_{il}}{\partial f_{il}}\frac{\partial f_{il}}{\partial d_{il}}\frac{\partial d_{il}}{\partial \pmb{y}_n}\\
&amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\frac{\partial q_{ij}}{w_{in}} \frac{\partial w_{in}}{\partial f_{in}}\frac{\partial f_{in}}{\partial d_{in}}\frac{\partial d_{in}}{\partial \pmb{y}_n} +  \sum_{j}\frac{\partial \mathcal{L}}{\partial q_{nj}}\sum_{l}\frac{\partial q_{nj}}{w_{nl}} \frac{\partial w_{nl}}{\partial f_{nl}}\frac{\partial f_{nl}}{\partial d_{nl}}\frac{\partial d_{nl}}{\partial \pmb{y}_n}
\end{split}
\end{split}\]</div>
<p>整理：<span class="math notranslate nohighlight">\(n\rightarrow i\)</span>，左式：<span class="math notranslate nohighlight">\(i\rightarrow j, j\rightarrow k\)</span>, 右式： <span class="math notranslate nohighlight">\(j\rightarrow k, l\rightarrow j\)</span>, 以及<span class="math notranslate nohighlight">\(f_{ij}=f{ji}, d_{ij}=d_{ji}\)</span>，可得</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\frac{\partial \mathcal{L}}{\partial \pmb{y}_n} 
&amp;=\sum_{ij}\frac{\partial \mathcal{L}}{\partial q_{ij}}\frac{\partial q_{ij}}{w_{in}} \frac{\partial w_{in}}{\partial f_{in}}\frac{\partial f_{in}}{\partial d_{in}}\frac{\partial d_{in}}{\partial \pmb{y}_n} +  \sum_{j}\frac{\partial \mathcal{L}}{\partial q_{nj}}\sum_{l}\frac{\partial q_{nj}}{w_{nl}} \frac{\partial w_{nl}}{\partial f_{nl}}\frac{\partial f_{nl}}{\partial d_{nl}}\frac{\partial d_{nl}}{\partial \pmb{y}_n}\\
&amp;=\sum_{jk}\frac{\partial \mathcal{L}}{\partial q_{jk}}\frac{\partial q_{jk}}{w_{ji}} \frac{\partial w_{ji}}{\partial f_{ji}}\frac{\partial f_{ji}}{\partial d_{ji}}\frac{\partial d_{ji}}{\partial \pmb{y}_i} + \sum_{jk}\frac{\partial \mathcal{L}}{\partial q_{ik}}\frac{\partial q_{ik}}{w_{ij}} \frac{\partial w_{ij}}{\partial f_{ij}}\frac{\partial f_{ij}}{\partial d_{ij}}\frac{\partial d_{ij}}{\partial \pmb{y}_i}\\
&amp;=\sum_j\left(   \underbrace{\sum_k \frac{\partial \mathcal{L}}{\partial q_{jk}}\frac{\partial q_{jk}}{w_{ji}} \frac{\partial w_{ji}}{\partial f_{ji}} }_{t_{ji}}   +  \underbrace{\sum_k \frac{\partial \mathcal{L}}{\partial q_{ik}}\frac{\partial q_{ik}}{w_{ij}} \frac{\partial w_{ij}}{\partial f_{ij}}}_{t_{ij}}  \right)\frac{\partial f_{ij}}{\partial d_{ij}}\frac{\partial d_{ij}}{\partial \pmb{y}_i}
\end{split}
\end{split}\]</div>
<p>  注意到，<span class="math notranslate nohighlight">\(q_{ij}=\frac{w_{ij}}{\sum_l w_{il}}\triangleq \frac{w_{ij}}{S_i}\)</span>，则</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial q_{ij}}{\partial w_{ij}}=\frac{1}{S_i}-\frac{q_{ij}}{S_i},\quad \frac{\partial q_{ik}}{\partial w_{ij}}=-\frac{q_{ik}}{S_i}
\]</div>
<p>代入<span class="math notranslate nohighlight">\(t_{ij}\)</span>，可得</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
t_{ij}&amp;=\sum_k \frac{\partial \mathcal{L}}{\partial q_{ik}}\frac{\partial q_{ik}}{w_{ij}} \frac{\partial w_{ij}}{\partial f_{ij}}\\
&amp;=\left( \frac{\partial \mathcal{L}}{\partial q_{ij}}\frac{\partial q_{ij}}{w_{ij}} +\sum_{k\neq j}\frac{\partial \mathcal{L}}{\partial q_{ik}}\frac{\partial q_{ik}}{w_{ij}} \right)\frac{\partial w_{ij}}{\partial f_{ij}}\\
&amp;=\left( \frac{1}{S_i} \frac{\partial \mathcal{L}}{\partial q_{ij}}  -\frac{q_{ij}}{S_i}\frac{\partial \mathcal{L}}{\partial q_{ij}} - \sum_{k\neq j}\frac{\partial \mathcal{L}}{\partial q_{ik}}\frac{q_{ik}}{S_i}        \right)\frac{\partial w_{ij}}{\partial f_{ij}}\\
&amp;=\frac{1}{S_i}\left(\frac{\partial \mathcal{L}}{\partial q_{ij}}  - \sum_{k}\frac{\partial \mathcal{L}}{\partial q_{ik}}q_{ik}  \right)\frac{\partial w_{ij}}{\partial f_{ij}}\\
&amp;=\frac{1}{S_i}\left(\frac{-p_{ij} }{q_{ij} }  - \sum_{k}\frac{-p_{ik} }{q_{ik} }q_{ik}  \right)(-w_{ij})\\
&amp;=-q_{ij}\left( \frac{-p_{ij} }{q_{ij} }+\sum_k p_{ik} \right)\\
&amp;=p_{ij}-q_{ij}
\end{split}
\end{split}\]</div>
<p>同理可求得<span class="math notranslate nohighlight">\(t_{ji}\)</span>，最后整理可得梯度为，</p>
<div class="math notranslate nohighlight">
\[
\boxed{\frac{\partial \mathcal{L}}{\partial \pmb{y}_i}=2\sum_j (\pmb{y}_i-\pmb{y}_j)(p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j})}
\]</div>
</div>
<ul class="simple">
<li><p>存在的问题</p></li>
</ul>
<ol class="arabic simple">
<li><p><strong>只关注数据的局部性，忽略了数据的全局性</strong>。对于目标函数来说，当<span class="math notranslate nohighlight">\(p_{j|i}\)</span>较大<span class="math notranslate nohighlight">\(q_{j|i}\)</span>较小时，代价较高；当<span class="math notranslate nohighlight">\(p_{j|i}\)</span>较小<span class="math notranslate nohighlight">\(q_{j|i}\)</span>较大时，代价较小；也就是说，高维空间数据点较近时，映射至低维空间后较远，那么会得到一个较高的惩罚，这是合理的。反之，高维空间较远的点映射后距离较近将会得到一个较低的惩罚，这就有问题了，这里应该得到一个较高的惩罚才合理。</p></li>
<li><p><strong>不对称性</strong>。<span class="math notranslate nohighlight">\(p_{j|i}\neq p_{i|j}, q_{j|i}\neq q_{i|j}\)</span>导致梯度计算量过大。</p></li>
<li><p><strong>拥挤问题</strong>。不同的类簇挤在一起即为拥挤问题。</p></li>
</ol>
</section>
<section id="id20">
<h3><span class="section-number">1.6.2. </span>对称SNE<a class="headerlink" href="#id20" title="此标题的永久链接">#</a></h3>
<p>  针对SNE的不对称问题</p>
</section>
</section>
</section>


                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">上一页</p>
        <p class="prev-next-title">机器学习基础</p>
      </div>
    </a>
    <a class="right-next"
       href="../kernel/base.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">下一页</p>
        <p class="prev-next-title"><span class="section-number">1. </span>核函数基础</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> 目录
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.1. 主成分分析</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">1.1.1. 最近重构性</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">1.1.2. 最大可分性</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">1.1.3. 优化问题的求解</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">1.1.4. 算法1–特征值分解</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svd">1.1.5. 算法2–SVD分解</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">1.1.6. 核主成分分析</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">1.2. 多维缩放</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">1.2.1. 算法</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">1.3. 等度量映射</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">1.3.1. 算法</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">1.3.2. 流形</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lle">1.4. LLE局部线性嵌入</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">1.4.1. LLE基本思想</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">1.4.2. LLE求解</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">1.5. 拉普拉斯特征映射</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">1.5.1. 拉普拉斯算子</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">1.5.2. 拉普拉斯矩阵</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">1.5.3. 拉普拉斯变换</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">1.6. 随机近邻嵌入</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sne">1.6.1. SNE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">1.6.2. 对称SNE</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
作者： SSPUIIP
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022, SSPUIIP.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>