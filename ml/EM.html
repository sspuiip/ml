<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3. EM算法概述 &#8212; Machine Learning Fundation 1.0 文档</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/nature.css?v=279e0f84" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <script src="../_static/documentation_options.js?v=f115507d"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="../_static/mathjax/tex-chtml.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="4. 概率图模型" href="PGM.html" />
    <link rel="prev" title="2. 表示学习" href="neuro_representation.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             accesskey="I">索引</a></li>
        <li class="right" >
          <a href="PGM.html" title="4. 概率图模型"
             accesskey="N">下一页</a> |</li>
        <li class="right" >
          <a href="neuro_representation.html" title="2. 表示学习"
             accesskey="P">上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">3. </span>EM算法概述</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="em">
<h1><span class="section-number">3. </span>EM算法概述<a class="headerlink" href="#em" title="Link to this heading">¶</a></h1>
<blockquote>
<div><p>  简言之，EM算法主要用于含隐变量概率模型的参数极大似然(或后验)估计。</p>
</div></blockquote>
<p>  概率模型一般都含有观测变量、隐变量。如果只有观测变量，则可以直接使用极大似然估计；如果模型含有隐变量时，则需要对极大似然估计法进行改进，这种改进就是EM算法。EM算法也称为含有隐变量概率模型的参数极大化似然估计法。<strong>EM算法是一种迭代算法，每次迭代由两步组成：E步，求隐变量的期望；M步，求模型的最大化</strong>。因此，这一算法也称为期望极大算法（expectation maximinzation algorithm）。</p>
<section id="id1">
<h2><span class="section-number">3.1. </span>EM算法<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h2>
<p>  令<span class="math notranslate nohighlight">\(\pmb{y}_i\)</span>为数据<span class="math notranslate nohighlight">\(i\)</span>的可观测变量，<span class="math notranslate nohighlight">\(\pmb{z}_i\)</span>为隐变量或缺失变量，则可观测数据的最大对数似然为，</p>
<div class="math notranslate nohighlight">
\[
\ell(\pmb{\theta})=\log P(\pmb{Y}|\pmb{\theta})=\log \sum_{\pmb{Z}}P(\pmb{Y},\pmb{Z}|\pmb{\theta})
\tag{1}
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{Y}=[\pmb{y}_1,...,\pmb{y}_n]^\top, \pmb{Z}=[\pmb{z}_1,...,\pmb{z}_n]^\top\)</span>, <span class="math notranslate nohighlight">\(\log P(\pmb{Y}|\pmb{\theta})\)</span>称为<strong>不完全数据对数似然</strong>。很明显，这个问题难于优化，因为<span class="math notranslate nohighlight">\(\log\)</span>不能被塞进到<code class="docutils literal notranslate"><span class="pre">sum</span></code>求和运算中(包含和的对数)。</p>
<p>  EM算法绕开这个问题如下，首先定义<strong>完全数据对数似然</strong>（complete data log likelihood）,</p>
<div class="math notranslate nohighlight">
\[
\ell_c(\pmb{\theta})\triangleq\log P(\pmb{Y},\pmb{Z}|\pmb{\theta})
\tag{2}
\]</div>
<p>因为<span class="math notranslate nohighlight">\(\pmb{z}_i\)</span>未知，上式不能直接计算。可以定义<strong>期望的完全数据对数似然</strong>(expected complete data log likelihood)，</p>
<div class="math notranslate nohighlight">
\[
Q(\pmb{\theta},\pmb{\theta}^{t-1})=\mathbb{E}_{\pmb{z}}[\ell_c (\pmb{\theta}|\mathcal{D},\pmb{\theta}^{t-1})]=\sum_{\pmb{Z}}P(\pmb{Z}|\pmb{Y},\pmb{\theta}^{t-1}) P(\pmb{Y},\pmb{Z}|\pmb{\theta})
\tag{3}
\]</div>
<p>其中<span class="math notranslate nohighlight">\(t\)</span>为当前迭代，<span class="math notranslate nohighlight">\(Q\)</span>为辅助函数(auxiliary function)，期望是关于旧参数<span class="math notranslate nohighlight">\(\pmb{\theta}^{t-1}\)</span>和观测数据<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>的。E步的目标正是计算<span class="math notranslate nohighlight">\(Q\)</span>函数。M步的目标为优化<span class="math notranslate nohighlight">\(Q\)</span>函数的参数<span class="math notranslate nohighlight">\(\pmb{\theta}\)</span>，</p>
<div class="math notranslate nohighlight">
\[
\pmb{\theta}^t =\arg\max\limits_{\pmb{\theta}} Q(\pmb{\theta},\pmb{\theta}^{t-1})
\]</div>
<p>最大后验则对应以下优化，</p>
<div class="math notranslate nohighlight">
\[
\pmb{\theta}^t =\arg\max\limits_{\pmb{\theta}} Q(\pmb{\theta},\pmb{\theta}^{t-1})+\log p(\pmb{\theta})
\]</div>
<section id="id2">
<h3><span class="section-number">3.1.1. </span>EM算法推导<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h3>
<p>  对于一个含有隐变量的概率模型，目标是极大化观测数据<span class="math notranslate nohighlight">\(Y\)</span>关于参数<span class="math notranslate nohighlight">\(\pmb{\theta}\)</span>的对数似然函数，即极大化下式，</p>
<div class="math notranslate nohighlight">
\[
\ell(\pmb{\theta})=\log p(\pmb{Y}|\pmb{\theta})=\log\sum_{\pmb{Z}}p(\pmb{Y},\pmb{Z}|\pmb{\theta})
\]</div>
<p>上式的极大化困难主要是含有未观测变量。EM通过迭代的方法逐步近似极大化<span class="math notranslate nohighlight">\(\ell(\pmb{\theta})\)</span>，希望达到<span class="math notranslate nohighlight">\(\ell(\pmb{\theta}^t)&gt;\ell(\pmb{\theta}^{t-1})\)</span>。现考虑，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\ell(\pmb{\theta})-\ell(\pmb{\theta}^{i})&amp;=\log\left(\sum_{\pmb{Z}} p(\pmb{Y}|\pmb{Z},\pmb{\theta})p(\pmb{Z}|\pmb{\theta}) \right)-\log p(\pmb{Y}|\pmb{\theta}^i)\\
&amp;=\log\left(\sum_{\pmb{Z}} p(\pmb{Z}|\pmb{Y},\pmb{\theta}^i)   \frac{p(\pmb{Y}|\pmb{Z},\pmb{\theta})p(\pmb{Z}|\pmb{\theta})}{p(\pmb{Z}|\pmb{Y},\pmb{\theta}^i)} \right)-\log p(\pmb{Y}|\pmb{\theta}^i)\\
&amp;\ge \sum_{\pmb{Z}} p(\pmb{Z}|\pmb{Y},\pmb{\theta}^i)\log\frac{p(\pmb{Y}|\pmb{Z},\pmb{\theta})p(\pmb{Z}|\pmb{\theta})}{p(\pmb{Z}|\pmb{Y},\pmb{\theta}^i)} -\log p(\pmb{Y}|\pmb{\theta}^i)\\
&amp;=\sum_{\pmb{Z}} p(\pmb{Z}|\pmb{Y},\pmb{\theta}^i)\log\frac{p(\pmb{Y}|\pmb{Z},\pmb{\theta})p(\pmb{Z}|\pmb{\theta})}{p(\pmb{Z}|\pmb{Y},\pmb{\theta}^i)p(\pmb{Y}|\pmb{\theta}^i)}
\end{split}
\end{split}\]</div>
<p>则有，</p>
<div class="math notranslate nohighlight">
\[
\ell(\pmb{\theta})\ge\ell(\pmb{\theta}^{i})+\sum_{\pmb{Z}} p(\pmb{Z}|\pmb{Y},\pmb{\theta}^i)\log\frac{p(\pmb{Y}|\pmb{Z},\pmb{\theta})p(\pmb{Z}|\pmb{\theta})}{p(\pmb{Z}|\pmb{Y},\pmb{\theta}^i)p(\pmb{Y}|\pmb{\theta}^i)}\triangleq \underbrace{B(\pmb{\theta},\pmb{\theta}^i)}_{\mathrm{\ell(\pmb{\theta})的下界}}
\]</div>
<p>显然有<span class="math notranslate nohighlight">\( \ell(\pmb{\theta}^i)=B(\pmb{\theta}^i,\pmb{\theta}^i)\)</span>成立。因此，可以使<span class="math notranslate nohighlight">\(B(\pmb{\theta},\pmb{\theta}^i)\)</span>增大的任何<span class="math notranslate nohighlight">\(\pmb{\theta}\)</span>都可以使得<span class="math notranslate nohighlight">\(\ell(\pmb{\theta})\)</span>增大。<span class="math notranslate nohighlight">\(B(\pmb{\theta},\pmb{\theta}^i)\)</span>也称之为<span class="math notranslate nohighlight">\(\ell(\pmb{\theta})\)</span>的一个<strong>下界</strong>。为此，可以极大化<span class="math notranslate nohighlight">\(B(\pmb{\theta},\pmb{\theta}^i)\)</span>，即</p>
<div class="math notranslate nohighlight">
\[
\pmb{\theta}^{t+1}=\arg\max\limits_{\pmb{\theta}}B(\pmb{\theta},\pmb{\theta}^i)
\]</div>
<p>具体地，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\pmb{\theta}^{t+1}&amp;=\arg\max\limits_{\pmb{\theta}}B(\pmb{\theta},\pmb{\theta}^i)\\
&amp;=\arg\max\limits_{\pmb{\theta}}\left( \ell(\pmb{\theta}^{i})+\sum_{\pmb{Z}} p(\pmb{Z}|\pmb{Y},\pmb{\theta}^i)\log\frac{p(\pmb{Y}|\pmb{Z},\pmb{\theta})p(\pmb{Z}|\pmb{\theta})}{p(\pmb{Z}|\pmb{Y},\pmb{\theta}^i)p(\pmb{Y}|\pmb{\theta}^i)}\right)\\
&amp;=\arg\max\limits_{\pmb{\theta}}\left( \sum_{\pmb{Z}} p(\pmb{Z}|\pmb{Y},\pmb{\theta}^i)\log p(\pmb{Y}|\pmb{Z},\pmb{\theta})p(\pmb{Z}|\pmb{\theta}) \right)\\
&amp;= \arg\max\limits_{\pmb{\theta}}\left( \sum_{\pmb{Z}} p(\pmb{Z}|\pmb{Y},\pmb{\theta}^i) \log p(\pmb{Y},\pmb{Z}|\pmb{\theta})  \right)\\
&amp;=\arg\max\limits_{\pmb{\theta}} Q(\pmb{\theta},\pmb{\theta}^{t})
\end{split}
\end{split}\]</div>
<p>到此，EM算法每次迭代都是通过不断极大化下界来极大化对数似然函数，从而实现隐变量概率模型的极大似然估计。</p>
</section>
<section id="id3">
<h3><span class="section-number">3.1.2. </span>案例<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h3>
<p>  假设有3枚硬币<span class="math notranslate nohighlight">\(A,B,C\)</span>，按以下规则投掷，并记录实验结果。先投掷<span class="math notranslate nohighlight">\(A\)</span>，若<span class="math notranslate nohighlight">\(A\)</span>的结果为正面，则投掷硬币<span class="math notranslate nohighlight">\(B\)</span>的结果记录为实验结果；若<span class="math notranslate nohighlight">\(A\)</span>的结果为反面，则投掷硬币<span class="math notranslate nohighlight">\(C\)</span>的结果记录为实验结果。假设整个实验只能观测到实验结果，不能观测到掷硬币的过程，独立重复<span class="math notranslate nohighlight">\(n\)</span>次试验，若观测到以下结果：</p>
<div class="math notranslate nohighlight">
\[
1,1,0,1,0,0,0,1,1,0,...
\]</div>
<p>问这3枚硬币正面出现的概率是多少？</p>
<p>  现假设硬币<span class="math notranslate nohighlight">\(A,B,C\)</span>正面向上的概率分别为<span class="math notranslate nohighlight">\(\pi,p,q\)</span>，<span class="math notranslate nohighlight">\(y\)</span>为一次实验的观测结果，<span class="math notranslate nohighlight">\(z\)</span>为<span class="math notranslate nohighlight">\(A\)</span>的投掷结果，则一次投掷的过程可以用以下模型来建模，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
p(y|\theta)&amp;=\sum_zp(y,z|\theta)=\sum_zp(y|z,\theta)p(z)\\
&amp;=p(y|z=1)p(z=1)+p(y|z=0)p(z=0)\\
&amp;= p^y(1-p)^{1-y}\pi+q^y(1-q)^{1-y}(1-\pi)
\end{split}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(n\)</span>次实验结果，可以表示为，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\mathcal{L}(\theta)&amp;=\prod_{j=1}^n p(y_j|\theta)\\
&amp;=\prod_{j=1}^n \left[\pi p^{y_j}(1-p)^{1-y_j}+(1-\pi)q^{y_j}(1-q)^{1-y_j}\right]
\end{split}
\end{split}\]</div>
<p>取对数，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\ell(\theta)&amp;=\sum_{j=1}^n\log p(y_j|\theta)\\
&amp;=\sum_{j=1}^n\log \left[p(z=1)p(y|z=1)+p(z=0)p(y|z=0)\right] \\
&amp;=\sum_{j=1}^n\log \left[\pi p^{y_j}(1-p)^{1-y_j}+(1-\pi)q^{y_j}(1-q)^{1-y_j}\right]
\end{split}
\end{split}\]</div>
<p>对该模型使用极大似然估计法，即可得最优参数值<span class="math notranslate nohighlight">\(\hat{\theta}=(\hat{\pi},\hat{p},\hat{q})\)</span>，</p>
<div class="math notranslate nohighlight">
\[
\hat{\theta}=\arg\max\limits_{\theta}\ell(\theta)
\]</div>
<p>  由于含有隐变量<span class="math notranslate nohighlight">\(z\)</span>，没有确定的观测值，上述问题没有解析解。可以通过迭代的方法求解。EM算法就是一种解决这类问题的迭代方法。</p>
<p>首先，对所有参数赋初值，<span class="math notranslate nohighlight">\(\theta^{(0)}=(\pi^{(0)},p^{(0)},q^{(0)})\)</span>，然后迭代计算参数的估计值，直至收敛。</p>
<ul class="simple">
<li><p>E步： 计算<span class="math notranslate nohighlight">\(\mathbb{E}_{z_j}[\log p(y_j,z_j)]\)</span>。 由题设可知(使用逆概公式)</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
p(z_j=1|y_j)&amp;=\frac{p(y_j|z_j=1)p(z_j=1)}{\sum_{z_j} p(y|z_j)p(z_j)}\\
&amp;=\frac{p(y_j|z_j=1)p(z_j=1)}{p(y_j|z_j=1)p(z_j=1)+p(y_j|z_j=0)p(z_j=0)}\\
&amp;=\frac{ \pi^{(i)} {p^{(i)}}^{y_j}(1-p^{(i)})^{1-y_j} }{\pi^{(i)} {p^{(i)}}^{y_j}(1-p^{(i)})^{1-y_j}+(1-\pi^{(i)}) {q^{(i)}}^{y_j}(1-p^{(i)})
^{1-y_j}  }\\
&amp;\triangleq\mu_j^{(i+1)}
\end{split}
\end{split}\]</div>
<ul class="simple">
<li><p>M步：计算参数的新估计值</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\pi^{(i+1)}&amp;=\frac1n\sum_{j=1}^n\mu_j^{(i+1)}\\
p^{(i+1)}&amp;=\frac{\sum_{j=1}^n\mu_j^{(i+1)}y_j}{\sum_{j=1}^n\mu_j^{(i+1)}}\\
q^{(i+1)}&amp;=\frac{\sum_{j=1}^n(1-\mu_j^{(i+1)})y_j}{\sum_{j=1}^n(1-\mu_j^{(i+1)})}
\end{split}
\end{split}\]</div>
<p>  注：上例中<span class="math notranslate nohighlight">\(Q(\pmb{\theta}|\pmb{\theta}^t)\)</span>计算如下，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
Q(\pmb{\theta}|\pmb{\theta}^t)&amp;= \mathbb{E}_{\pmb{Z}} [\log P(\pmb{Y},\pmb{Z}|\pmb{\theta})]\\
&amp;=\sum_{i}^N \mathbb{E}_{\pmb{Z}} [ \log P(y_i,z_i|\pmb{\theta}^t)]   \\
&amp;=\sum_{i}^N \mathbb{E}_{\pmb{Z}} [ \log (\pi p^{y_i}(1-p)^{1-y_i})^{\mathbb{I}(z_i=1)}((1-\pi) q^{y_i}(1-q)^{1-y_i})^{\mathbb{I}(z_i=0)}]   \\
&amp;=\sum_i P(z_i=1|y_i)\log (\pi p^{y_i}(1-p)^{1-y_i})+P(z_i=0|y_i)\log ((1-\pi) q^{y_i}(1-q)^{1-y_i})\\
&amp;=\sum_i\mu_j^{(t+1)}\log [\pi p^{y_i}(1-p)^{1-y_i}]+(1-\mu_j^{(t+1)})\log [(1-\pi) q^{y_i}(1-q)^{1-y_i}]
\end{split}
\end{split}\]</div>
</section>
<section id="id4">
<h3><span class="section-number">3.1.3. </span>实验代码<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># -*- coding: utf-8 -*-</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">@author: jimilaw</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">getY</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">EStep</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">Y</span><span class="p">):</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">pi</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">pi</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">pi</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">q</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">q</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">numerator</span><span class="o">/</span><span class="n">denominator</span>
    
   
<span class="n">MAX_ITERATION</span><span class="o">=</span><span class="mi">1000</span>

<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span><span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    
    <span class="n">Y</span><span class="o">=</span><span class="n">getY</span><span class="p">()</span>
    <span class="n">n</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="nb">iter</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="c1"># init param</span>
    <span class="n">pi</span><span class="o">=</span><span class="mf">0.5</span>
    <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span>
    <span class="n">q</span><span class="o">=</span><span class="mf">0.5</span>
    
    <span class="k">while</span> <span class="nb">iter</span> <span class="o">&lt;</span> <span class="n">MAX_ITERATION</span><span class="p">:</span>
        <span class="c1"># 1. E步 calculate the expectation of log prob. w.r.t. p(z)</span>
        <span class="nb">iter</span> <span class="o">=</span> <span class="nb">iter</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">Mu</span> <span class="o">=</span> <span class="n">EStep</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        
        <span class="c1"># 2. M步 update param</span>
        <span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Mu</span><span class="p">)</span><span class="o">/</span><span class="n">n</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Mu</span><span class="o">*</span><span class="n">Y</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Mu</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">Mu</span><span class="p">)</span><span class="o">*</span><span class="n">Y</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Mu</span><span class="p">)</span>
        
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;最终解：pi=&quot;</span><span class="p">,</span><span class="n">pi</span><span class="p">,</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">p=&quot;</span><span class="p">,</span><span class="n">p</span><span class="p">,</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">q=&quot;</span><span class="p">,</span><span class="n">q</span><span class="p">)</span>
        
</pre></div>
</div>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">目录</a></h3>
    <ul>
<li><a class="reference internal" href="#">3. EM算法概述</a><ul>
<li><a class="reference internal" href="#id1">3.1. EM算法</a><ul>
<li><a class="reference internal" href="#id2">3.1.1. EM算法推导</a></li>
<li><a class="reference internal" href="#id3">3.1.2. 案例</a></li>
<li><a class="reference internal" href="#id4">3.1.3. 实验代码</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>上一主题</h4>
    <p class="topless"><a href="neuro_representation.html"
                          title="上一章"><span class="section-number">2. </span>表示学习</a></p>
  </div>
  <div>
    <h4>下一主题</h4>
    <p class="topless"><a href="PGM.html"
                          title="下一章"><span class="section-number">4. </span>概率图模型</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>本页</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/ml/EM.md.txt"
            rel="nofollow">显示源代码</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">快速搜索</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="提交" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             >索引</a></li>
        <li class="right" >
          <a href="PGM.html" title="4. 概率图模型"
             >下一页</a> |</li>
        <li class="right" >
          <a href="neuro_representation.html" title="2. 表示学习"
             >上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">3. </span>EM算法概述</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; 版权所有 2022-2024, SSPUIIP.
      由 <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3创建。
    </div>
  </body>
</html>