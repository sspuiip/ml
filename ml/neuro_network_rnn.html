<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3. 循环神经网络 &#8212; Machine Learning Fundation 1.0 文档</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/nature.css?v=279e0f84" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <script src="../_static/documentation_options.js?v=f115507d"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="../_static/mathjax/tex-chtml.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="1. 聚类(一)" href="cmeans.html" />
    <link rel="prev" title="2. 卷积神经网络" href="neuro_network_cnn.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             accesskey="I">索引</a></li>
        <li class="right" >
          <a href="cmeans.html" title="1. 聚类(一)"
             accesskey="N">下一页</a> |</li>
        <li class="right" >
          <a href="neuro_network_cnn.html" title="2. 卷积神经网络"
             accesskey="P">上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">3. </span>循环神经网络</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1><span class="section-number">3. </span>循环神经网络<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h1>
<section id="word2vec">
<h2><span class="section-number">3.1. </span>Word2Vec<a class="headerlink" href="#word2vec" title="Link to this heading">¶</a></h2>
<p>  Word2Vec是语言模型的神经网络建模实现。其基本原理如下图所示：</p>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="../_images/word2vec_cbow.png"><img alt="CBOW" src="../_images/word2vec_cbow.png" style="width: 500px;" />
</a>
<figcaption>
<p><span class="caption-text">连续词袋模型的神经网络模型</span><a class="headerlink" href="#id5" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>上图中的模型假设单词向量(one-hot)长度为5，中间层长度为4。训练好之后的输入层与中间层之间的参数<span class="math notranslate nohighlight">\(\pmb{W}_{in}\)</span>即为所有单词的<strong>词向量</strong>表示。<strong>词向量表示类似于颜色空间的RGB表示</strong>。通过词向量间的运算可以得到类似词与词之间的距离，相似度等效果。例如：“国王 - 男人 + 女人 = 女王”、“猫”靠近“狗”等。 词向量实现了词到向量的转变，可直接迁移至下游任务（文本分类、机器翻译等），‌减少训练成本‌并提升数据集效果。</p>
<section id="id2">
<h3><span class="section-number">3.1.1. </span>语言模型<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h3>
<p>  对于给定的单词序列<span class="math notranslate nohighlight">\(x_1,x_2,...,x_t\)</span>，其出现的概率可以使用联合概率来评估，即</p>
<div class="math notranslate nohighlight" id="equation-joint-words-prob">
<span class="eqno">(1)<a class="headerlink" href="#equation-joint-words-prob" title="Link to this equation">¶</a></span>\[\begin{split}
\begin{split}
P(x_1,x_2,...,x_t)&amp;=P(x_t|x_{t-1},...,x_1)P(x_{t-2},...,x_1)...P(x_2|x_1)P(x_1)\\
&amp;=\prod_{i=1}^t P(x_i|x_{i-1},...,x_1)
\end{split}
\end{split}\]</div>
<p>该式<a class="reference internal" href="#equation-joint-words-prob">(1)</a>即为单词序列<span class="math notranslate nohighlight">\(x_1,x_2,...,x_t\)</span>出现的概率，也称之为<strong>语言模型</strong>。一个理想的语言模型可以根据训练好的式<a class="reference internal" href="#equation-joint-words-prob">(1)</a>生成文本。</p>
</section>
<section id="cbow">
<h3><span class="section-number">3.1.2. </span>CBOW应用于语言模型<a class="headerlink" href="#cbow" title="Link to this heading">¶</a></h3>
<p>  CBOW（Continuous Bag-of-Words，连续词袋模型）是基于上下文单词预测当前中心词的模型，与SKIP正好相反。例如：给定句子“the cat sat on the mat”，若中心词为“sat”，则上下文窗口（假设窗口大小为2）为[“the”, “cat”, “on”, “the”]，模型通过聚合这些上下文词的信息预测“sat”。CBOW应用于语言模型是指用固定窗口大小的单词依赖来近似式式<a class="reference internal" href="#equation-joint-words-prob">(1)</a>中的后验概率，即</p>
<div class="math notranslate nohighlight" id="equation-approx-posterior">
<span class="eqno">(2)<a class="headerlink" href="#equation-approx-posterior" title="Link to this equation">¶</a></span>\[
P(x_t|x_{t-1},...,x_1)\approx P(x_t|x_{t-1},x_{t-2},...,x_{t-k+1})
\]</div>
<p>其中<span class="math notranslate nohighlight">\(k\)</span>为固定窗口大小。窗口可以是单边，也可以是双边。以下是CBOW的简单示例。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SimpleCBOW</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="n">V</span><span class="p">,</span> <span class="n">H</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">hidden_size</span>

        <span class="c1"># 初始化权重</span>
        <span class="n">W_in</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>
        <span class="n">W_out</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>

        <span class="c1"># 生成层</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_layer0</span> <span class="o">=</span> <span class="n">MatMul</span><span class="p">(</span><span class="n">W_in</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_layer1</span> <span class="o">=</span> <span class="n">MatMul</span><span class="p">(</span><span class="n">W_in</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">MatMul</span><span class="p">(</span><span class="n">W_out</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_layer</span> <span class="o">=</span> <span class="n">SoftmaxWithLoss</span><span class="p">()</span>

        <span class="c1"># 将所有的权重和梯度整理到列表中</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">in_layer0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_layer1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grads</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">+=</span> <span class="n">layer</span><span class="o">.</span><span class="n">params</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grads</span> <span class="o">+=</span> <span class="n">layer</span><span class="o">.</span><span class="n">grads</span>

        <span class="c1"># 将单词的分布式表示设置为成员变量</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_vecs</span> <span class="o">=</span> <span class="n">W_in</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">contexts</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">h0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_layer0</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">contexts</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_layer1</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">contexts</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">h</span> <span class="o">=</span> <span class="p">(</span><span class="n">h0</span> <span class="o">+</span> <span class="n">h1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_layer</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>
        <span class="n">da</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">ds</span><span class="p">)</span>
        <span class="n">da</span> <span class="o">*=</span> <span class="mf">0.5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_layer1</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">da</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_layer0</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">da</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>
</pre></div>
</div>
</section>
<section id="id3">
<h3><span class="section-number">3.1.3. </span>存在的问题<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h3>
<p>  CBOW用作语言模型存在以下问题：</p>
<ul class="simple">
<li><p><strong>问题1</strong>. 虽然CBOW可以设置任意窗口，但训练时必须使用固定的窗口大小。超过窗口长度的单词将无法作为上下文。</p></li>
</ul>
<p>  该问题的一个简单解决方法是将CBOW的中间层拼接。但带来的后果是参数数量与上下文长度成比例增长。</p>
<ul class="simple">
<li><p><strong>问题2</strong>. CBOW忽略了上下文中单词的顺序。</p></li>
</ul>
<p>  为了记住上下文，可以使用RNN（Recurrent Neural Network）来实现。无论上下文的长短，RNN都可以记住。</p>
</section>
</section>
<section id="rnn">
<h2><span class="section-number">3.2. </span>RNN<a class="headerlink" href="#rnn" title="Link to this heading">¶</a></h2>
<p>  循环神经网络。</p>
</section>
<section id="lstm">
<h2><span class="section-number">3.3. </span>LSTM<a class="headerlink" href="#lstm" title="Link to this heading">¶</a></h2>
<p>  RNN的<strong>不足</strong>：RNN层不能学习长期依赖。RNN通过向过去传递“有意义的梯度”，能学习时间方向上的依赖。梯度包含了应该学习到的有意义信息，通过将这些信息向过去传递，RNN层学习到长期依赖。但如果梯度在传递过程中变弱，则权重参数将不会更新。随着时间回溯，这个RNN层不能避免<strong>梯度消失或梯度爆炸</strong>问题。</p>
<p>  为了解决这类问题，发展出了一种称为LSTM的网络，也称为长短时记忆网络。该模型通过“门机制”解决了长期信息保存和短期输入缺失的问题。以下是模型图概况。</p>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="../_images/lstm.png"><img alt="LSTM" src="../_images/lstm.png" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-text">长短时记忆网络</span><a class="headerlink" href="#id6" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>门函数的选择依据</strong></p></li>
</ul>
<p>  因为tanh函数的输出为<span class="math notranslate nohighlight">\([-1,+1]\)</span>，可以认为介于这个区间的数值表示“信息”的强弱。而sigmoid函数输出为<span class="math notranslate nohighlight">\([0,+1]\)</span>，可以认为是数据流出的比例。因此，在大多数情况下，门函数一般选sigmoid作为激活函数；而包含实质信息的数据则使用tanh作为激活函数。</p>
<ul class="simple">
<li><p><strong>输出门</strong></p></li>
</ul>
<p>  LSTM中，隐状态<span class="math notranslate nohighlight">\(H_t\)</span>仅对记忆单元<span class="math notranslate nohighlight">\(C_t\)</span>应用了tanh函数实现信息传递。如果考虑信息的重要性，可以添加一个门管理隐状态<span class="math notranslate nohighlight">\(H_t\)</span>的输出，这个门就是输出门，即，</p>
<div class="math notranslate nohighlight" id="equation-output-gate">
<span class="eqno">(3)<a class="headerlink" href="#equation-output-gate" title="Link to this equation">¶</a></span>\[
O_t = \sigma(x_tW_xo + H_{t-1}W_{ho} + b_o)
\]</div>
<p>因此，隐状态就相应的更新为，</p>
<div class="math notranslate nohighlight" id="equation-hidden-state-with-o-gate">
<span class="eqno">(4)<a class="headerlink" href="#equation-hidden-state-with-o-gate" title="Link to this equation">¶</a></span>\[
H_t = \underbrace{\text{tanh}(C_t)\odot O_t}_{隐状态与记忆单元的关系}
\]</div>
<ul class="simple">
<li><p><strong>遗忘门</strong></p></li>
</ul>
<p>  同理，对于上一时序的记忆单元<span class="math notranslate nohighlight">\(C_t\)</span>来说，为了去除不必要的记忆元，可以通过遗忘门来实现，即</p>
<div class="math notranslate nohighlight" id="equation-forget-gate">
<span class="eqno">(5)<a class="headerlink" href="#equation-forget-gate" title="Link to this equation">¶</a></span>\[
F_t = \sigma(x_tW_{xf} + H_{t-1}W_{hf} + b_f)
\]</div>
<p>因此可以得到新记忆单元<span class="math notranslate nohighlight">\(C_t\)</span>来自于<span class="math notranslate nohighlight">\(C_{t-1}\)</span>的一部分，即<span class="math notranslate nohighlight">\(C_{t-1}\odot F_t\)</span>。</p>
<ul class="simple">
<li><p><strong>候选记忆单元</strong></p></li>
</ul>
<p>  遗忘门删除了应该忘记的内容，如果不补充应当记忆的内容，则只会遗忘。为此，就当向<span class="math notranslate nohighlight">\(C_t\)</span>添加需要记忆的新信息，也就是候选记忆信息，即</p>
<div class="math notranslate nohighlight" id="equation-candidate-memo-cell">
<span class="eqno">(6)<a class="headerlink" href="#equation-candidate-memo-cell" title="Link to this equation">¶</a></span>\[
\tilde{C}_t = \tanh(x_tW_{xc}+H_{t-1}W_{hc}+b_c)
\]</div>
<ul class="simple">
<li><p><strong>输入门</strong></p></li>
</ul>
<p>  候选记忆信息加入记忆单元<span class="math notranslate nohighlight">\(C_t\)</span>时需取舍，因此可以根据门控机制添加一个输入门来控制，即</p>
<div class="math notranslate nohighlight" id="equation-input-gate">
<span class="eqno">(7)<a class="headerlink" href="#equation-input-gate" title="Link to this equation">¶</a></span>\[
I_t = \sigma(x_tW_{xi} + H_{t-1}W_{hi} + b_i)
\]</div>
<ul class="simple">
<li><p><strong>新记忆单元</strong></p></li>
</ul>
<p>  通过上述输入、遗忘操作可以得到新记忆单元，即</p>
<div class="math notranslate nohighlight" id="equation-t-memo-cell">
<span class="eqno">(8)<a class="headerlink" href="#equation-t-memo-cell" title="Link to this equation">¶</a></span>\[
C_t=\underbrace{C_{t-1}\odot F_t}_{遗忘部分} +\underbrace{I_t \odot \tilde{C}_t }_{输入部分}
\]</div>
</section>
<section id="id4">
<h2><span class="section-number">3.4. </span>自动文本生成<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h2>
<p>  利用RNN生成文本，其基本原理如下图所示：</p>
<figure class="align-default" id="id7">
<a class="reference internal image-reference" href="../_images/rnn_gen_text.svg"><img alt="rnn_gen_text" src="../_images/rnn_gen_text.svg" style="width: 300px;" />
</a>
<figcaption>
<p><span class="caption-text">循环神经网络生成文本</span><a class="headerlink" href="#id7" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>  文本生成的主要步骤如下：</p>
<ol class="arabic simple">
<li><p>语言模型根据已给出的单词序列，输出下一个候选单词的概率分布。</p></li>
<li><p>根据步骤1所得到的概率分布，采样生成下一个单词<span class="math notranslate nohighlight">\(x_{t+1}\)</span>。</p></li>
<li><p>将单词<span class="math notranslate nohighlight">\(x_{t+1}\)</span>输入语言模型，跳到步骤1，直到出现终止字符或满足终止条件。</p></li>
</ol>
</section>
<section id="seq2seq">
<h2><span class="section-number">3.5. </span>Seq2Seq<a class="headerlink" href="#seq2seq" title="Link to this heading">¶</a></h2>
<p>  Seq2Seq（Sequence to Sequence）模型是一种将一个序列映射到另一个序列的模型结构，广泛用于机器翻译、文本摘要、语音识别等任务。</p>
<figure class="align-default" id="id8">
<a class="reference internal image-reference" href="../_images/seq2seq.svg"><img alt="rnn_gen_text" src="../_images/seq2seq.svg" style="width: 800px;" />
</a>
<figcaption>
<p><span class="caption-text">Seq2Seq将一个序列映射到另一个序列</span><a class="headerlink" href="#id8" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Seq2seq的左边为编码器（Encoder），右边为解码器（Decoder），连接的桥梁为<span class="math notranslate nohighlight">\(\pmb{h}_t\)</span>。其<strong>基本思想</strong>为：编码器读取整个输入序列，压缩为一个“上下文向量”。解码器接收这个向量，然后逐步生成输出序列。如果用 Teacher Forcing，训练时每一步使用真实词作为下一步输入；推理时只能用前一步生成的词。</p>
<p>  其中编码器的主要输入输出为：</p>
<ul class="simple">
<li><p>输入：一个长度为<span class="math notranslate nohighlight">\(t_{in}\)</span>的输入序列（例如一个英文句子）</p></li>
<li><p>结构：通常是 RNN、LSTM 或 GRU</p></li>
<li><p>输出：最后一个隐藏状态（或整个隐藏状态序列），作为输入的“语义表示”</p></li>
</ul>
<p>  解码器的主要输入输出为：</p>
<ul class="simple">
<li><p>输入：编码器的输出 + 起始符</p></li>
<li><p>输出：一个长度为<span class="math notranslate nohighlight">\(t_{out}\)</span>的输出序列（例如一个法文句子）</p></li>
<li><p>解码过程是一步一步生成：每一步输出一个词，并作为下一步的输入</p></li>
</ul>
</section>
<section id="attention">
<h2><span class="section-number">3.6. </span>Attention<a class="headerlink" href="#attention" title="Link to this heading">¶</a></h2>
<p>  传统 Seq2Seq 模型用编码器将整个输入压缩成一个固定向量，这个向量可能会丢失信息，特别是在长序列中。Attention 机制可以让每一步解码时都重新看一遍输入的全部内容，并判断哪些部分重要。</p>
<figure class="align-default" id="id9">
<a class="reference internal image-reference" href="../_images/Attention.svg"><img alt="rnn_gen_text" src="../_images/Attention.svg" style="width: 700px;" />
</a>
<figcaption>
<p><span class="caption-text">带Attention的Seq2Seq模型</span><a class="headerlink" href="#id9" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>  从上图可以看出，相较于Seq2seq原始模型，解码器额外添加了Attention部分，编码器则额外输出了所有LSTM层的隐藏状态<span class="math notranslate nohighlight">\(\pmb{h}_s\)</span>。而最后一个隐藏状态<span class="math notranslate nohighlight">\(\pmb{h}_t\)</span>仍和原模型保持一致，即最后一个LSTM单元的隐藏单元仍作为解码器的连接桥梁。</p>
<ul class="simple">
<li><p><strong>重要元素的选择</strong></p></li>
</ul>
<p>  Attention模块的主要作用是从<span class="math notranslate nohighlight">\(\pmb{h}_s\)</span>中与当前目标<span class="math notranslate nohighlight">\(y_i\)</span>最相关的成员挑选出来。而挑选这个动作无法微分操作，因此选择一种类似加权累积和的作法替代。事实上，加权累积和是可以自动微分的。例如：假设有3个向量组成的<span class="math notranslate nohighlight">\(\pmb{h}_s\)</span>如下，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\pmb{h}_3=\begin{bmatrix}1&amp;2\\ 3&amp;4\\ 5&amp;6 \end{bmatrix}
\end{split}\]</div>
<p>如果我们想选择第1个向量<span class="math notranslate nohighlight">\([1,2]^\top\)</span>，可以设计一个权重向量<span class="math notranslate nohighlight">\(\pmb{a}=[0.9, 0.05,0.05]\)</span>并且将第1个元素的比例设置为较大，那么将两者相乘得到的结果与第1个向量<span class="math notranslate nohighlight">\([1,2]^\top\)</span>差别不会太大，即</p>
<div class="math notranslate nohighlight">
\[\begin{split}
[0.9, 0.05,0.05]\begin{bmatrix}1&amp;2\\ 3&amp;4\\ 5&amp;6 \end{bmatrix}=0.9\begin{bmatrix}1\\2 \end{bmatrix} + 0.05\begin{bmatrix}3\\4 \end{bmatrix} + 0.05\begin{bmatrix}5\\6 \end{bmatrix} =[1.3,2.3]
\end{split}\]</div>
<p>可见，通过上述计算，可以间接实现选择权重大的成员。对于权重向量<span class="math notranslate nohighlight">\(\pmb{a}\)</span>完全可以交于网络学习，达到误差最小。加权累积和的计算图如下所示：</p>
<figure class="align-default" id="id10">
<a class="reference internal image-reference" href="../_images/weight_sum.svg"><img alt="rnn_gen_text" src="../_images/weight_sum.svg" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-text">加权累积和实现隐层<span class="math notranslate nohighlight">\(\pmb{h}_s\)</span>的元素选择</span><a class="headerlink" href="#id10" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>权重向量的生成</strong></p></li>
</ul>
<p>  对于权重向量<span class="math notranslate nohighlight">\(\pmb{a}\)</span>来说，与之相关的部分为解码器的当前时序隐藏状态<span class="math notranslate nohighlight">\(\pmb{h}\)</span>和编码器的所有隐藏状态<span class="math notranslate nohighlight">\(\pmb{h}_s\)</span>，其计算图如下所示：</p>
<figure class="align-default" id="id11">
<a class="reference internal image-reference" href="../_images/attention_weight.svg"><img alt="rnn_gen_text" src="../_images/attention_weight.svg" style="width: 400px;" />
</a>
<figcaption>
<p><span class="caption-text">权重向量<span class="math notranslate nohighlight">\(\pmb{a}\)</span>的计算图</span><a class="headerlink" href="#id11" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">目录</a></h3>
    <ul>
<li><a class="reference internal" href="#">3. 循环神经网络</a><ul>
<li><a class="reference internal" href="#word2vec">3.1. Word2Vec</a><ul>
<li><a class="reference internal" href="#id2">3.1.1. 语言模型</a></li>
<li><a class="reference internal" href="#cbow">3.1.2. CBOW应用于语言模型</a></li>
<li><a class="reference internal" href="#id3">3.1.3. 存在的问题</a></li>
</ul>
</li>
<li><a class="reference internal" href="#rnn">3.2. RNN</a></li>
<li><a class="reference internal" href="#lstm">3.3. LSTM</a></li>
<li><a class="reference internal" href="#id4">3.4. 自动文本生成</a></li>
<li><a class="reference internal" href="#seq2seq">3.5. Seq2Seq</a></li>
<li><a class="reference internal" href="#attention">3.6. Attention</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>上一主题</h4>
    <p class="topless"><a href="neuro_network_cnn.html"
                          title="上一章"><span class="section-number">2. </span>卷积神经网络</a></p>
  </div>
  <div>
    <h4>下一主题</h4>
    <p class="topless"><a href="cmeans.html"
                          title="下一章"><span class="section-number">1. </span>聚类(一)</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>本页</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/ml/neuro_network_rnn.md.txt"
            rel="nofollow">显示源代码</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">快速搜索</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="提交" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             >索引</a></li>
        <li class="right" >
          <a href="cmeans.html" title="1. 聚类(一)"
             >下一页</a> |</li>
        <li class="right" >
          <a href="neuro_network_cnn.html" title="2. 卷积神经网络"
             >上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">3. </span>循环神经网络</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; 版权所有 2022-2024, SSPUIIP.
      由 <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3创建。
    </div>
  </body>
</html>