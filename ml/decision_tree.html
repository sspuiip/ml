<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>1. 决策树 &#8212; Machine Learning Fundation 1.0 文档</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/nature.css?v=279e0f84" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <script src="../_static/documentation_options.js?v=f115507d"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="../_static/mathjax/tex-chtml.js"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="2. 数据降维" href="dimension_reduce.html" />
    <link rel="prev" title="4. 计算学习理论" href="../base/calc_theory.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             accesskey="I">索引</a></li>
        <li class="right" >
          <a href="dimension_reduce.html" title="2. 数据降维"
             accesskey="N">下一页</a> |</li>
        <li class="right" >
          <a href="../base/calc_theory.html" title="4. 计算学习理论"
             accesskey="P">上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">1. </span>决策树</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1><span class="section-number">1. </span>决策树<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h1>
<p>  <strong>决策树</strong>是一种模拟人决策过程的树形结构。例如：如果有以下一段对话，</p>
<blockquote>
<div><p>女儿：多大年纪了？<br>
母亲：26。<br>
女儿：长的帅不帅？<br>
母亲：挺帅的。<br>
女儿：收入高不？<br>
母亲：不算很高，中等情况。<br>
女儿：是公务员不？<br>
母亲：是，在税务局上班呢。<br>
女儿：那好，我去见见。<br></p>
</div></blockquote>
<p>上述对话体现了人类做出见/不见决策的一个过程。根据对话的顺序，该决策过程可以用下图来刻画。</p>
<figure class="align-center" id="id7">
<pre  class="mermaid">
        %%{
    init: {
        'theme':'base',
        'themeVariables': {
            'fontSize': 8px
        }
    }
}%%
flowchart LR
  id1((年龄)) -- &lt;=30--&gt; id2((长相))
  id1 -- &gt;30 --&gt; id3[不见]
  id2 -- 帅 --&gt; id4((收入))
  id2 -- 不帅 --&gt; id5[不见]
  id4 -- 高 --&gt; id6[见]
  id4 -- 中 --&gt; id7((公务员))
  id4 -- 低 --&gt; id8[不见]
  id7 -- 是 --&gt; id9[见]
  id7 -- 不是 --&gt; id10[不见]
 
    </pre><figcaption>
<p><span class="caption-text">Fig 1. 决策过程模拟树。</span><a class="headerlink" href="#id7" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>  <strong>决策树在构建过程中有个基本问题需要解答</strong>。如上图所示，为什么选择年龄做为第1个分裂的结点而不是其它特征？也就是如何确定分裂特征的顺序？这一问题也称为<strong>特征选择</strong>问题。</p>
<p>  依据特征选择策略的不同，决策树算法大至有ID3,C4.5,CART等不同的决策树构建算法。这些算法的基本流程框架如下所示：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>决策树学习算法基本框架</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>输入</strong>: 训练集<span class="math notranslate nohighlight">\(D=\{(\pmb{x}_1,y_1),(\pmb{x}_2,y_2),...,(\pmb{x}_m,y_m)\}\)</span>，属性集<span class="math notranslate nohighlight">\(A=\{a_1,a_2,...,a_d\}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>输出</strong>: 以<em>root</em>为根的一棵决策树</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>过程</strong>: <em>decisionTree(D,A)</em></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>1: 生成结点<em>root</em> <br>2: <strong>if</strong>(<span class="math notranslate nohighlight">\(D\)</span>中样本属于同一类别<span class="math notranslate nohighlight">\(C\)</span>){ <br>3:    return; <br>4: }<br>5: <strong>if</strong>(<span class="math notranslate nohighlight">\(A=\emptyset\)</span> or <span class="math notranslate nohighlight">\(D\)</span>中样本在属性<span class="math notranslate nohighlight">\(A\)</span>取值一致){<br>6:   将root标记为叶结点，类别为<span class="math notranslate nohighlight">\(D\)</span>中样本数最多的类; return;<br>7: }<br>8: 根据属性挑选规则，在属性集<span class="math notranslate nohighlight">\(A\)</span>中挑选最优属性<span class="math notranslate nohighlight">\(a\)</span>;<br>9: <strong>for</strong>(<span class="math notranslate nohighlight">\(a\)</span>的每一个属性值<span class="math notranslate nohighlight">\(a^v\)</span>){<br>10:  为root生成一个分支结点，该结点的样本集<span class="math notranslate nohighlight">\(D_v\)</span>为<span class="math notranslate nohighlight">\(a==a^v\)</span>的所有样本;<br>11:  <strong>if</strong>(<span class="math notranslate nohighlight">\(D_v\)</span>为空){<br>12:    将分支结点(子结点)标记为叶结点;类别为<span class="math notranslate nohighlight">\(D\)</span>中样本最多的类别; return;<br>13:  <strong>else</strong><br>14:    以dicisionTree(<span class="math notranslate nohighlight">\(D_v\)</span>,A-<span class="math notranslate nohighlight">\(\{a\})\)</span>为分支结点继续分裂;<br>15:  }<br>16:}</p></td>
</tr>
</tbody>
</table>
<section id="id3">
<h2><span class="section-number">1.1. </span>ID3<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h2>
<p>  ID3（Iterative Dichotomiser 3）算法是一种经典的决策树学习算法，由Ross Quinlan于1986年提出。该算法的主要目的是通过构建一个决策树模型来对样本数据进行分类。ID3算法的核心思想是基于<strong>信息增益</strong>（Information Gain）来选择最佳的属性作为决策树的节点，以此来实现对数据的划分。</p>
<p>  <strong>定义（信息熵）</strong>. 假设有离散随机变量（连续型同样适用，只是公式的表示形式不一样）<span class="math notranslate nohighlight">\(X\)</span>其取值概率分别为<span class="math notranslate nohighlight">\(\{(x_1,p_1),...,(x_n,p_n)\}\)</span>(s.t.<span class="math notranslate nohighlight">\(\sum_i p_i=1\)</span>)，则该随机变量的信息熵(不确定性)为，</p>
<div class="math notranslate nohighlight" id="equation-entropy-def">
<span class="eqno">(1)<a class="headerlink" href="#equation-entropy-def" title="Link to this equation">¶</a></span>\[
\textrm{H}(X)=-\sum_i^n p_i\log p_i
\]</div>
<p>  <strong>定义（条件熵）</strong>. 假设有离散随机变量<span class="math notranslate nohighlight">\(X,Y\)</span>，则条件熵为(记<span class="math notranslate nohighlight">\(p(y_i)\triangleq P(Y=y_i)\)</span>)，</p>
<div class="math notranslate nohighlight" id="equation-cond-entropy">
<span class="eqno">(2)<a class="headerlink" href="#equation-cond-entropy" title="Link to this equation">¶</a></span>\[\begin{split}
      \begin{split}
        \textrm{H}(X|Y)&amp;=\sum_{y_i\in Y}p(y_i)\textrm{H}(X|y_i)\\
        &amp;=-\sum_{y_i\in Y}p(y_i)\sum_{x_j\in X}P(x_j|y_i)\log P(x_j|y_i)
      \end{split}
\end{split}\]</div>
<p>  <strong>定义（信息增益）</strong>. 随机变量<span class="math notranslate nohighlight">\(X\)</span>的信息熵与条件熵<span class="math notranslate nohighlight">\(X|Y\)</span>的差异即为信息增益，即</p>
<div class="math notranslate nohighlight" id="equation-information-gain">
<span class="eqno">(3)<a class="headerlink" href="#equation-information-gain" title="Link to this equation">¶</a></span>\[
        \textrm{IG}(X,Y)=\textrm{H}(X)-\textrm{H}(X|Y)
\]</div>
<section id="id2">
<h3><span class="section-number">1.1.1. </span>ID3构建算法<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h3>
<p>  ID3构建算法只需要对上一节的决策树学习算法的第8行进行修改，即为ID3学习算法。</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>ID3决策树学习算法基本框架</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>输入</strong>: 训练集<span class="math notranslate nohighlight">\(D=\{(\pmb{x}_1,y_1),(\pmb{x}_2,y_2),...,(\pmb{x}_m,y_m)\}\)</span>，属性集<span class="math notranslate nohighlight">\(A=\{a_1,a_2,...,a_d\}\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>输出</strong>: 以<em>root</em>为根的一棵决策树</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>过程</strong>: <em>decisionTree(D,A)</em></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>1: 生成结点<em>root</em> <br>2: <strong>if</strong>(<span class="math notranslate nohighlight">\(D\)</span>中样本属于同一类别<span class="math notranslate nohighlight">\(C\)</span>){ <br>3:    return; <br>4: }<br>5: <strong>if</strong>(<span class="math notranslate nohighlight">\(A=\emptyset\)</span> or <span class="math notranslate nohighlight">\(D\)</span>中样本在属性<span class="math notranslate nohighlight">\(A\)</span>取值一致){<br>6:   将root标记为叶结点，类别为<span class="math notranslate nohighlight">\(D\)</span>中样本数最多的类; return;<br>7: }<br>8: <strong>计算所有属性的信息增益，在属性集<span class="math notranslate nohighlight">\(A\)</span>中挑选最大增益属性</strong><span class="math notranslate nohighlight">\(a\)</span>;<br>9: <strong>for</strong>(<span class="math notranslate nohighlight">\(a\)</span>的每一个属性值<span class="math notranslate nohighlight">\(a^v\)</span>){<br>10:  为root生成一个分支结点，该结点的样本集<span class="math notranslate nohighlight">\(D_v\)</span>为<span class="math notranslate nohighlight">\(a==a^v\)</span>的所有样本;<br>11:  <strong>if</strong>(<span class="math notranslate nohighlight">\(D_v\)</span>为空){<br>12:    将分支结点(子结点)标记为叶结点;类别为<span class="math notranslate nohighlight">\(D\)</span>中样本最多的类别; return;<br>13:  <strong>else</strong><br>14:    以dicisionTree(<span class="math notranslate nohighlight">\(D_v\)</span>,A-<span class="math notranslate nohighlight">\(\{a\})\)</span>为分支结点继续分裂;<br>15:  }<br>16:}</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id4">
<h3><span class="section-number">1.1.2. </span>ID3应用案例<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h3>
<p>  应用ID3算法构建一棵决策树，该案例使用UCI数据集weather，内容如下表所示：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p><strong>outlook</strong></p></th>
<th class="head text-center"><p><strong>temperature</strong></p></th>
<th class="head text-center"><p><strong>humidity</strong></p></th>
<th class="head text-center"><p><strong>windy</strong></p></th>
<th class="head text-center"><p><strong>play</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>sunny</p></td>
<td class="text-center"><p>hot</p></td>
<td class="text-center"><p>high</p></td>
<td class="text-center"><p>FALSE</p></td>
<td class="text-center"><p>no</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>sunny</p></td>
<td class="text-center"><p>hot</p></td>
<td class="text-center"><p>high</p></td>
<td class="text-center"><p>TRUE</p></td>
<td class="text-center"><p>no</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>overcast</p></td>
<td class="text-center"><p>hot</p></td>
<td class="text-center"><p>high</p></td>
<td class="text-center"><p>FALSE</p></td>
<td class="text-center"><p>yes</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>rainy</p></td>
<td class="text-center"><p>mild</p></td>
<td class="text-center"><p>high</p></td>
<td class="text-center"><p>FALSE</p></td>
<td class="text-center"><p>yes</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>rainy</p></td>
<td class="text-center"><p>cool</p></td>
<td class="text-center"><p>normal</p></td>
<td class="text-center"><p>FALSE</p></td>
<td class="text-center"><p>yes</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>rainy</p></td>
<td class="text-center"><p>cool</p></td>
<td class="text-center"><p>normal</p></td>
<td class="text-center"><p>TRUE</p></td>
<td class="text-center"><p>no</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>overcast</p></td>
<td class="text-center"><p>cool</p></td>
<td class="text-center"><p>normal</p></td>
<td class="text-center"><p>TRUE</p></td>
<td class="text-center"><p>yes</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>sunny</p></td>
<td class="text-center"><p>mild</p></td>
<td class="text-center"><p>high</p></td>
<td class="text-center"><p>FALSE</p></td>
<td class="text-center"><p>no</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>sunny</p></td>
<td class="text-center"><p>cool</p></td>
<td class="text-center"><p>normal</p></td>
<td class="text-center"><p>FALSE</p></td>
<td class="text-center"><p>yes</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>rainy</p></td>
<td class="text-center"><p>mild</p></td>
<td class="text-center"><p>normal</p></td>
<td class="text-center"><p>FALSE</p></td>
<td class="text-center"><p>yes</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>sunny</p></td>
<td class="text-center"><p>mild</p></td>
<td class="text-center"><p>normal</p></td>
<td class="text-center"><p>TRUE</p></td>
<td class="text-center"><p>yes</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>overcast</p></td>
<td class="text-center"><p>mild</p></td>
<td class="text-center"><p>high</p></td>
<td class="text-center"><p>TRUE</p></td>
<td class="text-center"><p>yes</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>overcast</p></td>
<td class="text-center"><p>hot</p></td>
<td class="text-center"><p>normal</p></td>
<td class="text-center"><p>FALSE</p></td>
<td class="text-center"><p>yes</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>rainy</p></td>
<td class="text-center"><p>mild</p></td>
<td class="text-center"><p>high</p></td>
<td class="text-center"><p>TRUE</p></td>
<td class="text-center"><p>no</p></td>
</tr>
</tbody>
</table>
<div class="dropdown admonition">
<p class="admonition-title"><strong>计算过程</strong></p>
<br>
<p><strong>第1步. 根据信息增益选择分裂属性，对数据集D进行分裂</strong>。</p>
<p>  <strong>1.1 计算分类属性<code class="docutils literal notranslate"><span class="pre">play</span></code>的信息熵</strong>。</p>
<div class="math notranslate nohighlight">
\[
\textrm{H(play)}=-\frac{5}{14}\log\frac{5}{14}-\frac{9}{14}\log\frac{9}{14}=0.94
\]</div>
<p>  <strong>1.2 计算分类属性对所有其它属性的条件熵</strong>。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\textrm{H(play|outlook)}&amp;=-\frac{5}{14}\left(\frac25\log\frac25+\frac35\log\frac35\right)-\frac{4}{14}\left(1\log 1+0\log 0\right)-\frac{5}{14}\left(\frac25\log\frac25+\frac35\log\frac35\right)=0.69\\
\textrm{H(play|temperatur)}&amp;=-\frac{4}{14}\left(\frac12\log\frac12+\frac12\log\frac12\right)-\frac{6}{14}\left(\frac46\log\frac46 +\frac26\log\frac26\right)-\frac{4}{14}\left(\frac13\log\frac13+\frac23\log\frac23\right)=0.91\\
\textrm{H(play|humidity)}&amp;=-\frac{7}{14}\left(\frac37\log\frac37+\frac47\log\frac47\right)-\frac{7}{14}\left(\frac17\log\frac17 +\frac67\log\frac67\right)=0.78\\
\textrm{H(play|windy)}&amp;=-\frac{6}{14}\left(\frac36\log\frac36+\frac36\log\frac36\right)-\frac{8}{14}\left(\frac68\log\frac68 +\frac28\log\frac28\right)=0.89\\
\end{split}
\end{split}\]</div>
<p>  <strong>1.3 计算所有属性的信息增益</strong>。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\textrm{IG(play,outlook)}&amp;=\textrm{H(play)}-\textrm{H(play|outlook)}=0.24\\
\textrm{IG(play,temperatur)}&amp;=\textrm{H(play)}-\textrm{H(play|temperatur)}=0.02\\
\textrm{IG(play,humidity)}&amp;=\textrm{H(play)}-\textrm{H(play|humidity)}=0.15\\
\textrm{IG(play,windy)}&amp;=\textrm{H(play)}-\textrm{H(play|windy)}=0.04\\
\end{split}
\end{split}\]</div>
<p>  <strong>1.4 选择信息增益最大的属性<code class="docutils literal notranslate"><span class="pre">outlook</span></code>分裂</strong>。</p>
<p>  当<code class="docutils literal notranslate"><span class="pre">outlook=overcast</span></code>时，该子集的决策属性值一致，都为<code class="docutils literal notranslate"><span class="pre">yes</span></code>，因此不必对该分支继续分裂；相反地，D1·D2还需进一步分裂。分裂结果如下图所示：</p>
<figure class="align-center" id="id8">
<pre  class="mermaid">
        %%{
    init: {
        'theme':'base',
        'themeVariables': {
            'fontSize': 8px
        }
    }
}%%
flowchart LR
  id1((outlook)) -- overcast --&gt; id2[YES]
  id1 -- rainy --&gt; id3[(D1)]
  id1 -- sunny --&gt; id4[(D2)]
 
    </pre><figcaption>
<p><span class="caption-text">Fig 2. 根据属性<em>outlook</em>分裂数据集。</span><a class="headerlink" href="#id8" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p><strong>第2步. 根据信息增益选择分裂属性，对数据集D1进行分裂</strong>。</p>
<p>  <strong>2.1 计算分类属性<code class="docutils literal notranslate"><span class="pre">play</span></code>的信息熵</strong>。第1步已计算，直接使用上一步的结果。</p>
<p>  <strong>2.2 计算分类属性对所有未选择的属性的条件熵</strong>。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\textrm{H(play|temperatur)}&amp;=-\frac{2}{5}\left(\frac12\log\frac12+\frac12\log\frac12\right)-\frac{3}{5}\left(\frac13\log\frac13 +\frac23\log\frac23\right)=0.951\\
\textrm{H(play|humidity)}&amp;=-\frac{2}{5}\left(\frac12\log\frac12+\frac47\log\frac47\right)-\frac{3}{5}\left(\frac13\log\frac13 +\frac23\log\frac23\right)=0.951\\
\textrm{H(play|windy)}&amp;=-\frac{2}{5}\left(1\log 1+0 \log 0\right)-\frac{3}{5}\left(1\log 1+0 \log 0\right)=0.00\\
\end{split}
\end{split}\]</div>
<p>  <strong>2.3 计算所有属性的信息增益</strong>。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\textrm{IG(play,temperatur)}&amp;=\textrm{H(play)}-\textrm{H(play|temperatur)}=0.02\\
\textrm{IG(play,humidity)}&amp;=\textrm{H(play)}-\textrm{H(play|humidity)}=0.02\\
\textrm{IG(play,windy)}&amp;=\textrm{H(play)}-\textrm{H(play|windy)}=0.97\\
\end{split}
\end{split}\]</div>
<p>  <strong>2.4 选择信息增益最大的属性<code class="docutils literal notranslate"><span class="pre">windy</span></code>分裂</strong>。</p>
<p>  当<code class="docutils literal notranslate"><span class="pre">windy=yes</span></code>时，该子集的决策属性值一致，都为<code class="docutils literal notranslate"><span class="pre">no</span></code>；当<code class="docutils literal notranslate"><span class="pre">windy=no</span></code>时，该子集的决策属性值一致，都为<code class="docutils literal notranslate"><span class="pre">yes</span></code>，因此不必对所有分支继续分裂。至此，D1分裂结束。分裂结果如下图所示：</p>
<figure class="align-center" id="id9">
<pre  class="mermaid">
        %%{
    init: {
        'theme':'base',
        'themeVariables': {
            'fontSize': 8px
        }
    }
}%%
flowchart LR
  id1((outlook)) -- overcast --&gt; id2[YES]
  id1 -- rainy --&gt; id3[(windy)]
  id1 -- sunny --&gt; id4[(D2)]
  id3 -- yes --&gt; id5[NO]
  id3 -- no --&gt; id6[YES]
 
    </pre><figcaption>
<p><span class="caption-text">Fig 3. 根据属性<em>windy</em>分裂数据集D1。</span><a class="headerlink" href="#id9" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p><strong>第3步. 根据信息增益选择分裂属性，对数据集D2进行分裂</strong>。</p>
<p>  <strong>3.1 计算分类属性<code class="docutils literal notranslate"><span class="pre">play</span></code>的信息熵</strong>。第1步已计算，直接使用上一步的结果。</p>
<p>  <strong>3.2 计算分类属性对所有未选择的属性的条件熵</strong>。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\textrm{H(play|temperatur)}&amp;=-\frac{2}{5}\left(1\log 1+0 \log 0\right)-\frac{2}{5}\left(\frac12\log\frac12 +\frac12\log\frac12\right)-\frac{1}{5}\left(1\log 1+0 \log 0 \right)=0.4\\
\textrm{H(play|humidity)}&amp;=-\frac{2}{5}\left(1\log 1+0 \log 0\right)-\frac{3}{5}\left(1\log 1+0 \log 0\right)=0.0\\
\textrm{H(play|windy)}&amp;=-\frac{2}{5}\left(\frac12\log\frac12 +\frac12\log\frac12\right)-\frac{3}{5}\left(\frac13\log\frac13 +\frac23\log\frac23\right)=0.95\\
\end{split}
\end{split}\]</div>
<p>  <strong>3.3 计算所有属性的信息增益</strong>。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\textrm{IG(play,temperatur)}&amp;=\textrm{H(play)}-\textrm{H(play|temperatur)}=0.57\\
\textrm{IG(play,humidity)}&amp;=\textrm{H(play)}-\textrm{H(play|humidity)}=0.97\\
\textrm{IG(play,windy)}&amp;=\textrm{H(play)}-\textrm{H(play|windy)}=0.02\\
\end{split}
\end{split}\]</div>
<p>  <strong>3.4 选择信息增益最大的属性<code class="docutils literal notranslate"><span class="pre">humidity</span></code>分裂</strong>。</p>
<p>  当<code class="docutils literal notranslate"><span class="pre">humidity=high</span></code>时，该子集的决策属性值一致，都为<code class="docutils literal notranslate"><span class="pre">no</span></code>；当<code class="docutils literal notranslate"><span class="pre">humidity=normal</span></code>时，该子集的决策属性值一致，都为<code class="docutils literal notranslate"><span class="pre">yes</span></code>，因此不必对所有分支继续分裂。至此，D2分裂结束。分裂结果如下图所示：</p>
<figure class="align-center" id="id10">
<pre  class="mermaid">
        %%{
    init: {
        'theme':'base',
        'themeVariables': {
            'fontSize': 8px
        }
    }
}%%
flowchart LR
  id1((outlook)) -- overcast --&gt; id2[YES]
  id1 -- rainy --&gt; id3[(windy)]
  id1 -- sunny --&gt; id4[(humidity)]
  id3 -- yes --&gt; id5[NO]
  id3 -- no --&gt; id6[YES]
  id4 -- high --&gt; id7[NO]
  id4 -- normal --&gt; id8[YES]
    </pre><figcaption>
<p><span class="caption-text">Fig 4. 根据属性<em>humidity</em>分裂数据集D2。</span><a class="headerlink" href="#id10" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="cart">
<h2><span class="section-number">1.2. </span>CART<a class="headerlink" href="#cart" title="Link to this heading">¶</a></h2>
<p>  分类与回归树(Classification and Regression Tree, CART)是一种决策树，可用于处理分类或回归任务。CART的形式为一棵二分支的决策树，任意分支为某一特征（依据某种规则选择）的测试条件，其左子树取值为“真”，右子树取值为“假”。</p>
<section id="id5">
<h3><span class="section-number">1.2.1. </span>回归树<a class="headerlink" href="#id5" title="Link to this heading">¶</a></h3>
<p>  一棵回归树对应着训练集<span class="math notranslate nohighlight">\(D\)</span>(输入空间)的一个划分<span class="math notranslate nohighlight">\(\{D_1,D_2,...,D_k\}\)</span>，以及任意划分<span class="math notranslate nohighlight">\(D_j\)</span>都对应一个输出值<span class="math notranslate nohighlight">\(c_j\)</span>(<span class="math notranslate nohighlight">\(c_j\in\{c_1,c_2,...,c_k\}\)</span>)。回归树模型可表示为，</p>
<div class="math notranslate nohighlight" id="equation-regression-tree-model">
<span class="eqno">(4)<a class="headerlink" href="#equation-regression-tree-model" title="Link to this equation">¶</a></span>\[
f(\pmb{x})=\sum_{j=1}^k c_j\times \mathbb{I}(\pmb{x}\in D_j)
\]</div>
<ul class="simple">
<li><p><strong>数据集的划分</strong></p></li>
</ul>
<p>  选择第<span class="math notranslate nohighlight">\(i\)</span>个特征<span class="math notranslate nohighlight">\(a_i\)</span>和它的取值<span class="math notranslate nohighlight">\(v\)</span>，作为划分变量和划分点，并定义左右两个子区域如下，</p>
<div class="math notranslate nohighlight">
\[
R_1(a,v)=\{\pmb{x}|\pmb{x}_{a}\le v\},\quad R_2(a,v)=\{\pmb{x}|\pmb{x}_{a}&gt; v\}
\]</div>
<p>其中<span class="math notranslate nohighlight">\(\pmb{x}_a\)</span>为样本<span class="math notranslate nohighlight">\(\pmb{x}\)</span>在特征<span class="math notranslate nohighlight">\(a\)</span>的取值。然后寻求最优化划分变量和划分点，即，</p>
<div class="math notranslate nohighlight" id="equation-region-opt-target">
<span class="eqno">(5)<a class="headerlink" href="#equation-region-opt-target" title="Link to this equation">¶</a></span>\[
a^*,v^*=\mathop{\arg\min}\limits_{a,v}\left[\min_{c_1}\sum_{\pmb{x}_i\in R_1}(y_i-c_1)^2 + \min_{c_2}\sum_{\pmb{x}_i\in R_2}(y_i-c_2)^2 \right]
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(c_j\)</span><strong>的估计</strong></p></li>
</ul>
<p>  若数据集的划分确定后，可用平方误差<span class="math notranslate nohighlight">\(\sum_{x_i\in D_j}(y_i-f(\pmb{x}_i))^2\)</span>来表示回归树的训练误差。<span class="math notranslate nohighlight">\(c_j\)</span>可用平方误差最小来求解每个单元上的最优输出，即</p>
<div class="math notranslate nohighlight" id="equation-c-j-estimator">
<span class="eqno">(6)<a class="headerlink" href="#equation-c-j-estimator" title="Link to this equation">¶</a></span>\[
\hat{c}_j=\frac{1}{|D_j|}\sum_{i\in D_j} y_i
\]</div>
<p>  遍历所有特征，找到最优<span class="math notranslate nohighlight">\(a\)</span>，并确定<span class="math notranslate nohighlight">\((a,v)\)</span>，依此将数据集划分为2个区域。对每个区域重复以上步骤，直到区域内无数据集划分为止，至此一棵回归树就生成好了。由于使用的平方误差来表示训练误差，这种回归树也称为<strong>最小二乘回归树</strong>。</p>
</section>
<section id="id6">
<h3><span class="section-number">1.2.2. </span>分类树<a class="headerlink" href="#id6" title="Link to this heading">¶</a></h3>
<p>  分类树的构建与ID3类似，区别在于特征选择所用的标准为基尼指数。</p>
<p>  <strong>定义（基尼指数）</strong>. 假设数据集<span class="math notranslate nohighlight">\(D\)</span>的分类属性有<span class="math notranslate nohighlight">\(K\)</span>个类别，且样本属于类别<span class="math notranslate nohighlight">\(k\)</span>的概率为<span class="math notranslate nohighlight">\(p_k\)</span>，则基尼指数为，</p>
<div class="math notranslate nohighlight" id="equation-gini-def">
<span class="eqno">(7)<a class="headerlink" href="#equation-gini-def" title="Link to this equation">¶</a></span>\[
\textrm{gini}(D)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
\]</div>
<p>若样本集<span class="math notranslate nohighlight">\(D\)</span>根据特征<span class="math notranslate nohighlight">\(A\)</span>的取值划分为<span class="math notranslate nohighlight">\(D_1,D_2,...,D_m\)</span>个部分，则在特征<span class="math notranslate nohighlight">\(A\)</span>的条件下，集合<span class="math notranslate nohighlight">\(D\)</span>的基尼指数为，</p>
<div class="math notranslate nohighlight" id="equation-set-split-gini">
<span class="eqno">(8)<a class="headerlink" href="#equation-set-split-gini" title="Link to this equation">¶</a></span>\[
\textrm{gini}(D,A)=\sum_{i=1}^m\frac{|D_i|}{|D|}\textrm{gini}(D_i)
\]</div>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">目录</a></h3>
    <ul>
<li><a class="reference internal" href="#">1. 决策树</a><ul>
<li><a class="reference internal" href="#id3">1.1. ID3</a><ul>
<li><a class="reference internal" href="#id2">1.1.1. ID3构建算法</a></li>
<li><a class="reference internal" href="#id4">1.1.2. ID3应用案例</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cart">1.2. CART</a><ul>
<li><a class="reference internal" href="#id5">1.2.1. 回归树</a></li>
<li><a class="reference internal" href="#id6">1.2.2. 分类树</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>上一主题</h4>
    <p class="topless"><a href="../base/calc_theory.html"
                          title="上一章"><span class="section-number">4. </span>计算学习理论</a></p>
  </div>
  <div>
    <h4>下一主题</h4>
    <p class="topless"><a href="dimension_reduce.html"
                          title="下一章"><span class="section-number">2. </span>数据降维</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>本页</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/ml/decision_tree.md.txt"
            rel="nofollow">显示源代码</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">快速搜索</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="提交" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             >索引</a></li>
        <li class="right" >
          <a href="dimension_reduce.html" title="2. 数据降维"
             >下一页</a> |</li>
        <li class="right" >
          <a href="../base/calc_theory.html" title="4. 计算学习理论"
             >上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">1. </span>决策树</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; 版权所有 2022-2024, SSPUIIP.
      由 <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3创建。
    </div>
  </body>
</html>