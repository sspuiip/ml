<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4. 概率图模型 &#8212; Machine Learning Fundation 1.0 文档</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/nature.css?v=279e0f84" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <script src="../_static/documentation_options.js?v=f115507d"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="../_static/mathjax/tex-chtml.js"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="5. 贝叶斯分类" href="bayes.html" />
    <link rel="prev" title="3. EM算法概述" href="EM.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             accesskey="I">索引</a></li>
        <li class="right" >
          <a href="bayes.html" title="5. 贝叶斯分类"
             accesskey="N">下一页</a> |</li>
        <li class="right" >
          <a href="EM.html" title="3. EM算法概述"
             accesskey="P">上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">4. </span>概率图模型</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1><span class="section-number">4. </span>概率图模型<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h1>
<p>  根据已观察到的数据（样本集）对未知变量（样本所属类别）进行估计，这是从数据学习知识的基本途径，也是机器学习的主要任务。概率模型将这一任务转换为计算变量的概率分布，即利用已知变量推测未知变量，也称之为<strong>推断</strong>。具体来说，假设所关心的变量集为<span class="math notranslate nohighlight">\(Y\)</span>，可观测变量集为<span class="math notranslate nohighlight">\(O\)</span>，其它变量为<span class="math notranslate nohighlight">\(R\)</span>，推断就是通过联合分布<span class="math notranslate nohighlight">\(P(Y,R,O)\)</span>或条件分布<span class="math notranslate nohighlight">\(P(Y,R|O)\)</span>计算得到条件分布<span class="math notranslate nohighlight">\(P(Y|O)\)</span>。其中，联合分布<span class="math notranslate nohighlight">\(P(Y,R,O)\)</span>称为<strong>生成式模型</strong>，条件分布<span class="math notranslate nohighlight">\(P(Y,R|O)\)</span>称为<strong>判别式模型</strong>。但是，直接使用概率求和规则消去变量<span class="math notranslate nohighlight">\(R\)</span>是不可行的，因为即使所有变量只有2种取值的特殊情况，计算复杂度也高达<span class="math notranslate nohighlight">\(O(2^{|Y|+|R|})\)</span>。</p>
<p>  概率图模型是一种用图来表示变量间关系的概率模型。该图的结点表示一个（组）随机变量，结点之间的边表示变量间的相关关系。根据边的类型不同，图模型又可以继续细分为<strong>有向无环图</strong>（贝叶斯网，Bayesian network）和<strong>无向无环图</strong>（马尔可夫网，Markov network）两种。</p>
<section id="id2">
<h2><span class="section-number">4.1. </span>贝叶斯网<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h2>
<p>  有向图模型的关键属性是结点有序。按照父结点在子结点之前的顺序排序也称为拓扑序列。给定这个序列，则可以定义<strong>有序马尔可夫属性</strong>，即结点只依赖它的直接父结点，不依赖其所有的其它前序结点。</p>
<div class="math notranslate nohighlight" id="equation-makov-pre">
<span class="eqno">(1)<a class="headerlink" href="#equation-makov-pre" title="Link to this equation">¶</a></span>\[
x_s \bot \pmb{x}_{\mathrm{pre}(s)\backslash \mathrm{pa}(s)} | x_{\mathrm{pa}(s)}
\]</div>
<section id="id3">
<h3><span class="section-number">4.1.1. </span>联合分布<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h3>
<p>  有向图模型是基于假设条件独立的一种用于表示联合分布的方法。根据有序马尔可夫属性，我们可以从有向图模型得到图所表示的联合概率，</p>
<div class="math notranslate nohighlight" id="equation-bayesian-joint-dist">
<span class="eqno">(2)<a class="headerlink" href="#equation-bayesian-joint-dist" title="Link to this equation">¶</a></span>\[
p(\pmb{x}_{1:V}|G)=\prod_{t=1}^V p(x_t|\mathrm{pa}(t))
\]</div>
<div class="dropdown admonition">
<p class="admonition-title"><strong>例</strong>0. 下图给出了一个有向图模型</p>
<figure class="align-center" id="id20">
<pre  class="mermaid">
        flowchart TD
  x1((&quot;1&quot;)) --&gt; x2((&quot;2&quot;))
  x1--&gt;x3((&quot;3&quot;))
  x2--&gt;x4((&quot;4&quot;))
  x3--&gt;x4
  x3--&gt;x5((&quot;5&quot;))
    </pre><figcaption>
<p><span class="caption-text">Fig 0. A Bayesian Net</span><a class="headerlink" href="#id20" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>  根据链规则(chain-rule)可知，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
p(\pmb{x}_{1:5})&amp;=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)p(x_4|x_3,x_2,x_1)p(x_5|x_4,x_3,x_2,x_1)\\
&amp;=p(x_1)p(x_2|x_1)\underbrace{p(x_3|x_1)p(x_4|x_2,x_3)p(x_5|x_3)}_{\mathrm{有序马尔可夫属性}}\\
&amp;=p(\pmb{x}_{1:5}|G)
\end{split}
\end{split}\]</div>
<p>该有向图模型的联合分布可根据公式<a class="reference internal" href="#equation-bayesian-joint-dist">(2)</a>直接可得，</p>
<div class="math notranslate nohighlight">
\[
p(\pmb{x}_{1:5}|G)=\prod_{t=1}^5 p(x_t | \mathrm{pa}(t))
\]</div>
</div>
</section>
<section id="id4">
<h3><span class="section-number">4.1.2. </span>有向图的条件独立性<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h3>
<section id="d">
<h4><span class="section-number">4.1.2.1. </span><span class="math notranslate nohighlight">\(d\)</span>-分离<a class="headerlink" href="#d" title="Link to this heading">¶</a></h4>
<dl class="simple myst">
<dt>无向路径<span class="math notranslate nohighlight">\(P\)</span>的<span class="math notranslate nohighlight">\(d\)</span>-分离</dt><dd><p>无向路径<span class="math notranslate nohighlight">\(P\)</span>被结点集<span class="math notranslate nohighlight">\(E\)</span>所<span class="math notranslate nohighlight">\(d\)</span>-分离，当且仅当以下条件中至少一条成立:</p>
</dd>
</dl>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(P\)</span>包含一个链，<span class="math notranslate nohighlight">\(s\rightarrow m\rightarrow t\)</span>或<span class="math notranslate nohighlight">\(s\leftarrow m\leftarrow t\)</span>，其中<span class="math notranslate nohighlight">\(m\in E\)</span>。</p></li>
<li><p><span class="math notranslate nohighlight">\(P\)</span>包含一个叉，<span class="math notranslate nohighlight">\(s\swarrow m\searrow t\)</span>其中<span class="math notranslate nohighlight">\(m\in E\)</span>。</p></li>
<li><p><span class="math notranslate nohighlight">\(P\)</span>包含一个冲突结构或V-结构，<span class="math notranslate nohighlight">\(s\searrow m\swarrow t\)</span>，其中<span class="math notranslate nohighlight">\(m\notin E\)</span>且<span class="math notranslate nohighlight">\(m\)</span>的任意子孙也不属于<span class="math notranslate nohighlight">\(E\)</span>。</p></li>
</ol>
<dl class="simple myst">
<dt>结点集的<span class="math notranslate nohighlight">\(d\)</span>-分离</dt><dd><p>给定观察集<span class="math notranslate nohighlight">\(E\)</span>，结点集<span class="math notranslate nohighlight">\(A\)</span>与结点集<span class="math notranslate nohighlight">\(B\)</span>是<span class="math notranslate nohighlight">\(d\)</span>-分离的，当且仅当集合<span class="math notranslate nohighlight">\(A\)</span>的任意结点<span class="math notranslate nohighlight">\(a\)</span>出发到达集合<span class="math notranslate nohighlight">\(B\)</span>的任意结点<span class="math notranslate nohighlight">\(b\)</span>的无向路径是被集合<span class="math notranslate nohighlight">\(E\)</span>所<span class="math notranslate nohighlight">\(d\)</span>-分离的。</p>
</dd>
<dt>贝叶斯球算法</dt><dd><p>是一个简单的用于处理给定结点集<span class="math notranslate nohighlight">\(E\)</span>判定结点集<span class="math notranslate nohighlight">\(A\)</span>与结点集<span class="math notranslate nohighlight">\(B\)</span>是否<span class="math notranslate nohighlight">\(d\)</span>-分离问题的方法。基本思想为将<span class="math notranslate nohighlight">\(E\)</span>集合中的结点全部画上阴影表示为观测集，然后在集合<span class="math notranslate nohighlight">\(A\)</span>的每一个结点放置球，让它们按一定的规则弹跳，然后观察是否有球到达集合<span class="math notranslate nohighlight">\(B\)</span>的任意结点。如果有球到达，则不独立，反之则独立。</p>
</dd>
</dl>
<p>  弹跳规则如下：</p>
<figure class="align-default" id="id21">
<a class="reference internal image-reference" href="../_images/ball-bounce-rule.png"><img alt="AutoEncoder" src="../_images/ball-bounce-rule.png" style="width: 300px;" />
</a>
<figcaption>
<p><span class="caption-text">贝叶斯球弹跳规则</span><a class="headerlink" href="#id21" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>  需要注意的是，一个变量取值的确定与否会对变量间的独立性发生影响。例如，<span class="math notranslate nohighlight">\(v\)</span>结构的变量<span class="math notranslate nohighlight">\(Y\)</span>未观测到，则<span class="math notranslate nohighlight">\(X,Z\)</span>独立；若<span class="math notranslate nohighlight">\(Y\)</span>已观测，则<span class="math notranslate nohighlight">\(X,Z\)</span>不独立。可以验证，</p>
<div class="math notranslate nohighlight">
\[
p(X,Z)=\sum_Y p(X,Y,Z)=\sum_Y p(Y|X,Z)p(X)p(Z)=p(X)p(Z)
\]</div>
<p>  由<span class="math notranslate nohighlight">\(d\)</span>-分离得到的有向图条件独立属性也称之为<strong>有向全局Markov属性</strong>。</p>
</section>
<section id="dagmarkov">
<h4><span class="section-number">4.1.2.2. </span>DAG的其它Markov属性<a class="headerlink" href="#dagmarkov" title="Link to this heading">¶</a></h4>
<p>除了全局Markov属性，从<span class="math notranslate nohighlight">\(d\)</span>-分离标准，还可以得到如下结论：</p>
<div class="math notranslate nohighlight" id="equation-dag-local-markov">
<span class="eqno">(3)<a class="headerlink" href="#equation-dag-local-markov" title="Link to this equation">¶</a></span>\[
t\bot\{\mathrm{nd}(t)\backslash\mathrm{pa}(t)\}|\mathrm{pa}(t)
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\mathrm{nd}(t)\)</span>表示除<span class="math notranslate nohighlight">\(t\)</span>的子孙外的所有结点，即<span class="math notranslate nohighlight">\(\mathrm{nd}(t)=\mathcal{V}\backslash\{t\cup \mathrm{desc}(t)\}\)</span>。该属性称为<strong>有向局部Markov属性</strong>。</p>
<p>  此外，前文所提及的<strong>有序Markov属性</strong><a class="reference internal" href="#equation-makov-pre">(1)</a>。</p>
<p>  <strong>注意</strong>：以上三种属性是等价的。</p>
</section>
<section id="id5">
<h4><span class="section-number">4.1.2.3. </span>道德图<a class="headerlink" href="#id5" title="Link to this heading">¶</a></h4>
<dl class="simple myst">
<dt>道德图</dt><dd><p>使用有向分离技术，将有向图转为无向图，即为道德图。该过程也称为道德化。道德化后可以快速找到条件独立性。</p>
</dd>
</dl>
<p>  道德化步骤：</p>
<ol class="arabic simple">
<li><p>找出有向图的所有v型结构，在v型结构的两个父结点上加上一条边；</p></li>
<li><p>将有向边改为无向边</p></li>
</ol>
<dl class="simple myst">
<dt>道德图判定条件独立性</dt><dd><p>假设道德图中有变量<span class="math notranslate nohighlight">\(\{x,y\}\)</span>和变量集合<span class="math notranslate nohighlight">\(\pmb{z}=\{z_i\}\)</span>，若<span class="math notranslate nohighlight">\(x,y\)</span>能在图上被<span class="math notranslate nohighlight">\(\pmb{z}\)</span>分开，即道德图中将变量集合<span class="math notranslate nohighlight">\(\{\pmb{z}\}\)</span>去除后，<span class="math notranslate nohighlight">\(x,y\)</span>分别属于两个连通分支，则称<span class="math notranslate nohighlight">\(x,y\)</span>被<span class="math notranslate nohighlight">\(\pmb{z}\)</span>有向分离，即<span class="math notranslate nohighlight">\(x\bot y|\pmb{z}\)</span>。</p>
</dd>
</dl>
</section>
<section id="markov">
<h4><span class="section-number">4.1.2.4. </span>Markov毯<a class="headerlink" href="#markov" title="Link to this heading">¶</a></h4>
<dl class="simple myst">
<dt>Markov毯</dt><dd><p>是指让结点<span class="math notranslate nohighlight">\(t\)</span>与其它结点条件独立的<strong>结点集</strong>，记为<span class="math notranslate nohighlight">\(\mathrm{mb}(t)\)</span>。</p>
</dd>
</dl>
<p>  该结点集包含结点<span class="math notranslate nohighlight">\(t\)</span>的子结点集<span class="math notranslate nohighlight">\(\mathrm{ch}(t)\)</span>、父结点集<span class="math notranslate nohighlight">\(\mathrm{pa}(t)\)</span>以及协父结点集<span class="math notranslate nohighlight">\(\mathrm{copa}(t)\)</span>，即</p>
<div class="math notranslate nohighlight" id="equation-markov-blanket">
<span class="eqno">(4)<a class="headerlink" href="#equation-markov-blanket" title="Link to this equation">¶</a></span>\[
\mathrm{mb}(t)=\mathrm{ch}(t)\cup \mathrm{pa}(t)\cup\mathrm{copa}(t)
\]</div>
<p>  注意：协父母也会出现在Markov毯中。当我们在推导</p>
<div class="math notranslate nohighlight">
\[
p(x_t|\pmb{x}_{-t})=\frac{p(x_t,\pmb{x}_{-t})}{p(\pmb{x}_{-t})}
\]</div>
<p>时，所有不包含<span class="math notranslate nohighlight">\(x_t\)</span>的项都会消去（同时出现在分子分母），所以条件分布的乘积只会留下含<span class="math notranslate nohighlight">\(x_t\)</span>的分布，因此可以得到下式，</p>
<div class="math notranslate nohighlight" id="equation-full-condition">
<span class="eqno">(5)<a class="headerlink" href="#equation-full-condition" title="Link to this equation">¶</a></span>\[
p(x_t|\pmb{x}_{-t})\propto \underbrace{p\left(x_t|\pmb{x}_{\mathrm{pa}(t)}\right)\cdot\prod_{s\in\mathrm{ch}(t)}p\left(x_s|\pmb{x}_{\mathrm{pa}(t)}\right)}_{t的全条件(full\ conditon)}
\]</div>
<p>该式也称为结点<span class="math notranslate nohighlight">\(t\)</span>的<strong>全条件（full condition）</strong>。</p>
<div class="dropdown admonition">
<p class="admonition-title"><strong>例</strong>. 下图给出了一个有向图模型</p>
<figure class="align-center" id="id22">
<pre  class="mermaid">
        flowchart LR
  x1((&quot;1&quot;)) --&gt; x2((&quot;2&quot;))
  x1--&gt;x3((&quot;3&quot;))
  x2--&gt;x4((&quot;4&quot;))
  x2--&gt;x5((&quot;5&quot;))
  x3--&gt;x5
  x5--&gt;x6((&quot;6&quot;))
  x3--&gt;x6
  x4--&gt;x7((&quot;7&quot;))
  x5--&gt;x7
  x6--&gt;x7
    </pre><figcaption>
<p><span class="caption-text">Fig . 一个贝叶斯网示例</span><a class="headerlink" href="#id22" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>从上图中，根据Markov毯的定义可以得知，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\mathrm{mb}(3)&amp;=\{ 5,6\}\cup \{1 \}\cup \{2\}=\{1,2,5,6\}\\
\mathrm{mb}(5)&amp;=\{ 6,7\}\cup \{2,3 \}\cup \{4\}=\{2,3,4,6,7\}
\end{split}
\end{split}\]</div>
<p>以及结点5的全条件，</p>
<div class="math notranslate nohighlight">
\[
p(x_5|\pmb{x}_{-5})\propto p(x_5|x_2,x_3)p(x_6|x_3,x_5)p(x_7|x_4,x_5,x_6)
\]</div>
</div>
</section>
</section>
<section id="id6">
<h3><span class="section-number">4.1.3. </span>隐马尔可夫模型<a class="headerlink" href="#id6" title="Link to this heading">¶</a></h3>
<p>  隐马尔可夫模型（Hidden Markov Model, HMM）是一种有向图模型，主要用于时序数据建模、自然语言处理、语音识别等领域。</p>
<figure class="align-center" id="id23">
<pre  class="mermaid">
        block-beta
  columns 10
  x1((&quot;x1&quot;)) space x2((&quot;x2&quot;)) space x3((&quot;x3&quot;)) space x4((&quot;...&quot;)) space x5((&quot;xn&quot;)) space space space space space space space space space space space
  y1((&quot;y1&quot;)) space y2((&quot;y2&quot;)) space y3((&quot;y3&quot;)) space y4((&quot;...&quot;)) space y5((&quot;yn&quot;))
  y1 --&gt; y2
  y2 --&gt; y3
  y3 --&gt; y4
  y4 --&gt; y5
  y1 --&gt; x1
  y2 --&gt; x2
  y3 --&gt; x3
  y4 --&gt; x4
  y5 --&gt; x5
    </pre><figcaption>
<p><span class="caption-text">Fig 1. Hidden Markov Model</span><a class="headerlink" href="#id23" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>  如上图所示，隐马尔可夫模型中的变量分为两类，第一类为状态变量<span class="math notranslate nohighlight">\(\{y_1,y_2,...,y_n\}\)</span>，<span class="math notranslate nohighlight">\(y_i\)</span>表示第<span class="math notranslate nohighlight">\(i\)</span>时刻的系统状态，一般状态变量是不可观测的，也称为隐变量；第二类为观测变量<span class="math notranslate nohighlight">\(\{x_1,x_2,...,x_n\}\)</span>，<span class="math notranslate nohighlight">\(x_i\)</span>表示为第<span class="math notranslate nohighlight">\(i\)</span>时刻的观测值。图中的箭头代表的是变量间的依赖关系。具体来说有以下两点：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_t\)</span>只依赖<span class="math notranslate nohighlight">\(y_t\)</span>。<span class="math notranslate nohighlight">\(t\)</span>时刻的观测值只与<span class="math notranslate nohighlight">\(t\)</span>时刻的状态相关，与其它状态无关。</p></li>
<li><p><span class="math notranslate nohighlight">\(y_t\)</span>只依赖<span class="math notranslate nohighlight">\(y_{t-1}\)</span>。 该性质也就是所谓的<strong>马尔可夫性</strong>。</p></li>
</ul>
<p>根据依赖关系，HMM所有变量的<strong>联合分布</strong>为，</p>
<div class="math notranslate nohighlight" id="equation-hmm-joint">
<span class="eqno">(6)<a class="headerlink" href="#equation-hmm-joint" title="Link to this equation">¶</a></span>\[
P(x_1,...,x_n,y_1,...,y_n)=P(y_1)P(x_1|y_1)\prod_{i=2}^n P(x_i|y_i)P(y_{i}|y_{i-1})
\]</div>
<p>  除了上述变量间的依赖关系（也就是模型的结构信息），确定一个隐马尔可夫模型还需要三组参数，即<strong>模型参数<span class="math notranslate nohighlight">\(\lambda=[A,B,\pmb{\pi}]\)</span></strong>：</p>
<table class="docutils align-center" id="id24" style="width: 600px">
<caption><span class="caption-text">隐马尔可夫模型的主要参数</span><a class="headerlink" href="#id24" title="Link to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>参数</p></th>
<th class="head text-left"><p>描述</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>状态转移概率</strong>。<br>记为矩阵<span class="math notranslate nohighlight">\(A=[a_{ij}]_{N\times N}\)</span><br><span class="math notranslate nohighlight">\(a_{ij}=P(y_{t+1}=s_j | y_t=s_i)\)</span></p></td>
<td class="text-left"><p>各个状态间的跳转概率。<br>在任意时刻<span class="math notranslate nohighlight">\(t\)</span>，若状态为<span class="math notranslate nohighlight">\(s_i\)</span>，则下一时刻状态为<span class="math notranslate nohighlight">\(s_j\)</span>的概率。</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>输出观测概率</strong>。<br>记为矩阵<span class="math notranslate nohighlight">\(B=[b_{ij}]_{N\times M}\)</span><br><span class="math notranslate nohighlight">\(b_{ij}=P(x_t=o_j | y_t=s_i)\)</span></p></td>
<td class="text-left"><p>模型当前状态得到观测值的概率。<br>在任意时刻<span class="math notranslate nohighlight">\(t\)</span>，若状态为<span class="math notranslate nohighlight">\(s_i\)</span>，则观测值为<span class="math notranslate nohighlight">\(o_j\)</span>的概率。</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>初使状态概率</strong>。<br>记为<span class="math notranslate nohighlight">\(\pmb{\pi}=(\pi_1,...,\pi_N)\)</span><br><span class="math notranslate nohighlight">\(\pi_i = P(y_1=s_i)\)</span></p></td>
<td class="text-left"><p>模型在初始时刻各状态出现的概率。</p></td>
</tr>
</tbody>
</table>
<p>通过上述三组参数可以确定一个隐马尔可夫模型。现实应用中，隐马尔可夫模型一般主要用来解决以下3种问题：</p>
<table class="docutils align-center" id="id25" style="width: 600px">
<caption><span class="caption-text">隐马尔可夫模型的主要解决的问题</span><a class="headerlink" href="#id25" title="Link to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head text-left"><p>应用问题</p></th>
<th class="head text-left"><p>场景</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>1. 计算观测序列产生概率<span class="math notranslate nohighlight">\(P(\pmb{x}|\lambda)\)</span>，也就是如何评估模型与观测序列之间的匹配程度？</p></td>
<td class="text-left"><p>根据以往观测序列<span class="math notranslate nohighlight">\((x_1,...,x_{n-1}\)</span>推测当前时刻观测值<span class="math notranslate nohighlight">\(x_n\)</span>的可能性，可以转化为求概率<span class="math notranslate nohighlight">\(P(\pmb{x}|\lambda)\)</span>。</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>2. 给定模型<span class="math notranslate nohighlight">\(\lambda\)</span>和观测序列<span class="math notranslate nohighlight">\(\pmb{x}=(x_1,...,x_n)\)</span>，如何找到与此观测序列匹配的隐状态序列<span class="math notranslate nohighlight">\(\pmb{y}=(y_1,...,y_n)\)</span>，也就是根据观测序列如何推断出隐状态？</p></td>
<td class="text-left"><p>语音识别任务中，隐藏状态是文字，目标为根据观测信号推断最有可能的状态序列（文字序列）。</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>3. 给定观测序列<span class="math notranslate nohighlight">\(\pmb{x}=(x_1,...,x_n)\)</span>，如何优化参数<span class="math notranslate nohighlight">\(\lambda\)</span>使得序列出现的概率<span class="math notranslate nohighlight">\(P(\pmb{x}| \lambda)\)</span>最大，也就是如何训练模型？</p></td>
<td class="text-left"><p>根据训练样本得到最优参数。根据条件独立性，隐马尔可夫模型的三个问题都能高效求解。</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="id7">
<h2><span class="section-number">4.2. </span>马尔可夫随机场<a class="headerlink" href="#id7" title="Link to this heading">¶</a></h2>
<p>  马尔可夫随机场是一种<strong>无向图模型</strong>。图中结点表示一个（组）变量，结点之间的边表示变量之间的依赖关系。马尔可夫随机场的联合概率分布函数由一组<strong>势函数</strong>（potential functions），也称之为<strong>因子</strong>(factor)，构成。势函数是定义在变量子集上的非负实函数。</p>
<p>  马尔可夫随机场的变量子集根据结点特性可以加以区别。若一个结点子集中任意两结点之间都有边连接，则称该子集为一个<strong>团</strong>（clique）；若在一个团中加入另外任何一个结点后，不再形成团，则该团称为<strong>极大团</strong>（maximal clique）。</p>
<section id="id8">
<h3><span class="section-number">4.2.1. </span>联合概率<a class="headerlink" href="#id8" title="Link to this heading">¶</a></h3>
<p>  在马尔可夫随机场中，多个变量之间的联合概率分布可能基于团分解为多个势函数（因子）的乘积，每次个势函数只与一个团关联。具体来说，对于<span class="math notranslate nohighlight">\(n\)</span>个变量<span class="math notranslate nohighlight">\(\pmb{x}=\{x_1,...,x_n\}\)</span>，所有团构成的集合为<span class="math notranslate nohighlight">\(\mathcal{C}\)</span>，团<span class="math notranslate nohighlight">\(Q\in\mathcal{C}\)</span>相关的变量集合记为<span class="math notranslate nohighlight">\(\pmb{x}_Q\)</span>，则<strong>联合概率</strong>定义为，</p>
<div class="math notranslate nohighlight" id="equation-markov-joint">
<span class="eqno">(7)<a class="headerlink" href="#equation-markov-joint" title="Link to this equation">¶</a></span>\[
P(\pmb{x})=\frac1Z \prod_{Q\in\mathcal{C}}\psi_Q(\pmb{x}_Q)
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\psi_Q\)</span>为团<span class="math notranslate nohighlight">\(Q\)</span>对应的势函数，<span class="math notranslate nohighlight">\(Z=\sum_{\pmb{x}}\prod_{Q\in\mathcal{C}}\psi_Q(\pmb{x}_Q)\)</span>为常数，也称之为规范化因子。实际应用中精确计算<span class="math notranslate nohighlight">\(Z\)</span>往往很困难，但很多时候并不需要获得<span class="math notranslate nohighlight">\(Z\)</span>的精确值。</p>
<p>  若变量个数过多，则团的数据会很多，就会对联合概率的计算带来负担。可以发现，只要团<span class="math notranslate nohighlight">\(Q\)</span>不是极大团，则它必然被一个极大团<span class="math notranslate nohighlight">\(Q^*\)</span>包含。因此，可以根据极大团来定义联合概率。假设极大团构成的集合为<span class="math notranslate nohighlight">\(\mathcal{C}^*\)</span>，则有，</p>
<div class="math notranslate nohighlight" id="equation-maxclique-joint">
<span class="eqno">(8)<a class="headerlink" href="#equation-maxclique-joint" title="Link to this equation">¶</a></span>\[
P(\pmb{x})=\frac{1}{Z^*} \prod_{Q\in\mathcal{C}^*}\psi_Q(\pmb{x}_Q)
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(Z^*=\sum_{\pmb{x}}\prod_{Q\in\mathcal{C}^*}\psi_Q(\pmb{x}_Q)\)</span>。</p>
<div class="dropdown admonition">
<p class="admonition-title"><strong>例</strong>1. 假设有一随机变量集<span class="math notranslate nohighlight">\(\pmb{x}=\{x_1,x_2,...,x_6\}\)</span>的马尔可夫随机场如下图所示</p>
<figure class="align-center" id="id26">
<pre  class="mermaid">
        flowchart LR
  x1((x1)) --- x2((x2))
  x1((x1)) --- x3((x3))
  x2((x2)) --- x4((x4))
  x2((x2)) --- x6((x6))
  x2((x2)) --- x5((x5))
  x5((x5)) --- x6((x6))
  x3((x3)) --- x5((x5))
    </pre><figcaption>
<p><span class="caption-text">Fig 2. 马尔可夫随机场示例</span><a class="headerlink" href="#id26" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>则联合概率分布为，</p>
<div class="math notranslate nohighlight">
\[
P(\pmb{x})=\frac1Z \psi_{12}(x_1,x_2)\psi_{13}(x_1,x_3)\psi_{24}(x_2,x_4)\psi_{35}(x_3,x_5)\psi_{256}(x_2,x_5,x_6)
\]</div>
</div>
</section>
<section id="id9">
<h3><span class="section-number">4.2.2. </span>条件独立性<a class="headerlink" href="#id9" title="Link to this heading">¶</a></h3>
<p>  借助分离集得到变量的条件独立性。若结点集<span class="math notranslate nohighlight">\(A\)</span>到<span class="math notranslate nohighlight">\(B\)</span>中的结点都必须经过结点集<span class="math notranslate nohighlight">\(C\)</span>中的结点，则称<span class="math notranslate nohighlight">\(C\)</span>为<span class="math notranslate nohighlight">\(A\)</span>,<span class="math notranslate nohighlight">\(B\)</span>的<strong>分离集</strong>（separating set）。</p>
<div class="dropdown admonition">
<p class="admonition-title"><strong>例</strong>2. 图3给出了分离集<span class="math notranslate nohighlight">\(C\)</span>的示例。子集<span class="math notranslate nohighlight">\(B\)</span>和<span class="math notranslate nohighlight">\(A\)</span>的结点相连都要经过子集<span class="math notranslate nohighlight">\(C\)</span>的结点</p>
<figure class="align-center" id="id27">
<pre  class="mermaid">
        graph LR   
  x1 -.- x9((&quot;x9&quot;))
  subgraph A 
    x3 --- x1
    x2((&quot;x2&quot;)) --- x1((&quot;x1&quot;))
    x2 --- x3((&quot;x3&quot;))    
  end
  x2 --- x4
  x3 --- x5
  subgraph C
    x4((&quot;x4&quot;)) --- x5((&quot;x5&quot;))
  end
  x4 --- x6
  x5 --- x7
  subgraph B
    x6((&quot;x6&quot;)) --- x7((&quot;x7&quot;))
  end
  x7 -.- x8((&quot;x8&quot;))
    </pre><figcaption>
<p><span class="caption-text">Fig 3. 马尔可夫随机场分离集示例</span><a class="headerlink" href="#id27" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</div>
<ul class="simple">
<li><p><strong>全局马尔可夫性: 给定两个变量子集的分离集，则这两个变量子集条件独立</strong></p></li>
</ul>
<p>  在例2中，若令<span class="math notranslate nohighlight">\(A,B,C\)</span>对应的变量子集分别为<span class="math notranslate nohighlight">\(\pmb{x}_A,\pmb{x}_B,\pmb{x}_c\)</span>，则有<span class="math notranslate nohighlight">\(\pmb{x}_A\)</span>与<span class="math notranslate nohighlight">\(\pmb{x}_C\)</span>在给定条件<span class="math notranslate nohighlight">\(\pmb{x}_B\)</span>的条件下相互独立，即</p>
<div class="math notranslate nohighlight">
\[
\pmb{x}_A \bot\pmb{x}_C |\pmb{x}_B
\]</div>
<p>  由全局马尔可夫性可以得到两个有用的结论：</p>
<ol class="arabic simple">
<li><p><strong>局部马尔可夫性</strong>. 给定某变量的邻接变量，则该变量条件独立于其它变量，<span class="math notranslate nohighlight">\(\pmb{x}_v \bot \pmb{x}_{V\backslash \{ n(v)\cup v\}}|\pmb{x}_{n(v)}\)</span>。</p></li>
<li><p><strong>成对马尔可夫性</strong>. 给定其它所有变量，两个非邻接变量条件独立，<span class="math notranslate nohighlight">\(\pmb{x}_u \bot\pmb{x}_v|\pmb{x}_{V\backslash\{u\cup v\}}\)</span>。</p></li>
</ol>
</section>
</section>
<section id="id10">
<h2><span class="section-number">4.3. </span>推断与参数学习<a class="headerlink" href="#id10" title="Link to this heading">¶</a></h2>
<p>  基于概率图模型定义的联合概率分布，我们可以对感兴趣目标变量的<strong>边缘分布进行推断</strong>；或者给定某些观测变量为条件的<strong>条件分布进行推断</strong>。</p>
<p>  对于概率图模型，还需要确定具体分布的参数，这也称之为参数学习（估计）问题。一般使用极大似然估计或最大后验估计方法求解。特别地，如果将待学习的参数视为待推测变量，则参数估计和推断问题就非常相似，可以认为是推断问题。</p>
<p>  概率图模型的推断方法大致可以分为两类：</p>
<ol class="arabic simple">
<li><p>精确推断方法. 精确计算目标变量的边缘分布或条件。一般情况下，此类方法随着极大团规模的增长计算复杂度呈指数增长。</p></li>
<li><p>近似推断方法. 此类方法期望在较低的时间复杂度获得问题的近似解，实际任务中应用较为广泛。</p></li>
</ol>
<p>  具体来说，假设图模型所对应的变量集<span class="math notranslate nohighlight">\(\pmb{x}=\{x_1,...,x_n\}\)</span>可以拆分为<span class="math notranslate nohighlight">\(\pmb{x}_E\)</span>和<span class="math notranslate nohighlight">\(\pmb{x}_Q\)</span>两部分，推断问题的目标就是计算边缘分布<span class="math notranslate nohighlight">\(p(\pmb{x}_Q)\)</span>或条件分布<span class="math notranslate nohighlight">\(p(\pmb{x}_Q|\pmb{x}_E)\)</span>。由条件概率可知，</p>
<div class="math notranslate nohighlight" id="equation-infer-poster">
<span class="eqno">(9)<a class="headerlink" href="#equation-infer-poster" title="Link to this equation">¶</a></span>\[
p(\pmb{x}_Q|\pmb{x}_E)=\frac{p(\pmb{x}_Q,\pmb{x}_E)}{p(\pmb{x}_E)}=\frac{p(\pmb{x}_Q,\pmb{x}_E)}{\sum_{\pmb{x}_F}p(\pmb{x}_Q,\pmb{x}_E)}
\]</div>
<p>上式中，联合分布可以由图模型得到，所以推断问题的关键就是如何计算分母中的边缘分布，也就是</p>
<div class="math notranslate nohighlight" id="equation-edge-distribution">
<span class="eqno">(10)<a class="headerlink" href="#equation-edge-distribution" title="Link to this equation">¶</a></span>\[
p(\pmb{x}_E)=\sum_{\pmb{x}_Q}p(\pmb{x}_Q,\pmb{x}_E)
\]</div>
<blockquote>
<div><p><strong>推断问题</strong>假设模型参数<span class="math notranslate nohighlight">\(\pmb{\theta}\)</span>已知的前提下，计算变量后验<span class="math notranslate nohighlight">\(p(\pmb{x}_Q|\pmb{x}_E)\)</span><a class="reference internal" href="#equation-infer-poster">(9)</a>，而<strong>学习问题</strong>则是指计算模型参数<span class="math notranslate nohighlight">\(\pmb{\theta}\)</span>的最大后验估计。</p>
<p class="attribution">—概率图模型推断与学习</p>
</div></blockquote>
<p>  模型参数的最大后验估计即为，</p>
<div class="math notranslate nohighlight" id="equation-learning-parameter">
<span class="eqno">(11)<a class="headerlink" href="#equation-learning-parameter" title="Link to this equation">¶</a></span>\[
\hat{\pmb{\theta}}=\arg\max\limits_{\pmb{\theta}}\sum_{i=1}^N\log p(\pmb{x}_{i,v}|\pmb{\theta})+\log p(\pmb{\theta})
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{x}_{i,v}\)</span>为数据样本<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>的观测部分。注意若选择<span class="math notranslate nohighlight">\(p(\pmb{\theta})=1\)</span>，则最大后验估计变成最大似然估计。</p>
<section id="id11">
<h3><span class="section-number">4.3.1. </span>精确推断<a class="headerlink" href="#id11" title="Link to this heading">¶</a></h3>
<section id="id12">
<h4><span class="section-number">4.3.1.1. </span>变量消除法<a class="headerlink" href="#id12" title="Link to this heading">¶</a></h4>
<p>  精确推断利用图模型所描述的条件独立性来减少计算目标概率所需的计算量。变量消除法是最直观的精确推断算法也是其它精确推断算法的基础。下面通过一个例子来展示该方法。</p>
<div class="dropdown admonition">
<p class="admonition-title"><strong>例</strong>3. 下图给出了一个有向图模型</p>
<figure class="align-center" id="id28">
<pre  class="mermaid">
        flowchart LR
  x1((&quot;x1&quot;)) --&gt; x2((&quot;x2&quot;))
  x1 -. m12 .-&gt; x2
  x2 --&gt; x3((&quot;x3&quot;))
  x2 -.m23.-&gt; x3
  x3 --&gt; x4((&quot;x4&quot;))  
  x3 --&gt; x5((&quot;x5&quot;))
  x3 -.m35.-&gt; x5
  x4 -.m43.-&gt; x3
 
    </pre><figcaption>
<p><span class="caption-text">Fig 4. 贝叶斯网络结构示例</span><a class="headerlink" href="#id28" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>假设要推断目标是计算边缘分布<span class="math notranslate nohighlight">\(P(x_5)\)</span>，很显然只需要消去变量<span class="math notranslate nohighlight">\(\{x_1,...,x_4\}\)</span>，即</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
P(x_5)&amp;=\sum_{x_4}\sum_{x_3}\sum_{x_2}\sum_{x_1}P(x_1,x_2,x_3,x_4,x_5)\\
&amp;=\sum_{x_4}\sum_{x_3}\sum_{x_2}\sum_{x_1}P(x_1)P(x_2|x_1)P(x_3|x_2)P(x_4|x_3)P(x_5|x_3)
\end{split}
\end{split}\]</div>
<p>如果采用<span class="math notranslate nohighlight">\(\{x_1,x_2,x_4,x_3\}\)</span>的顺序计算，则有，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
P(x_5)&amp;=\sum_{x_4}\sum_{x_3}\sum_{x_2}\sum_{x_1}P(x_1)P(x_2|x_1)P(x_3|x_2)P(x_4|x_3)P(x_5|x_3)\\
&amp;=\sum_{x_3}P(x_5|x_3)\sum_{x_4}P(x_4|x_3)\sum_{x_2}P(x_3|x_2)\sum_{x_1}P(x_2|x_1)P(x_1)
\end{split}
\end{split}\]</div>
<p>引入记号<span class="math notranslate nohighlight">\(m_{ij}(x_j)\)</span>表示求各过程的中间结果，下标<span class="math notranslate nohighlight">\(i\)</span>表示对<span class="math notranslate nohighlight">\(x_i\)</span>求和，<span class="math notranslate nohighlight">\(j\)</span>表示该项剩余的其它变量。显然<span class="math notranslate nohighlight">\(m_{ij}(x_j)\)</span>是<span class="math notranslate nohighlight">\(x_j\)</span>的函数。对上式不断执行此过程，可得，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
P(x_5)&amp;=\sum_{x_3}P(x_5|x_3)\sum_{x_4}P(x_4|x_3)\sum_{x_2}P(x_3|x_2)\underbrace{\sum_{x_1}P(x_2|x_1)P(x_1)}_{m_{12}(x_2)}\\
&amp;=\sum_{x_3}P(x_5|x_3)\sum_{x_4}P(x_4|x_3)\sum_{x_2}P(x_3|x_2)m_{12}(x_2)\\
&amp;=\sum_{x_3}P(x_5|x_3)\sum_{x_4}P(x_4|x_3)m_{23}(x_3)\\
&amp;=\sum_{x_3}P(x_5|x_3)m_{23}(x_3)m_{43}(x_3)\\
&amp;=m_{35}(x_5)
\end{split}
\end{split}\]</div>
<p>显然，最后的计算结果是关于<span class="math notranslate nohighlight">\(x_5\)</span>的函数。事实上，该方法对无向图模型仍然适用。</p>
</div>
<p>  变量消去法通利用乘法对加法的分配律，把多个变量的积的求各问题转化为部分变量交替进行求积和求各的问题。这种转化使得每次的求和与求积运算限制在局部，仅与部分变量有关，从而提高了计算效率。</p>
<dl class="myst field-list simple">
<dt class="field-odd">缺陷<span class="colon">:</span></dt>
<dd class="field-odd"><p>如果需要计算多个边缘分布，重复使用变量消除法会造成大量的冗余计算。</p>
</dd>
</dl>
</section>
<section id="id13">
<h4><span class="section-number">4.3.1.2. </span>信念传播<a class="headerlink" href="#id13" title="Link to this heading">¶</a></h4>
<p>  信念传播将变量消除法中的求和操作当作一个消息传递过程，可以解决求解多个边缘分布重复计算的问题。变量消除法的求和操作可以视为以下过程，</p>
<div class="math notranslate nohighlight" id="equation-sum-operator">
<span class="eqno">(12)<a class="headerlink" href="#equation-sum-operator" title="Link to this equation">¶</a></span>\[
m_{ij}(x_j)=\sum_{x_i}\psi(x_i,x_j)\prod_{k\in\{ n(i)\backslash j\}}m_{ki}(x_i)
\]</div>
<p>该操作在信念传播算法中被当作为结点<span class="math notranslate nohighlight">\(x_i\)</span>向结点<span class="math notranslate nohighlight">\(x_j\)</span>传递了一个消息<span class="math notranslate nohighlight">\(m_{ij}(x_j)\)</span>。图4的虚线描述了<span class="math notranslate nohighlight">\(P(x_5)\)</span>计算的消息传递过程。可以看出，消息传递操作仅与变量<span class="math notranslate nohighlight">\(x_i\)</span>以及邻接结点直接相关，也就是计算限制在图的局部进行。</p>
<p>  一个结点仅在接收到来自其它所有邻接结点的消息后才能向另一接点发送消息，且结点的边缘分布正比于它所接收到的消息的乘积，即</p>
<div class="math notranslate nohighlight" id="equation-edge-dist-sum-prod">
<span class="eqno">(13)<a class="headerlink" href="#equation-edge-dist-sum-prod" title="Link to this equation">¶</a></span>\[
P(x_i)\propto \prod_{k\in n(i)}m_{ki}(x_i)
\]</div>
<p>例如，图4的结点<span class="math notranslate nohighlight">\(x_3\)</span>要向<span class="math notranslate nohighlight">\(x_5\)</span>发送消息，必须先接收到来自结点<span class="math notranslate nohighlight">\(x_2\)</span>和<span class="math notranslate nohighlight">\(x_4\)</span>的消息，且传递给<span class="math notranslate nohighlight">\(x_5\)</span>的消息<span class="math notranslate nohighlight">\(m_{35}(x_5)\)</span>正好为概率<span class="math notranslate nohighlight">\(P(x_5)\)</span>。</p>
<p>  如果图中没有环，则信念传播算法经过<strong>两个步骤</strong>即可完成所有的消息传递，从而能计算所有变量的边缘分布：</p>
<ol class="arabic simple">
<li><p>指定一个根结点，从所有叶结点开始向根结点传递消息，直到根结点收到所有邻接结点的消息；</p></li>
<li><p>从根结点开始向叶结点传递消息，直到所有叶结点收到消息</p></li>
</ol>
<figure class="align-center" id="id29">
<pre  class="mermaid">
        flowchart LR
  x1((&quot;x1&quot;)) -.m12.-&gt; x2((&quot;x2&quot;))
  x2 -- m21 --&gt; x1
  x2 -.m23.-&gt; x3((&quot;x3&quot;))
  x3 --m32--&gt; x2
  x3 -.m34.-&gt; x4((&quot;x4&quot;))  
  x3 -.m35.-&gt; x5((&quot;x5&quot;))
  x5 --m53--&gt; x3
  x4 --m43--&gt; x3
 
    </pre><figcaption>
<p><span class="caption-text">Fig 5. 信念传播算法示例，选<span class="math notranslate nohighlight">\(x_1\)</span>为根结点：实线为第1步消息传递；虚线为第2步。</span><a class="headerlink" href="#id29" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>经过上图的两步消息传递后，图的每个结点都有着不同方向的两条消息，基于这些消息和等式<a class="reference internal" href="#equation-edge-dist-sum-prod">(13)</a>即可计算得到所有变量的边缘分布。</p>
</section>
</section>
<section id="id14">
<h3><span class="section-number">4.3.2. </span>近似推断<a class="headerlink" href="#id14" title="Link to this heading">¶</a></h3>
<p>  近似推断主要采用采样（sampling）或变分推断（variational inference）来实现。精确推断方法通常需要很大的计算开销，实际应用中近似推断更为常见。</p>
<section id="mcmc">
<h4><span class="section-number">4.3.2.1. </span>MCMC采样<a class="headerlink" href="#mcmc" title="Link to this heading">¶</a></h4>
<p>  通过概率图模型可以得到一些感兴趣变量的概率分布，在很多实际应用中，对分布本身并没有多大兴趣，而是希望通过这些概率分布计算某些期望，并根据这些期望做出决策。例如图4的贝叶斯网，进行推断的目标可能是为了计算变量<span class="math notranslate nohighlight">\(x_5\)</span>的期望。如果直接计算或近似这个期望比推断概率分布更容易，则直接计算将使推断问题求解更高效。对于概率图模型来说，问题的关键就在于如何高效的基于图模型描述的概率分布来获取样本。</p>
<p>  概率图模型最常用的采样技术是马尔可夫链蒙特卡罗方法（Markov Chain Monte Carlo, MCMC）。假设有随机变量<span class="math notranslate nohighlight">\(x\sim p(x)\)</span>，函数<span class="math notranslate nohighlight">\(f:X\rightarrow \mathbb{R}\)</span>，则<span class="math notranslate nohighlight">\(f(x)\)</span>的期望为，</p>
<div class="math notranslate nohighlight" id="equation-efx">
<span class="eqno">(14)<a class="headerlink" href="#equation-efx" title="Link to this equation">¶</a></span>\[
p(f)=\mathbb{E}_p[f(x)]=\int_x f(x)p(x)dx
\]</div>
<p>若<span class="math notranslate nohighlight">\(p(x)\)</span>易采样且<span class="math notranslate nohighlight">\(f(x)\)</span>不易积分过于复杂，则可以通过Monte Carlor积分方法近似的计算期望如下，</p>
<div class="math notranslate nohighlight" id="equation-mc-integrate">
<span class="eqno">(15)<a class="headerlink" href="#equation-mc-integrate" title="Link to this equation">¶</a></span>\[
\hat{p}(f)\approx \frac1N\sum_{i=1}^N f(x_i), \quad x_i \sim p(x)
\]</div>
<p>  但实际应用中，图模型构造的密度函数<span class="math notranslate nohighlight">\(p(\pmb{x})\)</span>过于复杂，从而导致采样独立同分布的样本非常困难。幸运的是，随机过程理论有一个<strong>结论</strong>：</p>
<blockquote>
<div><p>马尔可夫链依条件（细致平稳条件）收敛于平稳分布<span class="math notranslate nohighlight">\(q(\pmb{x})\)</span>，收敛后得到的跳转序列<span class="math notranslate nohighlight">\(\{\pmb{x}_n,\pmb{x}_{n+1},...\}\)</span>恰好是<span class="math notranslate nohighlight">\(q(\pmb{x})\)</span>的样本序列。</p>
<p class="attribution">—非周期马尔可夫链</p>
</div></blockquote>
<p>那么一个绝秒的想法是：构造一个马尔可夫链，使得它的平稳分布恰好为<span class="math notranslate nohighlight">\(p(\pmb{x})\)</span>。而这就是MCMC方法的<strong>基本思想</strong>。</p>
<p>  MCMC方法如何构造马尔可夫链将会产生不同的MCMC算法。一般来说，使用MCMC方法构造一条马尔可夫链产生符合图模型的概率分布的样本集，再利用这些样本对所需要的函数进行估计。常用的MCMC算法有，<strong>Metropolis-Hastings算法</strong>、<strong>Gibbs算法</strong>等<a class="footnote-reference brackets" href="#id19" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>。</p>
</section>
<section id="id16">
<h4><span class="section-number">4.3.2.2. </span>变分推断<a class="headerlink" href="#id16" title="Link to this heading">¶</a></h4>
<p>  变分推断使用已知简单分布来逼近需要推断的复杂分布，通过限制近似分布得到局部最优确定解。</p>
<section id="id17">
<h5><span class="section-number">4.3.2.2.1. </span>盘式记法<a class="headerlink" href="#id17" title="Link to this heading">¶</a></h5>
<figure class="align-center" id="id30">
<pre  class="mermaid">
        %%{init: { &quot;theme&quot;: &quot;default&quot; } }%%
graph TD   
  z((&quot;z&quot;)) --&gt; x1((&quot;x1&quot;))
  z --&gt; x2((&quot;x2&quot;))
  z --&gt; x3((&quot;...&quot;))
  z --&gt; x4((&quot;xn&quot;))
  subgraph 观测变量
  x1
  x2
  x3
  x4
  end

    </pre><figcaption>
<p><span class="caption-text">Fig 4. 普通变量关系图</span><a class="headerlink" href="#id30" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>  概率图模型的盘式记法将相互独立、由相同机制产生的多个变量放在一个方框内，并在方框内标注重复出现的次数<span class="math notranslate nohighlight">\(N\)</span>。在很多学习任务中，对属性变量使用盘式记法将使得图表示非常简洁。例如：</p>
<figure class="align-center" id="id31">
<pre  class="mermaid">
        graph LR   
  z((&quot;z&quot;)) --&gt; x1((&quot;x&quot;))
  
  subgraph  N
  x1
  
  end

    </pre><figcaption>
<p><span class="caption-text">Fig 5. 盘式记法</span><a class="headerlink" href="#id31" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>上图观测变量的联合分布概率密度函数是,</p>
<div class="math notranslate nohighlight">
\[
p(\pmb{x}|\Theta)=\prod_{i=1}^N\sum_{\pmb{z}}p(x_i,\pmb{z}|\Theta),\quad \ln p(\pmb{x}|\Theta)=\sum_{i=1}^N\ln\left\{\sum_{\pmb{z}}p(x_i,\pmb{z}|\Theta)\right\}
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{x}=\{x_1,...,x_n\}\)</span>，<span class="math notranslate nohighlight">\(\Theta\)</span>是<span class="math notranslate nohighlight">\(\pmb{x},\pmb{z}\)</span>服从的分布的参数。</p>
<p>  一般来说，推断和学习的任务主要是由观察到的变量<span class="math notranslate nohighlight">\(\pmb{x}\)</span>来估计隐变量<span class="math notranslate nohighlight">\(\pmb{z}\)</span>和分布参数<span class="math notranslate nohighlight">\(\Theta\)</span>，即求解<span class="math notranslate nohighlight">\(p(\pmb{z}|\pmb{x},\Theta)\)</span>和<span class="math notranslate nohighlight">\(\Theta\)</span>。概率模型的参数估计通常通过最大化对数似然函数求解，对于具有隐变量的概率模型可以使用EM算法求解。</p>
<ul class="simple">
<li><p>在E步，根据<span class="math notranslate nohighlight">\(t\)</span>时刻的参数<span class="math notranslate nohighlight">\(\Theta^t\)</span>对<span class="math notranslate nohighlight">\(p(\pmb{x}|\Theta^t)\)</span>进行推断，并计算联合似然函数<span class="math notranslate nohighlight">\(p(\pmb{x},\pmb{z}|\Theta)\)</span>;</p></li>
<li><p>在M步，最优化<span class="math notranslate nohighlight">\(Q\)</span>函数的参数<span class="math notranslate nohighlight">\(\Theta\)</span>。</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-q-function">
<span class="eqno">(16)<a class="headerlink" href="#equation-q-function" title="Link to this equation">¶</a></span>\[\begin{split}
\begin{split}
Q^{t+1}&amp;=\arg\max\limits_{\Theta}Q(\Theta;\Theta^t)\\
&amp;=\arg\max\limits_{\Theta}\sum_{\pmb{z}}p(\pmb{z}|\pmb{x},\Theta^t)\ln p(\pmb{x},\pmb{z}|\Theta)\\
&amp;=\arg\max\limits_{\Theta} \mathbb{E}_\pmb{z}[\ln p(\pmb{x},\pmb{z}|\Theta)]
\end{split}
\end{split}\]</div>
<p>  <span class="math notranslate nohighlight">\(Q\)</span>函数实际上是完全数据对数似然<span class="math notranslate nohighlight">\(\ln p(\pmb{x},\pmb{z}|\Theta)\)</span>在分布<span class="math notranslate nohighlight">\(p(\pmb{z}|\pmb{x},\Theta^t)\)</span>下的期望。当分布与变量<span class="math notranslate nohighlight">\(\pmb{z}\)</span>的后验分布相等时，<span class="math notranslate nohighlight">\(Q\)</span>函数近似等于完全数据对数似然。最终，EM算法可以获得稳定优化的参数<span class="math notranslate nohighlight">\(\Theta\)</span>，隐变量<span class="math notranslate nohighlight">\(\pmb{z}\)</span>的分布也可能通过该参数得到。</p>
<p>  事实上<span class="math notranslate nohighlight">\(p(\pmb{z}|\pmb{x},\Theta^t)\)</span>是隐变量<span class="math notranslate nohighlight">\(\pmb{z}\)</span>的一个近似分布，如果将这个近似分布用<span class="math notranslate nohighlight">\(q(\pmb{z})\)</span>表示，则可以得出以下结论，</p>
<div class="math notranslate nohighlight" id="equation-low-bound">
<span class="eqno">(17)<a class="headerlink" href="#equation-low-bound" title="Link to this equation">¶</a></span>\[
\ln p(\pmb{x})=\mathcal{L}(q)-\mathrm{KL}(q||p)
\]</div>
<p>其中，</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(q)=\int q(\pmb{z})\ln \frac{p(\pmb{x},\pmb{z})}{q(\pmb{z})}d\pmb{z},\quad \mathrm{KL}(q||p)=\int q(\pmb{z})\ln\frac{p(\pmb{z}|\pmb{x})}{q(\pmb{z})}
\]</div>
<p>注意:</p>
<div class="math notranslate nohighlight">
\[
\int q(\pmb{z})\ln p(\pmb{x})=\ln p(\pmb{x})
\]</div>
<p>只有KL散度为0时，<span class="math notranslate nohighlight">\(\mathcal{L}(q)\)</span>才接近对数似然。因为KL散度非负，因此<span class="math notranslate nohighlight">\(\mathcal{L}(q)\)</span>也称为对数似然的一个下界。</p>
</section>
<section id="id18">
<h5><span class="section-number">4.3.2.2.2. </span>均值场方法<a class="headerlink" href="#id18" title="Link to this heading">¶</a></h5>
<p>  现实中，E步对<span class="math notranslate nohighlight">\(p(\pmb{x}|\Theta^t)\)</span>的推断很可能难以实现，此时可借助变分推断。通常假设<span class="math notranslate nohighlight">\(\pmb{z}\)</span>服从分布，</p>
<div class="math notranslate nohighlight" id="equation-q-factor">
<span class="eqno">(18)<a class="headerlink" href="#equation-q-factor" title="Link to this equation">¶</a></span>\[
q(\pmb{z})=\prod_{i=1}^M q_i(\pmb{z}_i)
\]</div>
<p>可以将多变量<span class="math notranslate nohighlight">\(\pmb{z}\)</span>拆解为一系列相互独立的多变量<span class="math notranslate nohighlight">\(\pmb{z}_i\)</span>，简写<span class="math notranslate nohighlight">\(q_i(\pmb{z}_i)\)</span>为<span class="math notranslate nohighlight">\(q_i\)</span>代入可得,</p>
<div class="math notranslate nohighlight" id="equation-mean-field">
<span class="eqno">(19)<a class="headerlink" href="#equation-mean-field" title="Link to this equation">¶</a></span>\[\begin{split}
\begin{split}
\mathcal{L}(q)&amp;=\int_{\pmb{z}}\prod_i q_i\left\{\ln p(\pmb{x},\pmb{z})-\sum_i\ln q_i \right\}d\pmb{z}\\
&amp;=\int_{\pmb{z}_{-j}}\int_{\pmb{z}_j} q_j\prod_{i\neq j}q_i\left\{\ln p(\pmb{x},\pmb{z})-\sum_i\ln q_i \right\}d\pmb{z}\\
&amp;=\int_{\pmb{z}_j} q_j\int_{\pmb{z}_{-j}}\prod_{i\neq j}q_i\left\{\ln p(\pmb{x},\pmb{z})-\sum_i\ln q_i \right\}d\pmb{z}\\
&amp;=\int_{\pmb{z}_j} q_j \underbrace{\int_{\pmb{z}_{-j}}\prod_{i\neq j}q_i\left\{\ln p(\pmb{x},\pmb{z})\right\}}_{\ln\tilde{p}(\pmb{x},\pmb{z})\triangleq\mathbb{E}_{i\neq j}[\ln p(\pmb{x},\pmb{z})]}d\pmb{z} - \int_{\pmb{z}_j} q_j\int_{\pmb{z}_{-j}}\prod_{i\neq j}q_i\left\{\sum_i\ln q_i\right\}d\pmb{z}\\
&amp;=\int q_j \mathbb{E}_{i\neq j}[\ln p(\pmb{x},\pmb{z})]d\pmb{z}_j-\int q_j \ln q_j d\pmb{z}_j+\mathrm{const}
\end{split}
\end{split}\]</div>
<p>  我们关心的是分布<span class="math notranslate nohighlight">\(q_j\)</span>，因此可以固定<span class="math notranslate nohighlight">\(q_{i\neq j}\)</span>再对<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>最大化。可以发现等式<a class="reference internal" href="#equation-mean-field">(19)</a>等价于<span class="math notranslate nohighlight">\(-\mathrm{KL}(q_j||\tilde{p}(\pmb{x},\pmb{z}))\)</span>，即当<span class="math notranslate nohighlight">\(q_j=\tilde{p}(\pmb{x},\pmb{z})\)</span>时<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>最大。因此可知变量子集<span class="math notranslate nohighlight">\(\pmb{z}_j\)</span>服从的最优分布<span class="math notranslate nohighlight">\(q_j^*\)</span>应满足，</p>
<div class="math notranslate nohighlight">
\[
\ln q_j^*(\pmb{z}_j)=\mathbb{E}_{i\neq j}[\ln p(\pmb{x},\pmb{z})]+\mathrm{const}
\]</div>
<p>即，</p>
<div class="math notranslate nohighlight" id="equation-optimize-solution">
<span class="eqno">(20)<a class="headerlink" href="#equation-optimize-solution" title="Link to this equation">¶</a></span>\[
q_j^*(\pmb{z}_j)=\frac{\exp(\mathbb{E}_{i\neq j}[\ln p(\pmb{x},\pmb{z})])}{\int \exp(\mathbb{E}_{i\neq j}[\ln p(\pmb{x},\pmb{z})]) d\pmb{z}_j}
\]</div>
<p>  最终，在满足条件<a class="reference internal" href="#equation-q-factor">(18)</a>下，变量子集<span class="math notranslate nohighlight">\(\pmb{z}_j\)</span>最接近的真实分布由<a class="reference internal" href="#equation-optimize-solution">(20)</a>得到。</p>
<p>  通过恰当的分割独立变量子集<span class="math notranslate nohighlight">\(\pmb{z}_j\)</span>并选择<span class="math notranslate nohighlight">\(q_j\)</span>服从的分布，<span class="math notranslate nohighlight">\(\mathbb{E}_{i\neq j}[\ln p(\pmb{x},\pmb{z})]\)</span>往往有闭式解，这使得基于<a class="reference internal" href="#equation-optimize-solution">(20)</a>能高效对隐变量<span class="math notranslate nohighlight">\(\pmb{z}\)</span>进行推断。事实上，对变量<span class="math notranslate nohighlight">\(\pmb{z}_j\)</span>分布<span class="math notranslate nohighlight">\(q_j^*\)</span>进行估计时整合了<span class="math notranslate nohighlight">\(\pmb{z}_j\)</span>之外的其他<span class="math notranslate nohighlight">\(\pmb{z}_{i\neq j}\)</span>的信息，这是通过联合似然函数<span class="math notranslate nohighlight">\(\ln p(\pmb{x},\pmb{z})\)</span>在<span class="math notranslate nohighlight">\(\pmb{z}_j\)</span>之外的隐变量分布上求期望得到的，因此也被称为<strong>均值场(mean field)方法</strong>。</p>
</section>
</section>
</section>
</section>
</section>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id19" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://sspuiip.github.io/ml/mathmodel/mcmc.html#mcmc">MCMC采样</a></p>
</aside>
</aside>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">目录</a></h3>
    <ul>
<li><a class="reference internal" href="#">4. 概率图模型</a><ul>
<li><a class="reference internal" href="#id2">4.1. 贝叶斯网</a><ul>
<li><a class="reference internal" href="#id3">4.1.1. 联合分布</a></li>
<li><a class="reference internal" href="#id4">4.1.2. 有向图的条件独立性</a><ul>
<li><a class="reference internal" href="#d">4.1.2.1. <span class="math notranslate nohighlight">\(d\)</span>-分离</a></li>
<li><a class="reference internal" href="#dagmarkov">4.1.2.2. DAG的其它Markov属性</a></li>
<li><a class="reference internal" href="#id5">4.1.2.3. 道德图</a></li>
<li><a class="reference internal" href="#markov">4.1.2.4. Markov毯</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id6">4.1.3. 隐马尔可夫模型</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id7">4.2. 马尔可夫随机场</a><ul>
<li><a class="reference internal" href="#id8">4.2.1. 联合概率</a></li>
<li><a class="reference internal" href="#id9">4.2.2. 条件独立性</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id10">4.3. 推断与参数学习</a><ul>
<li><a class="reference internal" href="#id11">4.3.1. 精确推断</a><ul>
<li><a class="reference internal" href="#id12">4.3.1.1. 变量消除法</a></li>
<li><a class="reference internal" href="#id13">4.3.1.2. 信念传播</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id14">4.3.2. 近似推断</a><ul>
<li><a class="reference internal" href="#mcmc">4.3.2.1. MCMC采样</a></li>
<li><a class="reference internal" href="#id16">4.3.2.2. 变分推断</a><ul>
<li><a class="reference internal" href="#id17">4.3.2.2.1. 盘式记法</a></li>
<li><a class="reference internal" href="#id18">4.3.2.2.2. 均值场方法</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>上一主题</h4>
    <p class="topless"><a href="EM.html"
                          title="上一章"><span class="section-number">3. </span>EM算法概述</a></p>
  </div>
  <div>
    <h4>下一主题</h4>
    <p class="topless"><a href="bayes.html"
                          title="下一章"><span class="section-number">5. </span>贝叶斯分类</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>本页</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/ml/PGM.md.txt"
            rel="nofollow">显示源代码</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">快速搜索</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="提交" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             >索引</a></li>
        <li class="right" >
          <a href="bayes.html" title="5. 贝叶斯分类"
             >下一页</a> |</li>
        <li class="right" >
          <a href="EM.html" title="3. EM算法概述"
             >上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">4. </span>概率图模型</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; 版权所有 2022-2024, SSPUIIP.
      由 <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3创建。
    </div>
  </body>
</html>