<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>6. 集成学习 &#8212; Machine Learning Fundation 1.0 文档</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/nature.css?v=279e0f84" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <script src="../_static/documentation_options.js?v=f115507d"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="../_static/mathjax/tex-chtml.js"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.8.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="7. 神经网络" href="neuro_network.html" />
    <link rel="prev" title="5. 概率图模型" href="PGM.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             accesskey="I">索引</a></li>
        <li class="right" >
          <a href="neuro_network.html" title="7. 神经网络"
             accesskey="N">下一页</a> |</li>
        <li class="right" >
          <a href="PGM.html" title="5. 概率图模型"
             accesskey="P">上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">6. </span>集成学习</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1><span class="section-number">6. </span>集成学习<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h1>
<p>  <strong>集成学习</strong>其实就是学习器集成，即通过构建多个学习器完成学习任务，综合所有学习器的学习结果按特定策略生成集成学习器的学习结果。结合策略一般有：平均法、投票法以及学习法等。</p>
<figure class="align-center" id="id6">
<pre  class="mermaid">
        %%{
    init: {
        'theme':'base',
        'themeVariables': {
            'fontSize': 8px
        }
    }
}%%
graph LR
A[集成学习] --&gt;B[不同类预测器同一训练集];
A[集成学习] -- 并行集成 --&gt;C[同类预测器采样训练子集];
B--&gt;D[硬投票法];
B--&gt;E[软投票法];
C--子集样本放回--&gt;F[Bagging];
C--子集样本不放回--&gt;G[Pasting];
A-- 串行集成 --&gt;H[训练模型集成];
H--&gt;I[Boosting];
H--&gt;J[Stacking];
I--调整样本权重与预测器权重--&gt;K[Adaboost];
I--对前一预测器的结果残差训练--&gt;L[GradientBoosting];
    </pre><figcaption>
<p><span class="caption-text">Fig 1. 集成学习概要</span><a class="headerlink" href="#id6" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>为什么需要集成学习</strong>？</p></li>
</ul>
<p>  （1）<strong>通俗一点的解释：和”三个臭皮匠抵个诸葛亮“是一个道理</strong>。<br>  （2）<strong>从理论的角度：集成学习方法可以把弱学习器变成可以精确预测的强学习器</strong>。在PAC(Probabily approximately correct)学习框架中，如果存在一个多项式的学习算法可以学习一个类（概念），并且正确率很高，那么这个类（概念）是<strong>强可学习</strong>的。如果存在一个多项式的学习算法可以学习一个类（概念），学习效果（正确率）仅比随机猜测略好，那么这个概念是<strong>弱可学习</strong>的。已有研究证明：<strong>强可学习与弱可学习是等价的</strong>。也就是说，在PAC学习框架下，一个概念强可学习的充要条件是这个概念是弱可学习的。那么，已经发现的“弱可学习算法”，如何提升为“强可学习算法”？答案是集成学习。</p>
<section id="id2">
<h2><span class="section-number">6.1. </span>投票法<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h2>
<p>  要创建一个更好的分类器，是简单的办法就是聚合每个分类器的预测，然后将得票最多的结果作为预测类别，这种大多数投票分类器被称为<strong>硬投票分类器</strong>。如下图所示：</p>
<figure class="align-center" id="id7">
<pre  class="mermaid">
        %%{
    init: {
        'theme':'base',
        'themeVariables': {
            'fontSize': 8px
        }
    }
}%%
graph LR
	A((数据集)) --train--&gt; B[逻辑回归];
	A --train--&gt; C[SVM];
	A --train--&gt; D[随机森林];
	A --train--&gt; E[...];
	A --train--&gt; F[其它分类器];
	B -.predict.-&gt; G[+1/-1];
	C -.predict.-&gt; H[+1/-1];
	D -.predict.-&gt; I[+1/-1];
	E -.predict.-&gt; J[+1/-1];
	F -.predict.-&gt; K[+1/-1];
	G --vote--&gt; L[+1/-1];
	H --vote--&gt; L[+1/-1];
	I --vote--&gt; L[+1/-1];
	J --vote--&gt; L[+1/-1];
	K --vote--&gt; L[+1/-1];
    </pre><figcaption>
<p><span class="caption-text">Fig 2. 多学习器集成–投票法</span><a class="headerlink" href="#id7" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>投票法效果</strong></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_moons</span>
<span class="c1">#数据集</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">VotingClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>

<span class="n">log_clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;liblinear&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">rnd_clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">svm_clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1">#硬投票法集成</span>
<span class="n">voting_clf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">log_clf</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">rnd_clf</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;svc&#39;</span><span class="p">,</span> <span class="n">svm_clf</span><span class="p">)],</span>
    <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;hard&#39;</span><span class="p">)</span>
<span class="n">voting_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="p">(</span><span class="n">log_clf</span><span class="p">,</span> <span class="n">rnd_clf</span><span class="p">,</span> <span class="n">svm_clf</span><span class="p">,</span> <span class="n">voting_clf</span><span class="p">):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>LogisticRegression 0.864
RandomForestClassifier 0.872
SVC 0.888
VotingClassifier 0.896
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">log_clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;liblinear&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">rnd_clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">svm_clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1">#软投票法</span>
<span class="n">voting_clf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">log_clf</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">rnd_clf</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;svc&#39;</span><span class="p">,</span> <span class="n">svm_clf</span><span class="p">)],</span>
    <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">)</span>
<span class="n">voting_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">for</span> <span class="n">clf</span> <span class="ow">in</span> <span class="p">(</span><span class="n">log_clf</span><span class="p">,</span> <span class="n">rnd_clf</span><span class="p">,</span> <span class="n">svm_clf</span><span class="p">,</span> <span class="n">voting_clf</span><span class="p">):</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>LogisticRegression 0.864
RandomForestClassifier 0.872
SVC 0.888
VotingClassifier 0.912
</pre></div>
</div>
<p>  投票分类器的准确率通常比集成中最好的分类器还要高。事实上，即使每个分类器都是弱学习器，通过集成依然可以实现一个强学习器。只要有足够大数量且多种类的弱学习器即可。</p>
<ul class="simple">
<li><p><strong>为什么会有效？</strong></p></li>
</ul>
<p>  下面的类比可以帮助理解。假设一个略微偏倚的硬币，它有51％的机率正面向上。如果投1000次，大致会得到510次正面，所以正面是大多数。也就是说，在1000次投掷之后，大多数硬币正面向上（1000硬币参与结果投票，至少501个硬币结果为正面）的概率接近75％。即</p>
<div class="math notranslate nohighlight">
\[
1-\textrm{pbinom}(499,1000,0.51)\approx 74.67\%
\]</div>
<p>10000次后，这个概率达到97%。同理，假设有1000个分类器的集成，每个分类器都只有51%的概率是正确的。如果，以大多数投票的类别作为预测结果，可以达到准确率接近75%。</p>
</section>
<section id="baggingpasting">
<h2><span class="section-number">6.2. </span>并行集成：bagging和pasting<a class="headerlink" href="#baggingpasting" title="Link to this heading">¶</a></h2>
<p>  每个预测器使用相同算法，但在训练集的随机子集上进行训练。如果样本放回，这种方法称为bagging(bootstrap aggregating)；如果样本不放回，这种方法称为pasting。</p>
<figure class="align-center" id="id8">
<pre  class="mermaid">
        %%{
    init: {
        'theme':'base',
        'themeVariables': {
            'fontSize': 8px
        }
    }
}%%
graph LR
	A[(训练集)]  --随机选择--&gt; B[(子集1)]
	A[(训练集)]  --随机选择--&gt; C[(子集2)]
	A[(训练集)]  --随机选择--&gt; D[(子集3)]
	A[(训练集)]  --随机选择--&gt; E[(...)]
	A[(训练集)]  --随机选择--&gt; F[(子集k)]
	B --训练--&gt; G[分类器1]
	C --训练--&gt; H[分类器2]
	D --训练--&gt; I[分类器3]
	E --训练--&gt; J[...]
	 F --训练--&gt; K[分类器k]
	 G --测试集预测--&gt; L[+1/-1]
	 H --测试集预测--&gt;M[+1/-1]
	 I --测试集预测--&gt; N[+1/-1]
	J --测试集预测--&gt; O[+1/-1]
	K --测试集预测--&gt; P[+1/-1]
	L --集成--&gt; Q[+1/-1]
	O--集成--&gt; Q[+1/-1]
	M--集成--&gt; Q[+1/-1]
	N --集成--&gt; Q[+1/-1]
	P --集成--&gt; Q[+1/-1]
    </pre><figcaption>
<p><span class="caption-text">Fig 3. 并行集成示例</span><a class="headerlink" href="#id8" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>并行集成的效果示例</strong></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#准备数据集</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_moons</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.30</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1">#Bagging集成分类器</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="n">bag_clf</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span>
    <span class="n">DecisionTreeClassifier</span><span class="p">(),</span> <span class="c1">#分类器种类</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>   <span class="c1">#分类器个数</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>    <span class="c1">#训练子集样本数</span>
    <span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>     <span class="c1">#bagging=True; pasting=False</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">bag_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">bag_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1">#打印结果</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="nb">print</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1">#对比决策树</span>
<span class="n">tree_clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tree_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred_tree</span> <span class="o">=</span> <span class="n">tree_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_tree</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0.904
0.856
</pre></div>
</div>
</section>
<section id="id3">
<h2><span class="section-number">6.3. </span>串行集成：提升法<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h2>
<p>  提升法是指可以将几个弱学习器结合成一个强学习器的任意集成方法。大多数提升法的<strong>总体思路</strong>是循环训练预测器，每次都其前序做出一些改正。</p>
<figure class="align-center" id="id9">
<pre  class="mermaid">
        %%{
    init: {
        'theme':'base',
        'themeVariables': {
            'fontSize': 8px
        }
    }
}%%
flowchart  LR
  A2 o--o C1;
  A3 o--o C2;
  A4 o--o C3;  
  subgraph  次序训练1 
  A1[数据集0]--训练--&gt;B1[预测器];
  B1 --调整数据权重--&gt; C1[数据集1];
  end
  subgraph 次序训练2
  A2[数据集1]--训练--&gt;B2[预测器];
  B2--调整数据权重--&gt; C2[数据集2];
  end
  subgraph 次序训练...
  A3[...]-..- C3[...];
  end
  subgraph 次序训练n
  A4[数据集n-1]--训练--&gt;B4[预测器n];
  end
    </pre><figcaption>
<p><span class="caption-text">Fig 4. 提升法示意</span><a class="headerlink" href="#id9" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<section id="adaboost">
<h3><span class="section-number">6.3.1. </span>Adaboost<a class="headerlink" href="#adaboost" title="Link to this heading">¶</a></h3>
<p>  假设二分类情况，以下给出了Adaboost算法的框架。给定训练集<span class="math notranslate nohighlight">\(T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}\)</span>，其中<span class="math notranslate nohighlight">\(x_i\in\mathcal{X}\subseteq \mathbb{R}^n, y_i\in\mathcal{Y}=\{+1,-1\}\)</span>。</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>算法：Adaboost<span class="math notranslate nohighlight">\(\left(S=\{(\pmb{x}_1,y_1),(\pmb{x}_2,y_2),...,(\pmb{x}_m,y_m)\}\right)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>01 <strong>for</strong> <span class="math notranslate nohighlight">\(i=1\)</span> <strong>to</strong> <span class="math notranslate nohighlight">\(m\)</span><br>02   <span class="math notranslate nohighlight">\(w_1(i)=\frac1m\)</span><br>03 <strong>for</strong> <span class="math notranslate nohighlight">\(t=1\)</span> <strong>to</strong> <span class="math notranslate nohighlight">\(T\)</span><br>04   <span class="math notranslate nohighlight">\(f_t\leftarrow\)</span>误差<span class="math notranslate nohighlight">\(\epsilon_t=\mathop{\mathbb{P}}\limits_{\pmb{x}_i\sim w_t}\left[f_t(\pmb{x}_i)\neq y_i\right]\)</span>较小的基分类器<br>05   <span class="math notranslate nohighlight">\(\alpha_t\leftarrow \frac12\log\frac{1-\epsilon_t}{\epsilon_t}\)</span><br>06   <span class="math notranslate nohighlight">\(Z_t\leftarrow 2[\epsilon_t(1-\epsilon_t)]^{\frac12}\)</span> (归一化因子)<br>07   <strong>for</strong> <span class="math notranslate nohighlight">\(i=1\)</span> <strong>to</strong> <span class="math notranslate nohighlight">\(m\)</span><br>08     <span class="math notranslate nohighlight">\(w_{t+1}(i)\leftarrow\frac{w_t(i)\exp(-\alpha_t\times y_i\times f_t(\pmb{x}_i))}{Z_t}\)</span><br>09 <span class="math notranslate nohighlight">\(f\leftarrow \sum_{t=1}^T\alpha_t f_t\)</span><br>10 <strong>return</strong> <span class="math notranslate nohighlight">\(f\)</span></p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>初使化训练数据的权值分布</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
W_1=(w^{(1)},...,w^{(N)}), w_{1i}=\frac{1}{N},i=1,2,...,N
\]</div>
<p>使用该权值分布对第一个预测器进行训练,</p>
<div class="math notranslate nohighlight">
\[
f_j(x):\mathcal{X}\rightarrow \{+1,-1\}, j=1,2,...,M
\]</div>
<p>计算加权误差率<span class="math notranslate nohighlight">\(r_1\)</span>如下，</p>
<div class="math notranslate nohighlight">
\[
r_j=\frac{\sum_{i=1,y_j^{(i)}\neq y^{(i)}}^N w^{(i)} }{\sum_{i=1}^N w^{(i)} }, j=1,2,...,M
\]</div>
<p>计算预测器权重<span class="math notranslate nohighlight">\(a_j\)</span>如下，</p>
<div class="math notranslate nohighlight">
\[
\alpha_j=\eta\log\frac{1-r_j}{r_j}, j=1,2,...,M
\]</div>
<ul class="simple">
<li><p>更新权值</p></li>
</ul>
<p>  对于<span class="math notranslate nohighlight">\(i=1,2,...,N\)</span>，如果<span class="math notranslate nohighlight">\(y_j^{(i)}=y^{(i)}\)</span>，则<span class="math notranslate nohighlight">\(w^{(i)}\leftarrow w^{(i)}\)</span>；否则，</p>
<div class="math notranslate nohighlight">
\[
w^{(i)}\leftarrow w^{(i)}\exp(\alpha_j)
\]</div>
<p>最后归一化，即，</p>
<div class="math notranslate nohighlight">
\[
w^{(i)}=\frac{w^{(i)}}{\sum_{i=1}^N w^{(i)}}
\]</div>
<ul class="simple">
<li><p>以此规则，训练后序所有预测器。</p></li>
</ul>
<section id="id4">
<h4><span class="section-number">6.3.1.1. </span>AdaBoost分类器预测<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h4>
<p>  集成分类器训练好之后，就可以按以下规则预测，</p>
<div class="math notranslate nohighlight">
\[
\hat{y}(x)=\arg\max_{k} \sum_{j=1,\hat{y}_j(x)=k}^M \alpha_j
\]</div>
</section>
<section id="id5">
<h4><span class="section-number">6.3.1.2. </span>AdaBoost例子<a class="headerlink" href="#id5" title="Link to this heading">¶</a></h4>
<p>  给定以下训练数据。假设弱分类器由<span class="math notranslate nohighlight">\(x&lt;v\)</span>或<span class="math notranslate nohighlight">\(x&gt;v\)</span>产生，其阈值<span class="math notranslate nohighlight">\(v\)</span>使该分类器在训练数据集上分类误差率最低。下面用AdaBoost算法学习一个强分类器。</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>序号</p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p>3</p></th>
<th class="head"><p>4</p></th>
<th class="head"><p>5</p></th>
<th class="head"><p>6</p></th>
<th class="head"><p>7</p></th>
<th class="head"><p>8</p></th>
<th class="head"><p>9</p></th>
<th class="head"><p>10</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(x\)</span></p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>2</p></td>
<td><p>3</p></td>
<td><p>4</p></td>
<td><p>5</p></td>
<td><p>6</p></td>
<td><p>7</p></td>
<td><p>8</p></td>
<td><p>9</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(y\)</span></p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>-1</p></td>
<td><p>-1</p></td>
<td><p>-1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>-1</p></td>
</tr>
</tbody>
</table>
<div align="center">表1：训练数据</div>
<p>  第一步，先求数据集的初使权值分布，</p>
<div class="math notranslate nohighlight">
\[
w^{(i)}=\frac{1}{10}, i=1,2,...,10
\]</div>
<p>在这个权值分布训练数据上，阈值取3.5时，分类误差率最低，故基本分类器为<span class="math notranslate nohighlight">\(f_1(x)=1; x&lt;3.5\)</span>。<span class="math notranslate nohighlight">\(f_1(x)\)</span>在训练集的误差<span class="math notranslate nohighlight">\(r_1\)</span>为<span class="math notranslate nohighlight">\(0.3\)</span>。所以，分类器<span class="math notranslate nohighlight">\(f_1(x)\)</span>的权值为：<span class="math notranslate nohighlight">\(a_1=\eta\log\frac{1-r_1}{r_1}=0.4236\)</span>，这里<span class="math notranslate nohighlight">\(\eta=0.5\)</span>。</p>
<p>  第二步，更新权值分布，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
w^{(i)}&amp;=0.1;\quad i=1,2,...,6,10.\\
w^{(i)}&amp;=0.1\exp(a_1)=0.1527;\quad i=7,8,9.
\end{split}
\end{split}\]</div>
<p>  归一化后得，</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>序号</p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p>3</p></th>
<th class="head"><p>4</p></th>
<th class="head"><p>5</p></th>
<th class="head"><p>6</p></th>
<th class="head"><p>7</p></th>
<th class="head"><p>8</p></th>
<th class="head"><p>9</p></th>
<th class="head"><p>10</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(w\)</span></p></td>
<td><p>0.08633</p></td>
<td><p>0.08633</p></td>
<td><p>0.08633</p></td>
<td><p>0.08633</p></td>
<td><p>0.08633</p></td>
<td><p>0.08633</p></td>
<td><p>0.13188</p></td>
<td><p>0.13188</p></td>
<td><p>0.13188</p></td>
<td><p>0.08633</p></td>
</tr>
</tbody>
</table>
<p>分类器<span class="math notranslate nohighlight">\(f_1(x)\)</span>的权值<span class="math notranslate nohighlight">\(u^{(1)}\)</span>为：<span class="math notranslate nohighlight">\(0.4236\)</span></p>
<p>  第三步，以第一步和第二步的规则执行，直到所有分类器都已训练。</p>
<p>  最终分类器为</p>
<div class="math notranslate nohighlight">
\[
f(x)=\textrm{sign}(u^{(1)}f_1(x)+u^{(2)}f_2(x)+...+u^{(k)}f_k(x))
\]</div>
</section>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">目录</a></h3>
    <ul>
<li><a class="reference internal" href="#">6. 集成学习</a><ul>
<li><a class="reference internal" href="#id2">6.1. 投票法</a></li>
<li><a class="reference internal" href="#baggingpasting">6.2. 并行集成：bagging和pasting</a></li>
<li><a class="reference internal" href="#id3">6.3. 串行集成：提升法</a><ul>
<li><a class="reference internal" href="#adaboost">6.3.1. Adaboost</a><ul>
<li><a class="reference internal" href="#id4">6.3.1.1. AdaBoost分类器预测</a></li>
<li><a class="reference internal" href="#id5">6.3.1.2. AdaBoost例子</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>上一主题</h4>
    <p class="topless"><a href="PGM.html"
                          title="上一章"><span class="section-number">5. </span>概率图模型</a></p>
  </div>
  <div>
    <h4>下一主题</h4>
    <p class="topless"><a href="neuro_network.html"
                          title="下一章"><span class="section-number">7. </span>神经网络</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>本页</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/ml/ensemble.md.txt"
            rel="nofollow">显示源代码</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">快速搜索</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="提交" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             >索引</a></li>
        <li class="right" >
          <a href="neuro_network.html" title="7. 神经网络"
             >下一页</a> |</li>
        <li class="right" >
          <a href="PGM.html" title="5. 概率图模型"
             >上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">6. </span>集成学习</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; 版权所有 2022-2024, SSPUIIP.
      由 <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3创建。
    </div>
  </body>
</html>