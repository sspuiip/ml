<!DOCTYPE html>

<html lang="zh-CN" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4. 计算学习理论 &#8212; Machine Learning Fundation 1.0 文档</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/nature.css?v=279e0f84" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <script src="../_static/documentation_options.js?v=f115507d"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="../_static/translations.js?v=beaddf03"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="../_static/mathjax/tex-chtml.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="1. 数据降维" href="../ml/dimension_reduce_2.html" />
    <link rel="prev" title="3. 高斯过程" href="gaussian_process.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             accesskey="I">索引</a></li>
        <li class="right" >
          <a href="../ml/dimension_reduce_2.html" title="1. 数据降维"
             accesskey="N">下一页</a> |</li>
        <li class="right" >
          <a href="gaussian_process.html" title="3. 高斯过程"
             accesskey="P">上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">4. </span>计算学习理论</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1><span class="section-number">4. </span>计算学习理论<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h1>
<p>  计算学习理论是指通过计算来学习的理论<a class="footnote-reference brackets" href="#mlzhou" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>。</p>
<p>  假设有样本集<span class="math notranslate nohighlight">\(D=\{(\pmb{x}_1,y_1),(\pmb{x}_2,y_2),...,(\pmb{x}_m,y_m)\},\pmb{x}_i\in\mathcal{X}, y_i\in\mathcal{Y}\)</span>，所有样本服从一个隐含未知的分布<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>，<span class="math notranslate nohighlight">\(D\)</span>中所有样本都是独立地从这个分布上采样而来。</p>
<p>  令<span class="math notranslate nohighlight">\(h:\mathcal{X}\rightarrow \mathcal{Y}\)</span>，则其<strong>泛化误差</strong>为，</p>
<div class="math notranslate nohighlight" id="equation-gen-error">
<span class="eqno">(1)<a class="headerlink" href="#equation-gen-error" title="Link to this equation">¶</a></span>\[
E[h;\mathcal{D}]=P_{\pmb{x}\in\mathcal{D}}[h(\pmb{x})\neq y]
\]</div>
<p><span class="math notranslate nohighlight">\(h\)</span>在<span class="math notranslate nohighlight">\(D\)</span>上的<strong>经验误差</strong>为，</p>
<div class="math notranslate nohighlight" id="equation-exp-error">
<span class="eqno">(2)<a class="headerlink" href="#equation-exp-error" title="Link to this equation">¶</a></span>\[
\hat{E}[h;D]=\frac{1}{m}\sum_{i=1}^m\mathbb{I}[h(\pmb{x}_i)\neq y_i]
\]</div>
<p>由于<span class="math notranslate nohighlight">\(D\)</span>是<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>的独立同分布采样，因此<span class="math notranslate nohighlight">\(h\)</span>的经验误差的期望等于其泛化误差。若<span class="math notranslate nohighlight">\(h\)</span>在数据集<span class="math notranslate nohighlight">\(D\)</span>上的经验误差为0，则称<span class="math notranslate nohighlight">\(h\)</span>与<span class="math notranslate nohighlight">\(D\)</span><strong>一致</strong>，否则称其与<span class="math notranslate nohighlight">\(D\)</span><strong>不一致</strong>。任意两个映射<span class="math notranslate nohighlight">\(h_1,h_2\in \mathcal{X}\rightarrow\mathcal{Y}\)</span>，可以通过其<strong>不合</strong>来度量它们之间的差别，</p>
<div class="math notranslate nohighlight" id="equation-eq-consistant">
<span class="eqno">(3)<a class="headerlink" href="#equation-eq-consistant" title="Link to this equation">¶</a></span>\[
d(h_1,h_2)=P_{\pmb{x}\in\mathcal{D}}[h_1(\pmb{x})\neq h_2(\pmb{x})]
\]</div>
<ol class="arabic">
<li><p><strong>常用不等式</strong></p>
<ul class="simple">
<li><p>Jensen不等式。对于任意凸函数<span class="math notranslate nohighlight">\(f(x)\)</span>有，</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-jensen">
<span class="eqno">(4)<a class="headerlink" href="#equation-eq-jensen" title="Link to this equation">¶</a></span>\[
   f(\mathbb{E}[x])\le\mathbb{E}[f(x)]
   \]</div>
<ul class="simple">
<li><p>Hoeffding不等式。若<span class="math notranslate nohighlight">\(x_1,...,x_m\)</span>为<span class="math notranslate nohighlight">\(m\)</span>个独立随机变量，且满足<span class="math notranslate nohighlight">\(0\le x_i\le 1\)</span>，则对于任意<span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span>有，</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-hoeffding">
<span class="eqno">(5)<a class="headerlink" href="#equation-eq-hoeffding" title="Link to this equation">¶</a></span>\[
   P\left(\frac1m\sum_{i=1}^m x_i-\frac1m\sum_{i=1}^m\mathbb{E}(x_i)\ge\epsilon \right)\le\exp(-2m\epsilon^2)
   \]</div>
<ul>
<li><p>McDiamid不等式。若<span class="math notranslate nohighlight">\(x_1,...,x_m\)</span>为<span class="math notranslate nohighlight">\(m\)</span>个独立随机变量，且对于任意<span class="math notranslate nohighlight">\(1\le i\le m\)</span>，函数<span class="math notranslate nohighlight">\(f\)</span>满足，</p>
<div class="math notranslate nohighlight">
\[
     \sup_{x_1,...,x_m,x_i'}\left| f(x_1,...,x_m)-f(x_1,...,x_{i-1},x_i',x_{i+1},...,x_m) \right|\le c_i
     \]</div>
<p>则对于任意<span class="math notranslate nohighlight">\(\epsilon&gt;0\)</span>，有</p>
<div class="math notranslate nohighlight" id="equation-eq-mcdiamid">
<span class="eqno">(6)<a class="headerlink" href="#equation-eq-mcdiamid" title="Link to this equation">¶</a></span>\[
     P\left( f(x_1,...,x_m) -\mathbb{E}[f(x_1,...,x_m)] \ge\epsilon\right)\le \exp\left(\frac{-2\epsilon^2}{\sum_i c_i^2}\right)
     \]</div>
</li>
</ul>
</li>
</ol>
<section id="id3">
<h2><span class="section-number">4.1. </span>概率近似正确学习理论<a class="headerlink" href="#id3" title="Link to this heading">¶</a></h2>
<p>  计算学习理论中最基本的是概率近似正确(probably approximately correct, PAC)学习理论。</p>
<p>  令<span class="math notranslate nohighlight">\(c\)</span>表示<strong>概念</strong>，即从样本空间<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>到标记空间<span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>的映射。若对于任意样本<span class="math notranslate nohighlight">\((\pmb{x}_i,y_i),i\in\{1,2,...\}\)</span>，有<span class="math notranslate nohighlight">\(c(\pmb{x}_i)=y_i\)</span>，则称<span class="math notranslate nohighlight">\(c\)</span>为<strong>目标概念</strong>。所有希望学得的目标概念所构成的集合称为<strong>概念类</strong>(concept class)，记为<span class="math notranslate nohighlight">\(\mathcal{C}\)</span>。</p>
<p>  给定学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>，它所考虑的所有可能概念的集合称为<strong>假设空间</strong>（hypothesis space）,记为<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>。由于学习算法并不知道概念类的真实存在，因此，<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>和<span class="math notranslate nohighlight">\(\mathcal{C}\)</span>通常是不同的，学习算法会把自认为可能的目标概念集中起来构成<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>。对于<span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>，由于并不能确定它是否为真目标概念，因此称为<strong>假设</strong>。</p>
<p>  若目标概念<span class="math notranslate nohighlight">\(c\in\mathcal{H}\)</span>，则<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>中存在假设能将所有示例按与真实标记一致的方式完全分开，则称该问题对学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>是<strong>可分的</strong>(separable)，亦称为<strong>一致的</strong>(consistent)。反之，若<span class="math notranslate nohighlight">\(c\notin\mathcal{H}\)</span>，则称之为不可分的、不一致的。</p>
<p>  给定训练集<span class="math notranslate nohighlight">\(D\)</span>，我们希望基于学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>所学模型的对应假设<span class="math notranslate nohighlight">\(h\)</span>尽可能接近目标概念<span class="math notranslate nohighlight">\(c\)</span>。但由于机器学习过程受到多种因素制约，例如:</p>
<ul class="simple">
<li><p>训练集样本数有限</p></li>
</ul>
<p>  由于样本数有限，通常会存在一些在<span class="math notranslate nohighlight">\(D\)</span>上“等效”的假设，学习算法对它们无法区别。</p>
<ul class="simple">
<li><p>采样得到训练集<span class="math notranslate nohighlight">\(D\)</span>的偶然性</p></li>
</ul>
<p>  即便是同样大小的训练集，学得结果也可能有所不同。</p>
<p>  因此，我们还是希望以较大的把握学得比较好的模型。也就是说，以较大的概率学得误差满足预设上限的模型。这就是“概率”、“近似正确”的含义。</p>
<section id="pac">
<h3><span class="section-number">4.1.1. </span>PAC<a class="headerlink" href="#pac" title="Link to this heading">¶</a></h3>
<p>  若定义<span class="math notranslate nohighlight">\(\delta\)</span>为置信度，则形式上可以给出PAC辩识的定义如下，</p>
<blockquote>
<div><p><strong>定义</strong> (<strong>PAC辩识</strong>). 对于<span class="math notranslate nohighlight">\(0&lt;\epsilon, \delta&lt;1\)</span>，所有<span class="math notranslate nohighlight">\(c\in\mathcal{C}\)</span>和分布<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>，若存在学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>，其输出假设<span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>满足</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-def-pac-identify">
<span class="eqno">(7)<a class="headerlink" href="#equation-def-pac-identify" title="Link to this equation">¶</a></span>\[
P(E(h)\le\epsilon)\ge 1-\delta
\]</div>
<p>则称学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>能从假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>中PAC辩识概念类<span class="math notranslate nohighlight">\(\mathcal{C}\)</span>。</p>
<blockquote>
<div><p><strong>定义</strong> (<strong>PAC可学习</strong>). 令<span class="math notranslate nohighlight">\(m\)</span>表示从分布<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>中独立同分布采样得到的样例数目，<span class="math notranslate nohighlight">\(0&lt;\epsilon, \delta&lt;1\)</span>，对于所有分布<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>，若存在学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>和多项式函数<span class="math notranslate nohighlight">\(\textrm{ploy}()\)</span>，使得对于<strong>任何<span class="math notranslate nohighlight">\(m\ge \textrm{ploy}(1/\epsilon,1/\delta,size(x),size(c))\)</span></strong>，<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>能从假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>中PAC辩识概念类<span class="math notranslate nohighlight">\(\mathcal{C}\)</span>，则称概念类<span class="math notranslate nohighlight">\(\mathcal{C}\)</span>对于假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>而言是PAC可学习的。也称之为概念类<span class="math notranslate nohighlight">\(\mathcal{C}\)</span>是PAC可学习的。</p>
</div></blockquote>
<blockquote>
<div><p><strong>定义</strong> (<strong>PAC学习算法</strong>). 若学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>使概念类<span class="math notranslate nohighlight">\(\mathcal{C}\)</span>为PAC可学习的，且<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>的<strong>运行时间</strong>也是多项式函数<span class="math notranslate nohighlight">\(\textrm{ploy}(1/\epsilon,1/\delta,size(x),size(c))\)</span>，则称概念类<span class="math notranslate nohighlight">\(\mathcal{C}\)</span>是高效PAC可学习的，称<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>为概念类<span class="math notranslate nohighlight">\(\mathcal{C}\)</span>的PAC学习算法。</p>
</div></blockquote>
<blockquote>
<div><p><strong>定义</strong> (<strong>样本复杂度</strong>). 满足PAC学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>所需<span class="math notranslate nohighlight">\(m\ge \textrm{ploy}(1/\epsilon,1/\delta,size(x),size(c))\)</span>的<strong>最小值</strong>，称为学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>的样本复杂度。</p>
</div></blockquote>
<blockquote>
<div><p><strong>显然，PAC学习给出了一个抽象刻画机器学习能力的框架。基于这个框架，能对很多问题进行探讨：例如研究某任务在什么条件下可学习到较好模型？需要多少样本才能获得好模型？某算法在什么条件下可进行有效学习等。</strong></p>
</div></blockquote>
<section id="mathcal-h">
<h4><span class="section-number">4.1.1.1. </span>有限假设空间(<span class="math notranslate nohighlight">\(|\mathcal{H}|\)</span>有限)<a class="headerlink" href="#mathcal-h" title="Link to this heading">¶</a></h4>
<ol class="arabic simple">
<li><p><strong>可分情况</strong>(<span class="math notranslate nohighlight">\(c\in\mathcal{H}\)</span>)</p></li>
</ol>
<p>  （1）模型。该情况下，使用“排除法”，给定<span class="math notranslate nohighlight">\(m\)</span>个样本集<span class="math notranslate nohighlight">\(D\)</span>可以找到满足误差参数<span class="math notranslate nohighlight">\(\epsilon\)</span>的假设<span class="math notranslate nohighlight">\(h\)</span>。首先排除与<span class="math notranslate nohighlight">\(D\)</span>不一致的假设，直到<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>中只剩下1个为止。假设空间可能存在不止一个与<span class="math notranslate nohighlight">\(D\)</span>一致的等效假设，对这些假设，无法根据<span class="math notranslate nohighlight">\(D\)</span>来对它们的优劣进一步区分。</p>
<p>  （2）样本数。到底需要多少样本才能学习到目标概念<span class="math notranslate nohighlight">\(c\)</span>的有效近似呢？对PAC来说，只要训练集<span class="math notranslate nohighlight">\(D\)</span>的规模能让学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>以概率<span class="math notranslate nohighlight">\(1-\delta\)</span>找到目标假设的<span class="math notranslate nohighlight">\(\epsilon\)</span>近似即可。首先估算泛化误差大于<span class="math notranslate nohighlight">\(\epsilon\)</span>但在数据集上仍表现完美的假设出现的概率，即</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
P[h(\pmb{x})=y]&amp;=1-P[h(\pmb{x})\neq y]\\
&amp;=1-E[h] \\
&amp;\le 1-\epsilon
\end{split}
\end{split}\]</div>
<p>  由于<span class="math notranslate nohighlight">\(D\)</span>包含了<span class="math notranslate nohighlight">\(m\)</span>个从<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>独立同分布采样而得的样例，因此，<span class="math notranslate nohighlight">\(h\)</span>与<span class="math notranslate nohighlight">\(D\)</span>表现一致的概率为，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
P[h(\pmb{x}_1=y_1)\wedge h(\pmb{x}_2=y_2)\wedge ...\wedge h(\pmb{x}_m=y_m)]&amp;=(1-P[h(\pmb{x})\neq y])^m\\
&amp;\le (1-\epsilon)^m
\end{split}
\end{split}\]</div>
<p>  由于事先并不知道学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>输出<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>中的哪个假设，但仅需保证泛化误差大于<span class="math notranslate nohighlight">\(\epsilon\)</span>，且在<span class="math notranslate nohighlight">\(D\)</span>表现完美的所有假设出现概率之和不大于<span class="math notranslate nohighlight">\(\delta\)</span>即可：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
P(h\in\mathcal{H}:E[h]&gt;\epsilon\wedge\hat{E}[h]=0)&amp;&lt;|\mathcal{H}|(1-\epsilon)^m\\
&amp;&lt;\underbrace{|\mathcal{H}|e^{-m\epsilon}}_{e^{-x}&gt;1-x}
\end{split}
\end{split}\]</div>
<p>只要令上式不大于<span class="math notranslate nohighlight">\(\delta\)</span>，即，</p>
<div class="math notranslate nohighlight">
\[
|\mathcal{H}|e^{-m\epsilon}\le \delta
\]</div>
<p>则可以得到，</p>
<div class="math notranslate nohighlight">
\[
m\ge \frac{1}{\epsilon}\left(\ln|\mathcal{H}|+\ln\frac{1}{\epsilon} \right)
\]</div>
<p>  由此可知，有限假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>都是PAC可学习的，所需的样本数如上式所示，输出假设<span class="math notranslate nohighlight">\(h\)</span>的泛化误差随着样例数目的增多而收敛到0，收敛速率为<span class="math notranslate nohighlight">\(O(1/m)\)</span>。</p>
<ol class="arabic simple" start="2">
<li><p><strong>不可分情况</strong>(<span class="math notranslate nohighlight">\(c\notin\mathcal{H}\)</span>)</p></li>
</ol>
<p>  （1）模型。目标概念<span class="math notranslate nohighlight">\(c\)</span>往往不存在于假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>，假定对于任何<span class="math notranslate nohighlight">\(h\in\mathcal{H},\hat{E}(h)\neq 0\)</span>，也就是<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>中的任意假设都会在训练集上出现或多或少的错误。由Hoeffding不等式可知，</p>
<blockquote>
<div><p><strong>引理</strong> 1.若训练集<span class="math notranslate nohighlight">\(D\)</span>包含<span class="math notranslate nohighlight">\(m\)</span>个从分布<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>上独立同分布采样而得到的样例, <span class="math notranslate nohighlight">\(0&lt;\epsilon&lt;1\)</span>，则对于任意<span class="math notranslate nohighlight">\(h\in \mathcal{H}\)</span>有，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-eq-not-split">
<span class="eqno">(8)<a class="headerlink" href="#equation-eq-not-split" title="Link to this equation">¶</a></span>\[\begin{split}
\begin{split}
P(\hat{E}(h)-E(h)\ge\epsilon)&amp;\le\exp(-2m\epsilon^2)\\
P(E(h)-\hat{E}(h)\ge\epsilon)&amp;\le\exp(-2m\epsilon^2)\\
P(|\hat{E}(h)-E(h)|\ge\epsilon)&amp;\le 2\exp(-2m\epsilon^2)\\
\end{split}
\end{split}\]</div>
<blockquote>
<div><p><strong>推论</strong> 若训练集<span class="math notranslate nohighlight">\(D\)</span>包含<span class="math notranslate nohighlight">\(m\)</span>个从分布<span class="math notranslate nohighlight">\(D\)</span>上独立同分布采样而得到样本， <span class="math notranslate nohighlight">\(0&lt;\epsilon&lt;1\)</span>，则对任意<span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>，下式至少以<span class="math notranslate nohighlight">\(1-\delta\)</span>的概率成立，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-eq-hyposis-error">
<span class="eqno">(9)<a class="headerlink" href="#equation-eq-hyposis-error" title="Link to this equation">¶</a></span>\[
\hat{E}(h)-\sqrt{\frac{\ln(2/\delta)}{2m}}\le E(h)\le \hat{E}(h)+\sqrt{\frac{\ln(2/\delta)}{2m}}
\]</div>
<p>上式说明，样本数<span class="math notranslate nohighlight">\(m\)</span>较大时，<span class="math notranslate nohighlight">\(h\)</span>的经验误差是其泛化误差较好的近似。</p>
<blockquote>
<div><p><strong>定理</strong> 1 若<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>为有限假设空间，<span class="math notranslate nohighlight">\(0&lt;\delta&lt;1\)</span>，则对任意<span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>有，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-thm-hyposis">
<span class="eqno">(10)<a class="headerlink" href="#equation-thm-hyposis" title="Link to this equation">¶</a></span>\[
P\left(|E(h)-\hat{E}(h)|\le\sqrt{\frac{\ln|\mathcal{H}|+\ln(2/\delta)}{2m}} \right)\ge1-\delta
\]</div>
<p>  显然，当<span class="math notranslate nohighlight">\(c\notin\mathcal{H}\)</span>时，学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>无法学得目标概念<span class="math notranslate nohighlight">\(c\)</span>的近似。但是，当<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>给定时，必存在一个泛化误差最小的假设，找出此假设的<span class="math notranslate nohighlight">\(\epsilon\)</span>近似也不失为一个较好的目标。<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>中泛化最小的假设是<span class="math notranslate nohighlight">\(\arg\min\limits_{h\in\mathcal{H}}E(h)\)</span>，于是，以此为目标可将PAC学习推广至<span class="math notranslate nohighlight">\(c\notin\mathcal{H}\)</span>的情况，这也称之为“不可知学习”(agnostic learning)，即</p>
<blockquote>
<div><p><strong>定义</strong> (<strong>不可知PAC学习</strong>). 若存在学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>和多项式函数<span class="math notranslate nohighlight">\(\text{ploy}()\)</span>，使得对于任意<span class="math notranslate nohighlight">\(m\ge\text{ploy}(1/\epsilon,1/\delta,size(\pmb{x}),size(c))\)</span>，<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>都能从假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>输出满足下式的假设<span class="math notranslate nohighlight">\(h\)</span>:</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-def-no-pac-learning">
<span class="eqno">(11)<a class="headerlink" href="#equation-def-no-pac-learning" title="Link to this equation">¶</a></span>\[
P\{E(h)-\arg\min\limits_{h'\in\mathcal{H}}E(h')\le\epsilon\}\ge 1-\delta
\]</div>
<p>则称假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>是不可知PAC可学习的。</p>
</section>
</section>
<section id="vc">
<h3><span class="section-number">4.1.2. </span>VC维<a class="headerlink" href="#vc" title="Link to this heading">¶</a></h3>
<section id="id4">
<h4><span class="section-number">4.1.2.1. </span>无限假设空间(<span class="math notranslate nohighlight">\(|\mathcal{H}|\)</span>无限)<a class="headerlink" href="#id4" title="Link to this heading">¶</a></h4>
<p>  对于无限假设空间的学习，需要度量<em>假设空间的复杂度</em>。最常见的办法是考虑假设空间的“VC维”。</p>
<blockquote>
<div><p><strong>定义</strong> (<strong>VC维</strong>). VC维就是假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>能打散的最大示例集大小，即</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-eq-vc">
<span class="eqno">(12)<a class="headerlink" href="#equation-eq-vc" title="Link to this equation">¶</a></span>\[
\text{VC}(\mathcal{H})=\max\limits_{m}\{m: \Pi_{\mathcal{H}}(m)=2^m\}
\]</div>
<p>  所谓的<strong>打散</strong>是指：如果假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>能对示例集<span class="math notranslate nohighlight">\(D\)</span>实现<em>所有不相同的对分</em>，则称为<span class="math notranslate nohighlight">\(D\)</span>能被<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>打散。而<strong>对分</strong>(赋予标记一次)是指假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>的假设<span class="math notranslate nohighlight">\(h\)</span>对示例集<span class="math notranslate nohighlight">\(D\)</span>中示例赋予的一种结果。<span class="math notranslate nohighlight">\(\Pi_{\mathcal{H}}(m)\)</span>为<strong>增长函数</strong>，即</p>
<blockquote>
<div><p><strong>定义</strong> (<strong>增长函数</strong>). <span class="math notranslate nohighlight">\(\forall m\in\mathbb{N}\)</span>，设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>的增长函数<span class="math notranslate nohighlight">\(\Pi_{\mathcal{H}}(m)\)</span>为，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-eq-inc-fun">
<span class="eqno">(13)<a class="headerlink" href="#equation-eq-inc-fun" title="Link to this equation">¶</a></span>\[\Pi_{\mathcal{H}}(m)=\max\limits_{\{\pmb{x}_1,...,\pmb{x}_m\}\subseteq\mathcal{X}}\left|\{(h(\pmb{x}_1),...,h(\pmb{x}_m)|h\in\mathcal{H}\} \right|\]</div>
<p>  增长函数表示假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>对<span class="math notranslate nohighlight">\(m\)</span>个示例所能赋予标记的最大可能数目。这一结果越大，则<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>的表示能力越强，对学习任务的适应能力越强。我们可以利用增长函数来估计经验误差与泛化误差之间的关系。</p>
<blockquote>
<div><p><strong>定理</strong> 2. 对假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>， <span class="math notranslate nohighlight">\(m\in\mathbb{N}, 0&lt;\epsilon&lt;1\)</span>和任意<span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>有，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-inc-fun-error">
<span class="eqno">(14)<a class="headerlink" href="#equation-inc-fun-error" title="Link to this equation">¶</a></span>\[P(|E(h)-\hat{E}(h)|&gt;\epsilon)\le 4\Pi_{\mathcal{H}}(2m)\exp\left(-\frac{m\epsilon^2}{8} \right)\]</div>
<p>  (一) <strong>VC维计算</strong></p>
<blockquote>
<div><p>若存在大小为<span class="math notranslate nohighlight">\(d\)</span>的示例数据集能被<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>打散，但不存在任何大小为<span class="math notranslate nohighlight">\(d+1\)</span>的示例集被<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>打散，则<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>的VC维是<span class="math notranslate nohighlight">\(d\)</span>。</p>
<p class="attribution">—VC维计算方法</p>
</div></blockquote>
<p>  <strong>例1</strong>. 对于实数域区间<span class="math notranslate nohighlight">\([a,b]\)</span>，令<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>表示实数域中所有闭区间构成的集合<span class="math notranslate nohighlight">\(\{h_{[a,b]}:a,b\in\mathbb{R},a\le b\}\)</span>, <span class="math notranslate nohighlight">\(\mathcal{X}=\mathbb{R}\)</span>。对于<span class="math notranslate nohighlight">\(x\in\mathcal{X}\)</span>，若<span class="math notranslate nohighlight">\(x\in [a,b]\)</span>，则<span class="math notranslate nohighlight">\(h_{[a,b]}=+1\)</span>，否则<span class="math notranslate nohighlight">\(h_{[a,b]}=-1\)</span>。若有<span class="math notranslate nohighlight">\(x_1=0.5, x_2=1.5\)</span>，则假设空间<span class="math notranslate nohighlight">\(\{h_{[0,1]},h_{[0,2]},h_{[1,2]},h_{[2,3]}\}\)</span>将<span class="math notranslate nohighlight">\(x_1,x_2\)</span>打散，所以<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>的VC维至少为2；对于任意大小为3的示例集<span class="math notranslate nohighlight">\(\{x_1,x_2,x_3\}\)</span>，不妨设<span class="math notranslate nohighlight">\(x_1&lt;x_2&lt;x_3\)</span>，则<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>中不存在任何假设<span class="math notranslate nohighlight">\(h_{[a,b]}\)</span>能实现对分结果<span class="math notranslate nohighlight">\(\{(x_1,+),(x_2,-),(x_3,+)\}\)</span>。因此<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>的VC维为2。</p>
<p>  (二) <strong>VC维与增长函数关系</strong></p>
<p>  由VC维的定义可知，VC维与增长函数有着密切联系。以下定理给出了二者之间的定量关系。</p>
<blockquote>
<div><p><strong>定理 (Sauer引理)</strong>. 若假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>的VC维为<span class="math notranslate nohighlight">\(d\)</span>，则对任意<span class="math notranslate nohighlight">\(m\in\mathbb{N}\)</span>有，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-eq-sauer">
<span class="eqno">(15)<a class="headerlink" href="#equation-eq-sauer" title="Link to this equation">¶</a></span>\[\begin{split}\Pi_{\mathcal{H}}(m)\le \sum_{i=0}^d\begin{pmatrix}m\\ i \end{pmatrix}\end{split}\]</div>
<p>证明略。由Sauer引理，可计算出增长函数的上界：</p>
<blockquote>
<div><p><strong>推论</strong>. 若假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>的VC维为<span class="math notranslate nohighlight">\(d\)</span>，则对任意整数<span class="math notranslate nohighlight">\(m\ge d\)</span>有，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-sauer-infer">
<span class="eqno">(16)<a class="headerlink" href="#equation-sauer-infer" title="Link to this equation">¶</a></span>\[\Pi_{\mathcal{H}}(m)\le \left(\frac{e\cdot m}{d}\right)^d\]</div>
<p>  (三) <strong>VC维与泛化误差界</strong></p>
<blockquote>
<div><p><strong>定理 3</strong>. 若假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>的VC维为<span class="math notranslate nohighlight">\(d\)</span>，则对任意<span class="math notranslate nohighlight">\(m&gt;d,0&lt;\delta&lt;1\)</span>和<span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>有</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-vc-error-relation">
<span class="eqno">(17)<a class="headerlink" href="#equation-vc-error-relation" title="Link to this equation">¶</a></span>\[\boxed{
P\left(\left| E(h)-\hat{E}(h) \right|\le\sqrt{\frac{8d\ln\frac{2em}{d}+8\ln\frac{4}{\delta}} {m}   } \right) \ge 1-\delta}\]</div>
<p>证明. 令<span class="math notranslate nohighlight">\(\delta=4\Pi_{\mathcal{H}}(2m)\exp\left(-\frac{m\epsilon^2}{8}\right)\le 4 \left(\frac{e\cdot m}{d}\right)^d\exp\left(-\frac{m\epsilon^2}{8}\right)\)</span>，可解得<span class="math notranslate nohighlight">\(\epsilon = \sqrt{\frac{8d\ln\frac{2em}{d}+8\ln\frac{4}{\delta}} {m}   }\)</span>。</p>
<p>  由此可知，泛化误差界<a class="reference internal" href="#equation-vc-error-relation">(17)</a>只与样例数<span class="math notranslate nohighlight">\(m\)</span>有关，收敛速率为<span class="math notranslate nohighlight">\(O(\sqrt{\frac{1}{m}})\)</span>，与数据分布<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>和样例集<span class="math notranslate nohighlight">\(D\)</span>无关。因此，基于VC维的泛化误差界是分布无关、数据独立的。</p>
<blockquote>
<div><p><strong>定义 (经验风险最小化)</strong>. 令<span class="math notranslate nohighlight">\(h\)</span>表示学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>输出的假设，若<span class="math notranslate nohighlight">\(h\)</span>满足</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-eq-exper-min">
<span class="eqno">(18)<a class="headerlink" href="#equation-eq-exper-min" title="Link to this equation">¶</a></span>\[\hat{E}(h)=\min\limits_{h'\in\mathcal{H}}\hat{E}(h')\]</div>
<p>则称<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>为满足经验风险最小化原则的算法。</p>
<p>  (四) <strong>VC维与PAC可学习</strong></p>
<blockquote>
<div><p><strong>定理</strong>. 任何VC维有限的假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>都是（不可知）PAC可学习的。</p>
</div></blockquote>
<ul class="simple">
<li><p><strong>样本量上界与VC维<a class="footnote-reference brackets" href="#myref" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></strong></p></li>
</ul>
<p>  如果<span class="math notranslate nohighlight">\(1\le d &lt;\infty\)</span>，那么学习<span class="math notranslate nohighlight">\(\epsilon,\delta\)</span>所需的样本量为，</p>
<div class="math notranslate nohighlight" id="equation-sample-count-up">
<span class="eqno">(19)<a class="headerlink" href="#equation-sample-count-up" title="Link to this equation">¶</a></span>\[m\le\frac{64}{\epsilon^2}\left[2d\ln\left(\frac{12}{\epsilon} \right) +\ln\left(\frac{4}{\delta} \right) \right]\]</div>
<ul class="simple">
<li><p><strong>样本量下界与VC维<a class="footnote-reference brackets" href="#myref" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></strong></p></li>
</ul>
<p>  如果<span class="math notranslate nohighlight">\(1\le d &lt;\infty\)</span>，那么学习<span class="math notranslate nohighlight">\(\epsilon&gt;0,\delta&lt;\frac{1}{64}\)</span>所需的样本量至少为<span class="math notranslate nohighlight">\(\frac{d}{320\epsilon^2}\)</span>，此外，如果<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>中至少包含2个<span class="math notranslate nohighlight">\(h\)</span>，那么对于<span class="math notranslate nohighlight">\(0&lt;\epsilon&lt;1\)</span>和<span class="math notranslate nohighlight">\(0&lt;\delta&lt;\frac14\)</span>来说，样本量至少为</p>
<div class="math notranslate nohighlight" id="equation-sample-count-up">
<span class="eqno">(20)<a class="headerlink" href="#equation-sample-count-up" title="Link to this equation">¶</a></span>\[m\ge 2\left[ \frac{1-\epsilon^2}{2\epsilon^2}\ln \left(\frac{1}{8\delta(1-2\delta)} \right) \right]\]</div>
</section>
</section>
<section id="rademacher">
<h3><span class="section-number">4.1.3. </span>Rademacher复杂度<a class="headerlink" href="#rademacher" title="Link to this heading">¶</a></h3>
<p>  基于VC维的泛化误差界是分布无关数据独立的，也就是说对任意数据分布都成立，使得基于VC维的可学习性分析结果具有一定的普适性；但从另一方面来说，由于没有考虑数据本身，基于VC维得到的泛化误差界比较松。Rademacher复杂度是另一种描述假设空间复杂度的途径，与VC维不同，它在一定程度上考虑了数据分布。</p>
<p>  假设<span class="math notranslate nohighlight">\(h\)</span>的经验误差为，</p>
<div class="math notranslate nohighlight" id="equation-hyposis-experi">
<span class="eqno">(21)<a class="headerlink" href="#equation-hyposis-experi" title="Link to this equation">¶</a></span>\[\begin{split}
\begin{split}
\hat{E}(h)&amp;=\frac1m\sum_{i=1}^m \mathbb{I}(h(\pmb{x}_i)\neq y_i)\\
&amp;=\frac1m\sum_{i=1}^m\frac{1-y_ih(\pmb{x}_i)}{2}\\
&amp;=\frac12-\frac{1}{2m}\sum_{i=1}^m y_ih(\pmb{x}_i)
\end{split}
\end{split}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\frac{1}{m}\sum_{i=1}^m y_ih(\pmb{x}_i)\)</span>体现了预测值<span class="math notranslate nohighlight">\(h(\pmb{x}_i)\)</span>与样例真实标记<span class="math notranslate nohighlight">\(y_i\)</span>之间的一致性，若对于所有<span class="math notranslate nohighlight">\(i\)</span>都有<span class="math notranslate nohighlight">\(h(\pmb{x}_i)=y_i\)</span>，则<span class="math notranslate nohighlight">\(\frac{1}{m}\sum_{i=1}^m y_ih(\pmb{x}_i)\)</span>取最大值1。换句话说，经验误差最小的假设满足下式，</p>
<div class="math notranslate nohighlight" id="equation-exp-error-min">
<span class="eqno">(22)<a class="headerlink" href="#equation-exp-error-min" title="Link to this equation">¶</a></span>\[
\arg\max\limits_{h\in\mathcal{H}}\frac1m\sum_{i=1}^m y_ih(\pmb{x}_i)
\]</div>
<p>  然而，现实应用中，样本标记可能会受到噪声影响，也就是某些标记<span class="math notranslate nohighlight">\(y_i\)</span>可能受随机因素影响不再是<span class="math notranslate nohighlight">\(\pmb{x}_i\)</span>的真实标记。<strong>这种情况下，选择假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>在训练集上表现良好的假设，有时还不如选择<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>中事先已考虑了随机噪声影响的假设</strong>。</p>
<p>  考虑随机变量<span class="math notranslate nohighlight">\(\sigma_i\)</span>，且<span class="math notranslate nohighlight">\(P(\sigma_i=+1)=0.5=P(\sigma_i=-1)\)</span>，称为<strong>Rademacher随机变量</strong>，将此变量引入<a class="reference internal" href="#equation-exp-error-min">(22)</a>，则有，</p>
<div class="math notranslate nohighlight" id="equation-radermacher-error">
<span class="eqno">(23)<a class="headerlink" href="#equation-radermacher-error" title="Link to this equation">¶</a></span>\[
\sup\limits_{h\in\mathcal{H}}\frac1m\sum_{i=1}^m\sigma_i h(\pmb{x}_i)
\]</div>
<p>考虑<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>中所有假设，可对式<a class="reference internal" href="#equation-radermacher-error">(23)</a>取期望，则有</p>
<div class="math notranslate nohighlight" id="equation-all-rade-expectation">
<span class="eqno">(24)<a class="headerlink" href="#equation-all-rade-expectation" title="Link to this equation">¶</a></span>\[\mathbb{E}_{\pmb{\sigma}}\left[\sup\limits_{h\in\mathcal{H}}\frac1m\sum_{i=1}^m\sigma_i h(\pmb{x}_i) \right]\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\pmb{\sigma}=\{\sigma_1,...,\sigma_m\}\)</span>。式<a class="reference internal" href="#equation-all-rade-expectation">(24)</a>的取值范围是<span class="math notranslate nohighlight">\([0,1]\)</span>，它体现了假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>的表示能力。例如：当<span class="math notranslate nohighlight">\(|\mathcal{H}|=1\)</span>时，只有一个假设，这时可计算出式<a class="reference internal" href="#equation-all-rade-expectation">(24)</a>的值为0；当<span class="math notranslate nohighlight">\(|\mathcal{H}|=2^m\)</span>且<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>能打散<span class="math notranslate nohighlight">\(D\)</span>时，任意<span class="math notranslate nohighlight">\(\pmb{\sigma}\)</span>中总有一个假设使得<span class="math notranslate nohighlight">\(h(\pmb{x}_i)=\sigma_i (i=1,2,3...,m)\)</span>，此时式<a class="reference internal" href="#equation-all-rade-expectation">(24)</a>的值为1。</p>
<blockquote>
<div><p><strong>定义 (经验Rademacher复杂度)</strong>. 函数空间<span class="math notranslate nohighlight">\(\mathcal{F}\)</span>关于样例集<span class="math notranslate nohighlight">\(Z\)</span>的经验Rademacher复杂度为，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-f-z-rade-comp">
<span class="eqno">(25)<a class="headerlink" href="#equation-f-z-rade-comp" title="Link to this equation">¶</a></span>\[\hat{R}_Z (\mathcal{F})=\mathbb{E}_{\pmb{\sigma}}\left[\sup\limits_{f\in\mathcal{F}}\frac{1}{m} \sum_{i=1}^m\sigma_i f(\pmb{z}_i) \right]\]</div>
<blockquote>
<div><p><strong>定义 (Rademacher复杂度)</strong>. 函数空间<span class="math notranslate nohighlight">\(\mathcal{F}\)</span>关于<span class="math notranslate nohighlight">\(\mathcal{Z}\)</span>上分布<span class="math notranslate nohighlight">\(\mathcal{Z}\)</span>的Rademacher复杂度为，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-f-z-rade-comp">
<span class="eqno">(26)<a class="headerlink" href="#equation-f-z-rade-comp" title="Link to this equation">¶</a></span>\[R_m (\mathcal{F})=\mathbb{E}_{Z\subseteq\mathcal{Z}:|Z|=m }\left[\hat{R}_Z (\mathcal{F}) \right]\]</div>
<p>  基于Rademacher复杂度可以得到关于函数空间<span class="math notranslate nohighlight">\(\mathcal{F}\)</span>的泛化误差界。</p>
<blockquote>
<div><p><strong>定理</strong>. 对实值函数空间<span class="math notranslate nohighlight">\(\mathcal{F}:\mathcal{Z}\rightarrow [0,1]\)</span>，根据分布<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>从<span class="math notranslate nohighlight">\(\mathcal{Z}\)</span>中独立同分布采样得到示例集<span class="math notranslate nohighlight">\(Z=\{\pmb{z}_1,...,\pmb{z}_m\}, \pmb{z}_i\in\mathcal{Z}\)</span>，<span class="math notranslate nohighlight">\(0&lt;\delta&lt;1\)</span>，对任意<span class="math notranslate nohighlight">\(f\in\mathcal{F}\)</span>，以至少<span class="math notranslate nohighlight">\(1-\delta\)</span>的概率有，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-rad-error-bound">
<span class="eqno">(27)<a class="headerlink" href="#equation-rad-error-bound" title="Link to this equation">¶</a></span>\[\begin{split}\boxed{
\begin{split}
\mathbb{E}[f(\pmb{z})]&amp;\le \frac1m\sum_{i=1}^m f(\pmb{z}_i) + 2R_m(\mathcal{F})+\sqrt{\frac{\ln(1/\delta)}{2m}}\\
\mathbb{E}[f(\pmb{z})]&amp;\le \frac1m\sum_{i=1}^m f(\pmb{z}_i) + 2\hat{R}_Z(\mathcal{F})+3\sqrt{\frac{\ln(2/\delta)}{2m}}\\
\end{split}}\end{split}\]</div>
<blockquote>
<div><p><strong>定理</strong>. 对假设空间<span class="math notranslate nohighlight">\(\mathcal{H}:\mathcal{X}\rightarrow\{-1,+1\}\)</span>，根据分布<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>从<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>中独立同分布采样得到示例集<span class="math notranslate nohighlight">\(D=\{\pmb{x}_1,...,\pmb{x}_m\}, \pmb{x}_i\in\mathcal{X}\)</span>，<span class="math notranslate nohighlight">\(0&lt;\delta&lt;1\)</span>，对任意<span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span>，以至少<span class="math notranslate nohighlight">\(1-\delta\)</span>的概率有，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-rad-error-bound">
<span class="eqno">(28)<a class="headerlink" href="#equation-rad-error-bound" title="Link to this equation">¶</a></span>\[\begin{split}\boxed{
\begin{split}
\mathbb{E}[h]&amp;\le \hat{E}(h)+ R_m(\mathcal{H})+\sqrt{\frac{\ln(1/\delta)}{2m}}\\
\mathbb{E}[h]&amp;\le \hat{E}(h)+ \hat{R}_D(\mathcal{H})+3\sqrt{\frac{\ln(2/\delta)}{2m}}\\
\end{split}}\end{split}\]</div>
<blockquote>
<div><p><strong>定理</strong>.  假设空间<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>的Rademacher复杂度<span class="math notranslate nohighlight">\(R_m(\mathcal{H})\)</span>与增长函数<span class="math notranslate nohighlight">\(\Pi_\mathcal{H}(m)\)</span>满足，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-rade-increase">
<span class="eqno">(29)<a class="headerlink" href="#equation-rade-increase" title="Link to this equation">¶</a></span>\[R_m(\mathcal{H})\le \sqrt{\frac{2\ln\Pi_{\mathcal{H}}(m)}{m}}\]</div>
<p>  由此可知，从Rademacher复杂度和增长函数能推导出基于VC维的泛化误差界，</p>
<div class="math notranslate nohighlight" id="equation-conclusion">
<span class="eqno">(30)<a class="headerlink" href="#equation-conclusion" title="Link to this equation">¶</a></span>\[
\boxed{
  E(h)\le\hat{E}(h)+\sqrt{\frac{2d\ln\frac{em}{d}}{m}}+\sqrt{\frac{\ln(1/\delta)}{2m}}
}
\]</div>
<p>可以发现，无论是基于VC维还是Rademacher复杂度来推导泛化误差界，所得到的结果均与具体学习算法无关，对所有学习算法都适用。这使得人们能够脱离具体学习算法的设计来考虑学习问题本身的性质。</p>
</section>
<section id="id7">
<h3><span class="section-number">4.1.4. </span>稳定性<a class="headerlink" href="#id7" title="Link to this heading">¶</a></h3>
<p>  稳定性主要考查算法在输入变化时，输出是否随之发生较大变化。</p>
<blockquote>
<div><p><strong>定义</strong> (<span class="math notranslate nohighlight">\(\beta\)</span>-均匀稳定性).  对任意<span class="math notranslate nohighlight">\(\pmb{x}\in\mathcal{X}, z=(\pmb{x},y)\)</span>，若学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>满足，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-beta-stability">
<span class="eqno">(31)<a class="headerlink" href="#equation-beta-stability" title="Link to this equation">¶</a></span>\[|\ell(\mathfrak{L}_D,z) - \ell(\mathfrak{L}_{D^{\backslash i}}, z) |\le \beta\]</div>
<p>则称算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>关于损失<span class="math notranslate nohighlight">\(\ell\)</span>满足<span class="math notranslate nohighlight">\(\beta\)</span>-均匀稳定性。</p>
<blockquote>
<div><p><strong>定理</strong> (稳定性泛化误差界)  若算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>关于损失<span class="math notranslate nohighlight">\(\ell\)</span>满足<span class="math notranslate nohighlight">\(\beta\)</span>-均匀稳定性，且损失函数<span class="math notranslate nohighlight">\(\ell\)</span>的上界为<span class="math notranslate nohighlight">\(M\)</span>， <span class="math notranslate nohighlight">\(0&lt;\delta&lt;1\)</span>，则对于任意<span class="math notranslate nohighlight">\(m\ge 1\)</span>（<span class="math notranslate nohighlight">\(m\)</span>为从分布<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>上独立同分步采样得到的数据集<span class="math notranslate nohighlight">\(D\)</span>的大小），至少以<span class="math notranslate nohighlight">\(1-\delta\)</span>的概率有，</p>
</div></blockquote>
<div class="math notranslate nohighlight" id="equation-stability-error-bound">
<span class="eqno">(32)<a class="headerlink" href="#equation-stability-error-bound" title="Link to this equation">¶</a></span>\[\ell(\mathfrak{L},D)\le\hat{\ell}(\mathfrak{L},D)+2\beta+(4m\beta+M)\sqrt{\frac{\ln(1/\delta)}{2m}}\]</div>
<p>  上述定理给出了基于稳定性分析推导出的学习算法<span class="math notranslate nohighlight">\(\mathfrak{L}\)</span>学习得到的假设的泛化误差界。稳定性分析不必考虑假设空间所有可能假设，只需要根据算法自身的稳定性来讨论输出假设<span class="math notranslate nohighlight">\(\mathfrak{L}_D\)</span>的泛化误差界。</p>
</section>
</section>
</section>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="mlzhou" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">1</a><span class="fn-bracket">]</span></span>
<p>周志华. 机器学习. 北京：清华大学出版社，2016.</p>
</aside>
<aside class="footnote brackets" id="myref" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id5">1</a>,<a role="doc-backlink" href="#id6">2</a>)</span>
<p>Sanjeev Kulkarni, Gilbert Harman et al. An elementary introduction to statistical learning theory.</p>
</aside>
</aside>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">目录</a></h3>
    <ul>
<li><a class="reference internal" href="#">4. 计算学习理论</a><ul>
<li><a class="reference internal" href="#id3">4.1. 概率近似正确学习理论</a><ul>
<li><a class="reference internal" href="#pac">4.1.1. PAC</a><ul>
<li><a class="reference internal" href="#mathcal-h">4.1.1.1. 有限假设空间(<span class="math notranslate nohighlight">\(|\mathcal{H}|\)</span>有限)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#vc">4.1.2. VC维</a><ul>
<li><a class="reference internal" href="#id4">4.1.2.1. 无限假设空间(<span class="math notranslate nohighlight">\(|\mathcal{H}|\)</span>无限)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#rademacher">4.1.3. Rademacher复杂度</a></li>
<li><a class="reference internal" href="#id7">4.1.4. 稳定性</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>上一主题</h4>
    <p class="topless"><a href="gaussian_process.html"
                          title="上一章"><span class="section-number">3. </span>高斯过程</a></p>
  </div>
  <div>
    <h4>下一主题</h4>
    <p class="topless"><a href="../ml/dimension_reduce_2.html"
                          title="下一章"><span class="section-number">1. </span>数据降维</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>本页</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/base/calc_theory.md.txt"
            rel="nofollow">显示源代码</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">快速搜索</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="提交" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="总索引"
             >索引</a></li>
        <li class="right" >
          <a href="../ml/dimension_reduce_2.html" title="1. 数据降维"
             >下一页</a> |</li>
        <li class="right" >
          <a href="gaussian_process.html" title="3. 高斯过程"
             >上一页</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Machine Learning Fundation 1.0 文档</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">4. </span>计算学习理论</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; 版权所有 2022-2024, SSPUIIP.
      由 <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3创建。
    </div>
  </body>
</html>