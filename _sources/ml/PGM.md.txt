# 概率图模型

&emsp;&emsp;根据已观察到的数据（样本集）对未知变量（样本所属类别）进行估计，这是从数据学习知识的基本途径，也是机器学习的主要任务。概率模型将这一任务转换为计算变量的概率分布，即利用已知变量推测未知变量，也称之为**推断**。具体来说，假设所关心的变量集为$Y$，可观测变量集为$O$，其它变量为$R$，推断就是通过联合分布$P(Y,R,O)$或条件分布$P(Y,R|O)$计算得到条件分布$P(Y|O)$。其中，联合分布$P(Y,R,O)$称为**生成式模型**，条件分布$P(Y,R|O)$称为**判别式模型**。但是，直接使用概率求和规则消去变量$R$是不可行的，因为即使所有变量只有2种取值的特殊情况，计算复杂度也高达$O(2^{|Y|+|R|})$。

&emsp;&emsp;概率图模型是一种用图来表示变量间关系的概率模型。该图的结点表示一个（组）随机变量，结点之间的边表示变量间的相关关系。根据边的类型不同，图模型又可以继续细分为**有向无环图**（贝叶斯网，Bayesian network）和**无向无环图**（马尔可夫网，Markov network）两种。

## 隐马尔可夫模型

&emsp;&emsp;隐马尔可夫模型（Hidden Markov Model, HMM）是一种有向图模型，主要用于时序数据建模、自然语言处理、语音识别等领域。

```{mermaid}
---
caption: Fig 1. Hidden Markov Model
zoom: 100%
align: center
---
block-beta
  columns 10
  x1(("x1")) space x2(("x2")) space x3(("x3")) space x4(("...")) space x5(("xn")) space space space space space space space space space space space
  y1(("y1")) space y2(("y2")) space y3(("y3")) space y4(("...")) space y5(("yn"))
  y1 --> y2
  y2 --> y3
  y3 --> y4
  y4 --> y5
  y1 --> x1
  y2 --> x2
  y3 --> x3
  y4 --> x4
  y5 --> x5
```
&emsp;&emsp;如上图所示，隐马尔可夫模型中的变量分为两类，第一类为状态变量$\{y_1,y_2,...,y_n\}$，$y_i$表示第$i$时刻的系统状态，一般状态变量是不可观测的，也称为隐变量；第二类为观测变量$\{x_1,x_2,...,x_n\}$，$x_i$表示为第$i$时刻的观测值。图中的箭头代表的是变量间的依赖关系。具体来说有以下两点：

- $x_t$只依赖$y_t$。$t$时刻的观测值只与$t$时刻的状态相关，与其它状态无关。

- $y_t$只依赖$y_{t-1}$。 该性质也就是所谓的**马尔可夫性**。

根据依赖关系，HMM所有变量的**联合分布**为，

$$
P(x_1,...,x_n,y_1,...,y_n)=P(y_1)P(x_1|y_1)\prod_{i=2}^n P(x_i|y_i)P(y_{i}|y_{i-1})
\tag{1}
$$

&emsp;&emsp;除了上述变量间的依赖关系（也就是模型的结构信息），确定一个隐马尔可夫模型还需要三组参数，即**模型参数$\lambda=[A,B,\pmb{\pi}]$**：


| 参数 | 描述 |
| :---: | :---: | 
| **状态转移概率**。<br>记为矩阵$A=[a_{ij}]_{N\times N}$<br>$a_{ij}=P(y_{t+1}=s_j \| y_t=s_i)$ | 各个状态间的跳转概率。<br>在任意时刻$t$，若状态为$s_i$，则下一时刻状态为$s_j$的概率。 |
|**输出观测概率**。<br>记为矩阵$B=[b_{ij}]_{N\times M}$<br>$b_{ij}=P(x_t=o_j \| y_t=s_i)$ | 模型当前状态得到观测值的概率。<br>在任意时刻$t$，若状态为$s_i$，则观测值为$o_j$的概率。 |
|**初使状态概率**。<br>记为$\pmb{\pi}=(\pi_1,...,\pi_N)$<br>$\pi_i = P(y_1=s_i)$ | 模型在初始时刻各状态出现的概率。|

通过上述三组参数可以确定一个隐马尔可夫模型。

&emsp;&emsp;现实应用中，隐马尔可夫模型一般主要用来解决以下3种问题：

|应用问题 | 场景 |
| :---: | :---:|
|1. 计算观测序列产生概率$P(\pmb{x}\|\lambda)$，也就是如何评估模型与观测序列之间的匹配程度？    | 根据以往观测序列$(x_1,...,x_{n-1}$推测当前时刻观测值$x_n$的可能性，可以转化为求概率$P(\pmb{x}\|\lambda)$。 |
| 2. 给定模型$\lambda$和观测序列$\pmb{x}=(x_1,...,x_n)$，如何找到与此观测序列匹配的隐状态序列$\pmb{y}=(y_1,...,y_n)$，也就是根据观测序列如何推断出隐状态？     | 语音识别任务中，隐藏状态是文字，目标为根据观测信号推断最有可能的状态序列（文字序列）。 |
| 3. 给定观测序列$\pmb{x}=(x_1,...,x_n)$，如何优化参数$\lambda$使得序列出现的概率$P(\pmb{x}\| \lambda)$最大，也就是如何训练模型？| 根据训练样本得到最优参数。根据条件独立性，隐马尔可夫模型的三个问题都能高效求解。 |

## 马尔可夫随机场

&emsp;&emsp;马尔可夫随机场是一种无向图模型。图中结点表示一个（组）变量，结点之间的边表示变量之间的依赖关系。马尔可夫随机场的联合概率分布函数由一组**势函数**（potential functions），也称之为**因子**(factor)，构成。势函数是定义在变量子集上的非负实函数。

&emsp;&emsp;马尔可夫随机场的变量子集根据结点特性可以加以区别。若一个结点子集中任意两结点之间都有边连接，则称该子集为一个**团**（clique）；若在一个团中加入另外任何一个结点后，不再形成团，则该团称为**极大团**（maximal clique）。

&emsp;&emsp;在马尔可夫随机场中，多个变量之间的联合概率分布可能基于团分解为多个势函数（因子）的乘积，每次个势函数只与一个团关联。具体来说，对于$n$个变量$\pmb{x}=\{x_1,...,x_n\}$，所有团构成的集合为$\mathcal{C}$，团$Q\in\mathcal{C}$相关的变量集合记为$\pmb{x}_Q$，则**联合概率**定义为，

$$
P(\pmb{x})=\frac1Z \prod_{Q\in\mathcal{C}}\psi_Q(\pmb{x}_Q)\tag{2}
$$

其中，$\psi_Q$为团$Q$对应的势函数，$Z=\sum_{\pmb{x}}\prod_{Q\in\mathcal{C}}\psi_Q(\pmb{x}_Q)$为常数，也称之为规范化因子。实际应用中精确计算$Z$往往很困难，但很多时候并不需要获得$Z$的精确值。

&emsp;&emsp;若变量个数过多，则团的数据会很多，就会对联合概率的计算带来负担。可以发现，只要团$Q$不是极大团，则它必然被一个极大团$Q^*$包含。因此，可以根据极大团来定义联合概率。假设极大团构成的集合为$\mathcal{C}^*$，则有，

$$
P(\pmb{x})=\frac{1}{Z^*} \prod_{Q\in\mathcal{C}^*}\psi_Q(\pmb{x}_Q)\tag{3}
$$
其中，$Z^*=\sum_{\pmb{x}}\prod_{Q\in\mathcal{C}^*}\psi_Q(\pmb{x}_Q)$。


&emsp;&emsp;**例**1。假设有一随机变量集$\pmb{x}=\{x_1,x_2,...,x_6\}$的马尔可夫随机场如下图所示，

```{mermaid}
---
caption: Fig 2. 马尔可夫随机场示例 
align: center
---
flowchart LR
  x1((x1)) --- x2((x2))
  x1((x1)) --- x3((x3))
  x2((x2)) --- x4((x4))
  x2((x2)) --- x6((x6))
  x2((x2)) --- x5((x5))
  x5((x5)) --- x6((x6))
  x3((x3)) --- x5((x5))

```
则联合概率分布为，

$$
P(\pmb{x})=\frac1Z \psi_{12}(x_1,x_2)\psi_{13}(x_1,x_3)\psi_{24}(x_2,x_4)\psi_{35}(x_3,x_5)\psi_{256}(x_2,x_5,x_6)
$$

